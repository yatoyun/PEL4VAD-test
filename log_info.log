[2024-03-08 15:32:02,178][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__8968.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-08 15:32:05,227][main.py][line:259][INFO] total params:8.3485M
[2024-03-08 15:32:05,227][main.py][line:262][INFO] Training Mode
[2024-03-08 15:32:05,227][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-08 15:32:05,228][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-08 15:32:21,495][main.py][line:76][INFO] Random initialize AUCAUC:0.5465 Anomaly AUC:0.54814
[2024-03-08 15:33:21,158][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2385 loss2:0.8576 loss3:0.5210 | AUC:0.8696 Anomaly AUC:0.6845
[2024-03-08 15:33:39,434][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.1675 loss2:0.8741 loss3:0.4971 | AUC:0.8677 Anomaly AUC:0.6876
[2024-03-08 15:33:57,159][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1875 loss2:0.9453 loss3:0.4745 | AUC:0.8667 Anomaly AUC:0.6849
[2024-03-08 15:34:14,743][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1731 loss2:0.9008 loss3:0.4821 | AUC:0.8701 Anomaly AUC:0.6894
[2024-03-08 15:34:32,173][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1262 loss2:0.8853 loss3:0.4671 | AUC:0.8625 Anomaly AUC:0.6813
[2024-03-08 15:34:49,819][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.1606 loss2:0.8352 loss3:0.5040 | AUC:0.8698 Anomaly AUC:0.6936
[2024-03-08 15:35:07,453][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0986 loss2:0.8686 loss3:0.4671 | AUC:0.8595 Anomaly AUC:0.6703
[2024-03-08 15:35:24,883][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1451 loss2:0.9696 loss3:0.4691 | AUC:0.8688 Anomaly AUC:0.6876
[2024-03-08 15:35:42,380][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0608 loss2:0.8765 loss3:0.4582 | AUC:0.8590 Anomaly AUC:0.6770
[2024-03-08 15:35:59,983][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1415 loss2:0.8649 loss3:0.4677 | AUC:0.8642 Anomaly AUC:0.6887
[2024-03-08 15:36:17,596][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.1759 loss2:0.7825 loss3:0.4345 | AUC:0.8626 Anomaly AUC:0.6947
[2024-03-08 15:36:35,311][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1407 loss2:0.7715 loss3:0.4699 | AUC:0.8679 Anomaly AUC:0.6927
[2024-03-08 15:36:52,808][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1093 loss2:0.7906 loss3:0.4574 | AUC:0.8680 Anomaly AUC:0.6934
[2024-03-08 15:37:10,555][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.3595 loss2:0.8727 loss3:0.4316 | AUC:0.8720 Anomaly AUC:0.6987
[2024-03-08 15:37:28,251][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.1008 loss2:0.7785 loss3:0.4537 | AUC:0.8744 Anomaly AUC:0.7041
[2024-03-08 15:37:45,975][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0892 loss2:0.8855 loss3:0.4508 | AUC:0.8683 Anomaly AUC:0.6856
[2024-03-08 15:38:03,437][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.2581 loss2:0.9190 loss3:0.4232 | AUC:0.8706 Anomaly AUC:0.6848
[2024-03-08 15:38:21,010][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.0804 loss2:0.7881 loss3:0.4338 | AUC:0.8779 Anomaly AUC:0.6992
[2024-03-08 15:38:38,590][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0461 loss2:0.7499 loss3:0.4415 | AUC:0.8786 Anomaly AUC:0.7037
[2024-03-08 15:38:56,270][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.0766 loss2:0.7517 loss3:0.4427 | AUC:0.8789 Anomaly AUC:0.7111
[2024-03-08 15:39:13,925][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.0951 loss2:0.8181 loss3:0.4431 | AUC:0.8756 Anomaly AUC:0.7043
[2024-03-08 15:39:31,670][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0407 loss2:0.7272 loss3:0.4370 | AUC:0.8789 Anomaly AUC:0.7067
[2024-03-08 15:39:49,489][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.0913 loss2:0.8769 loss3:0.4232 | AUC:0.8810 Anomaly AUC:0.7081
[2024-03-08 15:40:07,037][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0974 loss2:0.8820 loss3:0.4448 | AUC:0.8843 Anomaly AUC:0.7157
[2024-03-08 15:40:24,497][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0516 loss2:0.7669 loss3:0.4238 | AUC:0.8837 Anomaly AUC:0.7169
[2024-03-08 15:40:41,895][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.0783 loss2:0.7693 loss3:0.4246 | AUC:0.8812 Anomaly AUC:0.7103
[2024-03-08 15:40:57,787][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.0783 loss2:0.7693 loss3:0.4246 | AUC:0.8813 Anomaly AUC:0.7105
[2024-03-08 15:41:15,510][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0325 loss2:0.7432 loss3:0.4209 | AUC:0.8808 Anomaly AUC:0.7131
[2024-03-08 15:41:32,860][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.0738 loss2:0.7230 loss3:0.4164 | AUC:0.8843 Anomaly AUC:0.7222
[2024-03-08 15:41:50,500][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1481 loss2:0.8322 loss3:0.4276 | AUC:0.8837 Anomaly AUC:0.7188
[2024-03-08 15:42:08,054][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0261 loss2:0.7556 loss3:0.4178 | AUC:0.8783 Anomaly AUC:0.7060
[2024-03-08 15:42:25,595][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.0408 loss2:0.7624 loss3:0.4038 | AUC:0.8809 Anomaly AUC:0.7117
[2024-03-08 15:42:42,955][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0392 loss2:0.7836 loss3:0.3997 | AUC:0.8804 Anomaly AUC:0.7110
[2024-03-08 15:43:00,663][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0367 loss2:0.7346 loss3:0.3966 | AUC:0.8808 Anomaly AUC:0.7171
[2024-03-08 15:43:18,212][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0239 loss2:0.7883 loss3:0.4079 | AUC:0.8807 Anomaly AUC:0.7153
[2024-03-08 15:43:35,812][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0530 loss2:0.7610 loss3:0.4030 | AUC:0.8839 Anomaly AUC:0.7209
[2024-03-08 15:43:53,386][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0257 loss2:0.7614 loss3:0.3983 | AUC:0.8772 Anomaly AUC:0.7085
[2024-03-08 15:44:10,977][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0194 loss2:0.7229 loss3:0.4019 | AUC:0.8788 Anomaly AUC:0.7028
[2024-03-08 15:44:28,552][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0636 loss2:0.7300 loss3:0.3852 | AUC:0.8859 Anomaly AUC:0.7183
[2024-03-08 15:44:45,919][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0410 loss2:0.7448 loss3:0.3858 | AUC:0.8841 Anomaly AUC:0.7154
[2024-03-08 15:45:03,869][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0403 loss2:0.7444 loss3:0.3893 | AUC:0.8829 Anomaly AUC:0.7205
[2024-03-08 15:45:21,354][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.1350 loss2:0.7755 loss3:0.3935 | AUC:0.8853 Anomaly AUC:0.7234
[2024-03-08 15:45:38,951][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0541 loss2:0.7769 loss3:0.3790 | AUC:0.8803 Anomaly AUC:0.7172
[2024-03-08 15:45:56,379][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0221 loss2:0.7013 loss3:0.3916 | AUC:0.8825 Anomaly AUC:0.7183
[2024-03-08 15:46:13,850][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0216 loss2:0.6764 loss3:0.3903 | AUC:0.8834 Anomaly AUC:0.7171
[2024-03-08 15:46:31,303][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0187 loss2:0.6393 loss3:0.3801 | AUC:0.8851 Anomaly AUC:0.7198
[2024-03-08 15:46:48,894][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.0677 loss2:0.7501 loss3:0.3839 | AUC:0.8856 Anomaly AUC:0.7220
[2024-03-08 15:47:06,650][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0704 loss2:0.6573 loss3:0.3877 | AUC:0.8811 Anomaly AUC:0.7191
[2024-03-08 15:47:24,012][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.2590 loss2:0.8340 loss3:0.3784 | AUC:0.8734 Anomaly AUC:0.6906
[2024-03-08 15:47:41,680][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0533 loss2:0.6074 loss3:0.3751 | AUC:0.8769 Anomaly AUC:0.7007
[2024-03-08 15:47:59,437][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0831 loss2:0.6792 loss3:0.3765 | AUC:0.8808 Anomaly AUC:0.7122
[2024-03-08 15:48:16,898][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0254 loss2:0.6389 loss3:0.3757 | AUC:0.8808 Anomaly AUC:0.7162
[2024-03-08 15:48:32,859][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0254 loss2:0.6389 loss3:0.3757 | AUC:0.8807 Anomaly AUC:0.7161
[2024-03-08 15:48:50,960][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0207 loss2:0.7613 loss3:0.3734 | AUC:0.8820 Anomaly AUC:0.7230
[2024-03-08 15:49:08,717][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0185 loss2:0.6826 loss3:0.3751 | AUC:0.8829 Anomaly AUC:0.7235
[2024-03-08 15:49:26,407][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0125 loss2:0.6586 loss3:0.3767 | AUC:0.8809 Anomaly AUC:0.7136
[2024-03-08 15:49:43,889][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0230 loss2:0.4519 loss3:0.3766 | AUC:0.8771 Anomaly AUC:0.7043
[2024-03-08 15:50:01,620][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0167 loss2:0.5960 loss3:0.3729 | AUC:0.8790 Anomaly AUC:0.7120
[2024-03-08 15:50:19,111][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0217 loss2:0.7423 loss3:0.3690 | AUC:0.8791 Anomaly AUC:0.7182
[2024-03-08 15:50:36,831][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0188 loss2:0.5729 loss3:0.3705 | AUC:0.8727 Anomaly AUC:0.7025
[2024-03-08 15:50:54,255][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0191 loss2:0.6301 loss3:0.3714 | AUC:0.8710 Anomaly AUC:0.6972
[2024-03-08 15:51:11,827][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0528 loss2:0.5885 loss3:0.3704 | AUC:0.8743 Anomaly AUC:0.7042
[2024-03-08 15:51:29,375][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0112 loss2:0.6600 loss3:0.3695 | AUC:0.8750 Anomaly AUC:0.7011
[2024-03-08 15:51:46,725][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.0136 loss2:0.5600 loss3:0.3735 | AUC:0.8727 Anomaly AUC:0.6966
[2024-03-08 15:52:04,312][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0124 loss2:0.6596 loss3:0.3793 | AUC:0.8705 Anomaly AUC:0.6890
[2024-03-08 15:52:21,946][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.0204 loss2:0.6084 loss3:0.3605 | AUC:0.8832 Anomaly AUC:0.7132
[2024-03-08 15:52:39,668][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0646 loss2:0.6513 loss3:0.3648 | AUC:0.8793 Anomaly AUC:0.7042
[2024-03-08 15:52:57,171][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0147 loss2:0.5856 loss3:0.3671 | AUC:0.8640 Anomaly AUC:0.6734
[2024-03-08 15:53:14,595][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0105 loss2:0.5856 loss3:0.3667 | AUC:0.8765 Anomaly AUC:0.6958
[2024-03-08 15:53:32,466][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.0195 loss2:0.5962 loss3:0.3642 | AUC:0.8690 Anomaly AUC:0.6797
[2024-03-08 15:53:49,954][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0190 loss2:0.5852 loss3:0.3627 | AUC:0.8651 Anomaly AUC:0.6726
[2024-03-08 15:54:07,486][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0133 loss2:0.6580 loss3:0.3682 | AUC:0.8703 Anomaly AUC:0.6883
[2024-03-08 15:54:24,902][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0155 loss2:0.5956 loss3:0.3649 | AUC:0.8699 Anomaly AUC:0.6880
[2024-03-08 15:54:42,413][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.3288 loss2:0.7847 loss3:0.3575 | AUC:0.8563 Anomaly AUC:0.6659
[2024-03-08 15:54:59,783][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.2608 loss2:0.7818 loss3:0.3735 | AUC:0.8672 Anomaly AUC:0.6737
[2024-03-08 15:55:17,361][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0569 loss2:0.6403 loss3:0.3746 | AUC:0.8827 Anomaly AUC:0.7140
[2024-03-08 15:55:34,790][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0193 loss2:0.6450 loss3:0.3694 | AUC:0.8875 Anomaly AUC:0.7296
[2024-03-08 15:55:52,224][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.1404 loss2:0.6847 loss3:0.3682 | AUC:0.8849 Anomaly AUC:0.7211
[2024-03-08 15:56:07,974][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.1404 loss2:0.6847 loss3:0.3682 | AUC:0.8850 Anomaly AUC:0.7213
[2024-03-08 15:56:26,022][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.1320 loss2:0.6478 loss3:0.3824 | AUC:0.8841 Anomaly AUC:0.7195
[2024-03-08 15:56:43,734][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0722 loss2:0.6181 loss3:0.3708 | AUC:0.8784 Anomaly AUC:0.7125
[2024-03-08 15:57:01,420][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.0101 loss2:0.6098 loss3:0.3691 | AUC:0.8827 Anomaly AUC:0.7147
[2024-03-08 15:57:18,981][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0376 loss2:0.4838 loss3:0.3655 | AUC:0.8730 Anomaly AUC:0.6936
[2024-03-08 15:57:36,575][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.0151 loss2:0.6234 loss3:0.3635 | AUC:0.8750 Anomaly AUC:0.6952
[2024-03-08 15:57:54,337][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.0152 loss2:0.5705 loss3:0.3629 | AUC:0.8743 Anomaly AUC:0.6927
[2024-03-08 15:58:11,761][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.0150 loss2:0.5540 loss3:0.3641 | AUC:0.8738 Anomaly AUC:0.6906
[2024-03-08 15:58:29,319][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0742 loss2:0.5657 loss3:0.3657 | AUC:0.8746 Anomaly AUC:0.6935
[2024-03-08 15:58:46,891][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0329 loss2:0.5313 loss3:0.3587 | AUC:0.8735 Anomaly AUC:0.6852
[2024-03-08 15:59:04,563][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.0160 loss2:0.5996 loss3:0.3633 | AUC:0.8638 Anomaly AUC:0.6693
[2024-03-08 15:59:22,088][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.0141 loss2:0.5317 loss3:0.3619 | AUC:0.8634 Anomaly AUC:0.6672
[2024-03-08 15:59:39,674][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.0187 loss2:0.5253 loss3:0.3632 | AUC:0.8676 Anomaly AUC:0.6732
[2024-03-08 15:59:57,368][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0171 loss2:0.6195 loss3:0.3643 | AUC:0.8611 Anomaly AUC:0.6567
[2024-03-08 16:00:15,068][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0196 loss2:0.5383 loss3:0.3627 | AUC:0.8563 Anomaly AUC:0.6511
[2024-03-08 16:00:32,683][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.0161 loss2:0.4986 loss3:0.3612 | AUC:0.8721 Anomaly AUC:0.6891
[2024-03-08 16:00:50,143][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.0213 loss2:0.5126 loss3:0.3629 | AUC:0.8631 Anomaly AUC:0.6671
[2024-03-08 16:01:07,816][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0085 loss2:0.5475 loss3:0.3592 | AUC:0.8678 Anomaly AUC:0.6756
[2024-03-08 16:01:25,302][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.0157 loss2:0.5719 loss3:0.3598 | AUC:0.8576 Anomaly AUC:0.6520
[2024-03-08 16:01:42,714][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0331 loss2:0.5396 loss3:0.3630 | AUC:0.8632 Anomaly AUC:0.6685
[2024-03-08 16:02:00,525][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0136 loss2:0.4984 loss3:0.3582 | AUC:0.8732 Anomaly AUC:0.6875
[2024-03-08 16:02:18,232][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0166 loss2:0.4622 loss3:0.3597 | AUC:0.8674 Anomaly AUC:0.6732
[2024-03-08 16:02:35,932][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0178 loss2:0.5003 loss3:0.3604 | AUC:0.8658 Anomaly AUC:0.6692
[2024-03-08 16:02:53,513][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0117 loss2:0.4684 loss3:0.3568 | AUC:0.8636 Anomaly AUC:0.6626
[2024-03-08 16:03:11,040][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.0119 loss2:0.4025 loss3:0.3580 | AUC:0.8557 Anomaly AUC:0.6427
[2024-03-08 16:03:28,482][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0150 loss2:0.4866 loss3:0.3584 | AUC:0.8580 Anomaly AUC:0.6478
[2024-03-08 16:03:44,335][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0150 loss2:0.4866 loss3:0.3584 | AUC:0.8579 Anomaly AUC:0.6475
[2024-03-08 16:04:02,149][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0130 loss2:0.4128 loss3:0.3586 | AUC:0.8620 Anomaly AUC:0.6544
[2024-03-08 16:04:19,611][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.0102 loss2:0.4568 loss3:0.3572 | AUC:0.8594 Anomaly AUC:0.6502
[2024-03-08 16:04:37,163][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0158 loss2:0.5023 loss3:0.3574 | AUC:0.8612 Anomaly AUC:0.6538
[2024-03-08 16:04:54,959][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0157 loss2:0.4877 loss3:0.3560 | AUC:0.8651 Anomaly AUC:0.6647
[2024-03-08 16:05:12,438][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0254 loss2:0.4690 loss3:0.3585 | AUC:0.8503 Anomaly AUC:0.6312
[2024-03-08 16:05:30,078][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0118 loss2:0.5282 loss3:0.3552 | AUC:0.8536 Anomaly AUC:0.6372
[2024-03-08 16:05:47,469][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0119 loss2:0.4796 loss3:0.3592 | AUC:0.8458 Anomaly AUC:0.6194
[2024-03-08 16:06:05,001][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0094 loss2:0.4864 loss3:0.3587 | AUC:0.8550 Anomaly AUC:0.6429
[2024-03-08 16:06:22,446][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0189 loss2:0.5443 loss3:0.3569 | AUC:0.8407 Anomaly AUC:0.6086
[2024-03-08 16:06:39,754][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0107 loss2:0.4573 loss3:0.3624 | AUC:0.8374 Anomaly AUC:0.6061
[2024-03-08 16:06:57,450][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0115 loss2:0.5240 loss3:0.3585 | AUC:0.8543 Anomaly AUC:0.6464
[2024-03-08 16:07:15,038][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0137 loss2:0.4342 loss3:0.3591 | AUC:0.8271 Anomaly AUC:0.5851
[2024-03-08 16:07:32,581][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0187 loss2:0.3975 loss3:0.3580 | AUC:0.8619 Anomaly AUC:0.6606
[2024-03-08 16:07:50,205][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0121 loss2:0.4994 loss3:0.3559 | AUC:0.8599 Anomaly AUC:0.6561
[2024-03-08 16:08:07,692][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0108 loss2:0.4288 loss3:0.3584 | AUC:0.8530 Anomaly AUC:0.6392
[2024-03-08 16:08:25,380][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0113 loss2:0.4325 loss3:0.3579 | AUC:0.8617 Anomaly AUC:0.6576
[2024-03-08 16:08:42,963][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0124 loss2:0.3541 loss3:0.3574 | AUC:0.8656 Anomaly AUC:0.6635
[2024-03-08 16:09:00,568][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0193 loss2:0.4338 loss3:0.3612 | AUC:0.8453 Anomaly AUC:0.6285
[2024-03-08 16:09:17,995][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0087 loss2:0.4416 loss3:0.3555 | AUC:0.8674 Anomaly AUC:0.6689
[2024-03-08 16:09:35,465][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.0128 loss2:0.4905 loss3:0.3555 | AUC:0.8689 Anomaly AUC:0.6732
[2024-03-08 16:09:53,196][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0128 loss2:0.4689 loss3:0.3567 | AUC:0.8586 Anomaly AUC:0.6570
[2024-03-08 16:10:10,741][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0161 loss2:0.4725 loss3:0.3611 | AUC:0.8420 Anomaly AUC:0.6298
[2024-03-08 16:10:28,208][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0123 loss2:0.4079 loss3:0.3602 | AUC:0.8615 Anomaly AUC:0.6713
[2024-03-08 16:10:45,661][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0133 loss2:0.4374 loss3:0.3553 | AUC:0.8497 Anomaly AUC:0.6376
[2024-03-08 16:11:03,268][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0153 loss2:0.3863 loss3:0.3582 | AUC:0.8522 Anomaly AUC:0.6370
[2024-03-08 16:11:19,227][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0153 loss2:0.3863 loss3:0.3582 | AUC:0.8523 Anomaly AUC:0.6372
[2024-03-08 16:11:19,250][main.py][line:171][INFO] Training completes in 38m 58s | best AUCAUC:0.8875 Anomaly AUC:0.7296

[2024-03-08 16:26:40,022][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__8968.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-08 16:26:44,077][main.py][line:259][INFO] total params:8.3485M
[2024-03-08 16:26:44,078][main.py][line:262][INFO] Training Mode
[2024-03-08 16:26:44,078][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-08 16:26:44,078][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-08 16:27:00,528][main.py][line:76][INFO] Random initialize AUCAUC:0.4872 Anomaly AUC:0.51080
[2024-03-08 16:28:00,576][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.3139 loss2:1.1129 loss3:0.5099 | AUC:0.8539 Anomaly AUC:0.6507
[2024-03-08 16:28:19,045][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.2765 loss2:1.1279 loss3:0.4952 | AUC:0.8584 Anomaly AUC:0.6607
[2024-03-08 16:28:37,249][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.2657 loss2:1.1189 loss3:0.4937 | AUC:0.8609 Anomaly AUC:0.6664
[2024-03-08 16:28:55,063][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.2467 loss2:1.0859 loss3:0.5087 | AUC:0.8606 Anomaly AUC:0.6683
[2024-03-08 16:29:12,631][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1861 loss2:1.0639 loss3:0.5228 | AUC:0.8604 Anomaly AUC:0.6677
[2024-03-08 16:29:30,574][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.2446 loss2:0.9635 loss3:0.5120 | AUC:0.8634 Anomaly AUC:0.6747
[2024-03-08 16:29:48,360][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.1521 loss2:0.9717 loss3:0.5379 | AUC:0.8535 Anomaly AUC:0.6690
[2024-03-08 16:30:06,368][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.2450 loss2:1.0606 loss3:0.5501 | AUC:0.8649 Anomaly AUC:0.6744
[2024-03-08 16:30:24,242][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.1303 loss2:0.9759 loss3:0.4978 | AUC:0.8641 Anomaly AUC:0.6661
[2024-03-08 16:30:42,048][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.2821 loss2:1.0005 loss3:0.4956 | AUC:0.8604 Anomaly AUC:0.6654
[2024-03-08 16:30:59,852][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.2182 loss2:0.8874 loss3:0.4924 | AUC:0.8565 Anomaly AUC:0.6658
[2024-03-08 16:31:17,712][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.2907 loss2:0.9340 loss3:0.4885 | AUC:0.8621 Anomaly AUC:0.6709
[2024-03-08 16:31:35,521][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.2370 loss2:0.9025 loss3:0.5072 | AUC:0.8654 Anomaly AUC:0.6762
[2024-03-08 16:31:53,105][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.3383 loss2:0.9319 loss3:0.4837 | AUC:0.8634 Anomaly AUC:0.6724
[2024-03-08 16:32:10,826][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.2335 loss2:0.8922 loss3:0.4936 | AUC:0.8638 Anomaly AUC:0.6673
[2024-03-08 16:32:28,512][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.1466 loss2:0.9430 loss3:0.4859 | AUC:0.8555 Anomaly AUC:0.6619
[2024-03-08 16:32:46,432][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.2381 loss2:0.9752 loss3:0.4808 | AUC:0.8625 Anomaly AUC:0.6696
[2024-03-08 16:33:08,173][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__8968.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-08 16:33:11,229][main.py][line:259][INFO] total params:8.3485M
[2024-03-08 16:33:11,229][main.py][line:262][INFO] Training Mode
[2024-03-08 16:33:11,229][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-08 16:33:11,229][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-08 16:33:29,036][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-08 16:34:35,315][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2292 loss2:0.8658 loss3:0.5250 | AUC:0.8703 Anomaly AUC:0.6929
[2024-03-08 16:34:55,138][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.1923 loss2:0.8876 loss3:0.4779 | AUC:0.8649 Anomaly AUC:0.6874
[2024-03-08 16:35:14,465][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1853 loss2:0.9184 loss3:0.5116 | AUC:0.8658 Anomaly AUC:0.6907
[2024-03-08 16:35:33,934][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1594 loss2:0.9077 loss3:0.4875 | AUC:0.8701 Anomaly AUC:0.6899
[2024-03-08 16:35:53,198][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1293 loss2:0.8975 loss3:0.4681 | AUC:0.8683 Anomaly AUC:0.6860
[2024-03-08 16:36:12,384][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.1705 loss2:0.8445 loss3:0.5018 | AUC:0.8708 Anomaly AUC:0.6931
[2024-03-08 16:36:31,327][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0687 loss2:0.8590 loss3:0.4880 | AUC:0.8665 Anomaly AUC:0.6854
[2024-03-08 16:36:50,514][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1815 loss2:0.9835 loss3:0.4692 | AUC:0.8682 Anomaly AUC:0.6868
[2024-03-08 16:37:09,690][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0654 loss2:0.8817 loss3:0.4646 | AUC:0.8697 Anomaly AUC:0.6867
[2024-03-08 16:37:28,834][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1423 loss2:0.8800 loss3:0.4642 | AUC:0.8668 Anomaly AUC:0.6844
[2024-03-08 16:37:47,907][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.1770 loss2:0.7918 loss3:0.4391 | AUC:0.8731 Anomaly AUC:0.6997
[2024-03-08 16:38:06,959][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1349 loss2:0.7857 loss3:0.4656 | AUC:0.8771 Anomaly AUC:0.7068
[2024-03-08 16:38:26,069][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.0727 loss2:0.8002 loss3:0.4776 | AUC:0.8757 Anomaly AUC:0.7028
[2024-03-08 16:38:45,308][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.3176 loss2:0.8524 loss3:0.4328 | AUC:0.8743 Anomaly AUC:0.6969
[2024-03-08 16:39:04,514][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.2219 loss2:0.8464 loss3:0.4645 | AUC:0.8754 Anomaly AUC:0.6980
[2024-03-08 16:39:23,714][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0950 loss2:0.8828 loss3:0.4480 | AUC:0.8715 Anomaly AUC:0.6872
[2024-03-08 16:39:42,945][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.2026 loss2:0.9135 loss3:0.4312 | AUC:0.8743 Anomaly AUC:0.6897
[2024-03-08 16:40:02,244][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.0987 loss2:0.8251 loss3:0.4379 | AUC:0.8817 Anomaly AUC:0.7056
[2024-03-08 16:40:21,360][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0444 loss2:0.7496 loss3:0.4463 | AUC:0.8807 Anomaly AUC:0.7081
[2024-03-08 16:40:40,747][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.1285 loss2:0.7996 loss3:0.4391 | AUC:0.8863 Anomaly AUC:0.7262
[2024-03-08 16:40:59,979][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1262 loss2:0.8307 loss3:0.4369 | AUC:0.8788 Anomaly AUC:0.7153
[2024-03-08 16:41:19,126][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0386 loss2:0.7383 loss3:0.4471 | AUC:0.8801 Anomaly AUC:0.7146
[2024-03-08 16:41:38,320][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.1337 loss2:0.9037 loss3:0.4224 | AUC:0.8845 Anomaly AUC:0.7116
[2024-03-08 16:41:57,608][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0700 loss2:0.8645 loss3:0.4369 | AUC:0.8796 Anomaly AUC:0.7042
[2024-03-08 16:42:16,918][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0783 loss2:0.7898 loss3:0.4390 | AUC:0.8815 Anomaly AUC:0.7118
[2024-03-08 16:42:36,256][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.0746 loss2:0.7716 loss3:0.4336 | AUC:0.8818 Anomaly AUC:0.7138
[2024-03-08 16:42:53,343][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.0746 loss2:0.7716 loss3:0.4336 | AUC:0.8819 Anomaly AUC:0.7140
[2024-03-08 16:43:12,894][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0615 loss2:0.7796 loss3:0.4218 | AUC:0.8842 Anomaly AUC:0.7222
[2024-03-08 16:43:31,854][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.1170 loss2:0.7607 loss3:0.4216 | AUC:0.8820 Anomaly AUC:0.7248
[2024-03-08 16:43:51,191][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1344 loss2:0.8445 loss3:0.4316 | AUC:0.8868 Anomaly AUC:0.7274
[2024-03-08 16:44:10,316][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0486 loss2:0.7680 loss3:0.4321 | AUC:0.8851 Anomaly AUC:0.7238
[2024-03-08 16:44:29,256][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.0377 loss2:0.7716 loss3:0.4157 | AUC:0.8848 Anomaly AUC:0.7212
[2024-03-08 16:44:48,354][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0414 loss2:0.7819 loss3:0.4118 | AUC:0.8880 Anomaly AUC:0.7294
[2024-03-08 16:45:07,695][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0412 loss2:0.7407 loss3:0.4088 | AUC:0.8868 Anomaly AUC:0.7310
[2024-03-08 16:45:26,994][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0365 loss2:0.7935 loss3:0.4167 | AUC:0.8827 Anomaly AUC:0.7186
[2024-03-08 16:45:46,157][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0521 loss2:0.7851 loss3:0.4124 | AUC:0.8827 Anomaly AUC:0.7196
[2024-03-08 16:46:05,285][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0348 loss2:0.7764 loss3:0.4063 | AUC:0.8877 Anomaly AUC:0.7297
[2024-03-08 16:46:24,439][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0248 loss2:0.7254 loss3:0.4180 | AUC:0.8895 Anomaly AUC:0.7266
[2024-03-08 16:46:43,618][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0316 loss2:0.7375 loss3:0.4060 | AUC:0.8818 Anomaly AUC:0.7110
[2024-03-08 16:47:02,958][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0176 loss2:0.7518 loss3:0.4061 | AUC:0.8793 Anomaly AUC:0.7085
[2024-03-08 16:47:22,164][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0430 loss2:0.7686 loss3:0.3981 | AUC:0.8867 Anomaly AUC:0.7255
[2024-03-08 16:47:41,229][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.0978 loss2:0.7692 loss3:0.4018 | AUC:0.8878 Anomaly AUC:0.7273
[2024-03-08 16:48:00,415][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0342 loss2:0.7855 loss3:0.4024 | AUC:0.8868 Anomaly AUC:0.7265
[2024-03-08 16:48:19,568][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0243 loss2:0.7036 loss3:0.4002 | AUC:0.8849 Anomaly AUC:0.7271
[2024-03-08 16:48:38,790][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0268 loss2:0.6870 loss3:0.3990 | AUC:0.8861 Anomaly AUC:0.7269
[2024-03-08 16:48:58,124][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0212 loss2:0.6586 loss3:0.3944 | AUC:0.8862 Anomaly AUC:0.7235
[2024-03-08 16:49:17,393][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1104 loss2:0.7587 loss3:0.3991 | AUC:0.8845 Anomaly AUC:0.7190
[2024-03-08 16:49:36,408][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0161 loss2:0.6455 loss3:0.3949 | AUC:0.8872 Anomaly AUC:0.7250
[2024-03-08 16:49:55,377][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.1346 loss2:0.7814 loss3:0.3942 | AUC:0.8884 Anomaly AUC:0.7280
[2024-03-08 16:50:14,642][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0722 loss2:0.6252 loss3:0.3889 | AUC:0.8849 Anomaly AUC:0.7153
[2024-03-08 16:50:33,794][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0131 loss2:0.6913 loss3:0.3957 | AUC:0.8829 Anomaly AUC:0.7091
[2024-03-08 16:50:52,998][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0188 loss2:0.6429 loss3:0.3910 | AUC:0.8866 Anomaly AUC:0.7232
[2024-03-08 16:51:10,277][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0188 loss2:0.6429 loss3:0.3910 | AUC:0.8866 Anomaly AUC:0.7232
[2024-03-08 16:51:29,704][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0495 loss2:0.7940 loss3:0.3827 | AUC:0.8804 Anomaly AUC:0.7176
[2024-03-08 16:51:48,759][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0288 loss2:0.6921 loss3:0.3885 | AUC:0.8860 Anomaly AUC:0.7275
[2024-03-08 16:52:07,805][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0264 loss2:0.6826 loss3:0.3936 | AUC:0.8809 Anomaly AUC:0.7206
[2024-03-08 16:52:26,976][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0172 loss2:0.4791 loss3:0.3932 | AUC:0.8814 Anomaly AUC:0.7166
[2024-03-08 16:52:46,283][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0320 loss2:0.6111 loss3:0.3875 | AUC:0.8825 Anomaly AUC:0.7184
[2024-03-08 16:53:05,381][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0401 loss2:0.7602 loss3:0.3849 | AUC:0.8770 Anomaly AUC:0.7107
[2024-03-08 16:53:24,678][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0383 loss2:0.5954 loss3:0.3879 | AUC:0.8666 Anomaly AUC:0.6950
[2024-03-08 16:53:43,924][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0277 loss2:0.6490 loss3:0.3855 | AUC:0.8655 Anomaly AUC:0.6972
[2024-03-08 16:54:02,979][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0523 loss2:0.6053 loss3:0.3802 | AUC:0.8669 Anomaly AUC:0.6993
[2024-03-08 16:54:22,213][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0118 loss2:0.6711 loss3:0.3810 | AUC:0.8764 Anomaly AUC:0.7154
[2024-03-08 16:54:41,390][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.0328 loss2:0.5803 loss3:0.3824 | AUC:0.8731 Anomaly AUC:0.7011
[2024-03-08 16:55:00,684][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0158 loss2:0.6808 loss3:0.3780 | AUC:0.8714 Anomaly AUC:0.6934
[2024-03-08 16:55:19,813][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.0260 loss2:0.5952 loss3:0.3848 | AUC:0.8746 Anomaly AUC:0.6927
[2024-03-08 16:55:38,897][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0648 loss2:0.6013 loss3:0.3753 | AUC:0.8763 Anomaly AUC:0.6936
[2024-03-08 16:55:58,194][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0470 loss2:0.6037 loss3:0.3763 | AUC:0.8767 Anomaly AUC:0.6976
[2024-03-08 16:56:17,310][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0908 loss2:0.6389 loss3:0.3729 | AUC:0.8682 Anomaly AUC:0.6867
[2024-03-08 16:56:36,445][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.2260 loss2:0.7684 loss3:0.3741 | AUC:0.8736 Anomaly AUC:0.6925
[2024-03-08 16:56:55,461][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0331 loss2:0.6313 loss3:0.3754 | AUC:0.8764 Anomaly AUC:0.6936
[2024-03-08 16:57:14,421][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0297 loss2:0.6522 loss3:0.3783 | AUC:0.8718 Anomaly AUC:0.6873
[2024-03-08 16:57:33,625][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0151 loss2:0.6384 loss3:0.3800 | AUC:0.8769 Anomaly AUC:0.7020
[2024-03-08 16:57:52,875][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.0167 loss2:0.6080 loss3:0.3729 | AUC:0.8799 Anomaly AUC:0.7086
[2024-03-08 16:58:12,097][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0191 loss2:0.5843 loss3:0.3746 | AUC:0.8764 Anomaly AUC:0.6954
[2024-03-08 16:58:31,351][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0183 loss2:0.5753 loss3:0.3705 | AUC:0.8775 Anomaly AUC:0.6978
[2024-03-08 16:58:50,369][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0169 loss2:0.6433 loss3:0.3718 | AUC:0.8600 Anomaly AUC:0.6598
[2024-03-08 16:59:09,461][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0176 loss2:0.6570 loss3:0.3667 | AUC:0.8678 Anomaly AUC:0.6776
[2024-03-08 16:59:26,808][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0176 loss2:0.6570 loss3:0.3667 | AUC:0.8678 Anomaly AUC:0.6775
[2024-03-08 16:59:46,183][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.0213 loss2:0.5779 loss3:0.3679 | AUC:0.8743 Anomaly AUC:0.6900
[2024-03-08 17:00:05,350][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0396 loss2:0.6052 loss3:0.3697 | AUC:0.8542 Anomaly AUC:0.6452
[2024-03-08 17:00:24,533][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.0140 loss2:0.6066 loss3:0.3685 | AUC:0.8581 Anomaly AUC:0.6457
[2024-03-08 17:00:43,664][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0203 loss2:0.4868 loss3:0.3700 | AUC:0.8613 Anomaly AUC:0.6540
[2024-03-08 17:01:02,887][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.0321 loss2:0.6351 loss3:0.3638 | AUC:0.8711 Anomaly AUC:0.6806
[2024-03-08 17:01:22,276][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.0091 loss2:0.6027 loss3:0.3736 | AUC:0.8747 Anomaly AUC:0.6931
[2024-03-08 17:01:41,230][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.0106 loss2:0.5790 loss3:0.3687 | AUC:0.8727 Anomaly AUC:0.6947
[2024-03-08 17:02:00,551][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0271 loss2:0.6188 loss3:0.3724 | AUC:0.8707 Anomaly AUC:0.6943
[2024-03-08 17:02:19,574][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0101 loss2:0.5668 loss3:0.3667 | AUC:0.8798 Anomaly AUC:0.7095
[2024-03-08 17:02:38,753][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.0161 loss2:0.6026 loss3:0.3633 | AUC:0.8724 Anomaly AUC:0.6887
[2024-03-08 17:02:57,733][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.0133 loss2:0.5333 loss3:0.3621 | AUC:0.8771 Anomaly AUC:0.6988
[2024-03-08 17:03:16,578][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.1058 loss2:0.5536 loss3:0.3706 | AUC:0.8468 Anomaly AUC:0.6282
[2024-03-08 17:03:35,910][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0118 loss2:0.6518 loss3:0.3677 | AUC:0.8699 Anomaly AUC:0.6844
[2024-03-08 17:03:55,025][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0115 loss2:0.5587 loss3:0.3766 | AUC:0.8587 Anomaly AUC:0.6596
[2024-03-08 17:04:14,210][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.0114 loss2:0.5017 loss3:0.3690 | AUC:0.8540 Anomaly AUC:0.6428
[2024-03-08 17:04:33,369][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.0150 loss2:0.5195 loss3:0.3680 | AUC:0.8636 Anomaly AUC:0.6679
[2024-03-08 17:04:52,445][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0259 loss2:0.5646 loss3:0.3630 | AUC:0.8584 Anomaly AUC:0.6599
[2024-03-08 17:05:11,674][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.1491 loss2:0.6669 loss3:0.3625 | AUC:0.8670 Anomaly AUC:0.6790
[2024-03-08 17:05:30,650][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0196 loss2:0.5526 loss3:0.3655 | AUC:0.8656 Anomaly AUC:0.6724
[2024-03-08 17:05:49,697][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0134 loss2:0.5176 loss3:0.3606 | AUC:0.8645 Anomaly AUC:0.6696
[2024-03-08 17:06:08,741][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0162 loss2:0.4766 loss3:0.3615 | AUC:0.8644 Anomaly AUC:0.6771
[2024-03-08 17:06:27,914][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0272 loss2:0.5516 loss3:0.3686 | AUC:0.8457 Anomaly AUC:0.6323
[2024-03-08 17:06:47,057][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0113 loss2:0.4875 loss3:0.3626 | AUC:0.8684 Anomaly AUC:0.6856
[2024-03-08 17:07:06,096][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.0159 loss2:0.4212 loss3:0.3639 | AUC:0.8552 Anomaly AUC:0.6520
[2024-03-08 17:07:24,962][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0114 loss2:0.5021 loss3:0.3625 | AUC:0.8619 Anomaly AUC:0.6662
[2024-03-08 17:07:42,162][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0114 loss2:0.5021 loss3:0.3625 | AUC:0.8619 Anomaly AUC:0.6661
[2024-03-08 17:08:01,423][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0128 loss2:0.4284 loss3:0.3644 | AUC:0.8578 Anomaly AUC:0.6554
[2024-03-08 17:08:20,509][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.0151 loss2:0.4760 loss3:0.3604 | AUC:0.8716 Anomaly AUC:0.6879
[2024-03-08 17:08:39,617][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0210 loss2:0.5163 loss3:0.3652 | AUC:0.8441 Anomaly AUC:0.6142
[2024-03-08 17:08:58,621][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0334 loss2:0.5165 loss3:0.3635 | AUC:0.8575 Anomaly AUC:0.6528
[2024-03-08 17:09:17,791][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0316 loss2:0.4796 loss3:0.3650 | AUC:0.8568 Anomaly AUC:0.6554
[2024-03-08 17:09:36,894][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0145 loss2:0.5372 loss3:0.3610 | AUC:0.8557 Anomaly AUC:0.6519
[2024-03-08 17:09:55,837][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0137 loss2:0.5024 loss3:0.3588 | AUC:0.8636 Anomaly AUC:0.6722
[2024-03-08 17:10:15,239][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0089 loss2:0.5033 loss3:0.3620 | AUC:0.8551 Anomaly AUC:0.6439
[2024-03-08 17:10:34,078][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0128 loss2:0.5580 loss3:0.3556 | AUC:0.8675 Anomaly AUC:0.6762
[2024-03-08 17:10:53,109][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0138 loss2:0.4711 loss3:0.3624 | AUC:0.8310 Anomaly AUC:0.5874
[2024-03-08 17:11:12,015][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0170 loss2:0.5299 loss3:0.3594 | AUC:0.8608 Anomaly AUC:0.6613
[2024-03-08 17:11:31,363][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0100 loss2:0.4464 loss3:0.3610 | AUC:0.8571 Anomaly AUC:0.6479
[2024-03-08 17:11:50,436][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0097 loss2:0.3936 loss3:0.3591 | AUC:0.8704 Anomaly AUC:0.6928
[2024-03-08 17:12:09,548][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0134 loss2:0.5001 loss3:0.3594 | AUC:0.8460 Anomaly AUC:0.6261
[2024-03-08 17:12:28,543][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0131 loss2:0.4306 loss3:0.3592 | AUC:0.8598 Anomaly AUC:0.6555
[2024-03-08 17:12:47,857][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0121 loss2:0.4406 loss3:0.3561 | AUC:0.8669 Anomaly AUC:0.6806
[2024-03-08 17:13:07,006][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0139 loss2:0.3601 loss3:0.3584 | AUC:0.8484 Anomaly AUC:0.6272
[2024-03-08 17:13:26,022][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0118 loss2:0.4325 loss3:0.3564 | AUC:0.8585 Anomaly AUC:0.6518
[2024-03-08 17:13:45,338][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0096 loss2:0.4389 loss3:0.3549 | AUC:0.8601 Anomaly AUC:0.6554
[2024-03-08 17:14:04,440][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.0106 loss2:0.4762 loss3:0.3558 | AUC:0.8609 Anomaly AUC:0.6593
[2024-03-08 17:14:23,351][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0142 loss2:0.4608 loss3:0.3560 | AUC:0.8580 Anomaly AUC:0.6504
[2024-03-08 17:14:42,471][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0107 loss2:0.4688 loss3:0.3591 | AUC:0.8435 Anomaly AUC:0.6194
[2024-03-08 17:15:01,487][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0163 loss2:0.3906 loss3:0.3616 | AUC:0.8463 Anomaly AUC:0.6295
[2024-03-08 17:15:20,598][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0149 loss2:0.4515 loss3:0.3553 | AUC:0.8546 Anomaly AUC:0.6505
[2024-03-08 17:15:39,495][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0116 loss2:0.3787 loss3:0.3586 | AUC:0.8442 Anomaly AUC:0.6243
[2024-03-08 17:15:56,597][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0116 loss2:0.3787 loss3:0.3586 | AUC:0.8443 Anomaly AUC:0.6246
[2024-03-08 17:15:56,619][main.py][line:171][INFO] Training completes in 42m 28s | best AUCAUC:0.8895 Anomaly AUC:0.7266

[2024-03-09 01:46:43,585][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__8968.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 01:46:43,769][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 01:46:43,769][main.py][line:268][INFO] Test Mode
[2024-03-09 01:46:43,769][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/ucf__8968.pkl.
[2024-03-09 01:47:01,926][infer.py][line:94][INFO] offline AUC:0.9005 Anomaly-AUC:0.7507 AP:0.3884 FAR:0.0101 | Complete in 0m 18s

[2024-03-09 01:47:59,482][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 01:47:59,670][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 01:47:59,670][main.py][line:268][INFO] Test Mode
[2024-03-09 01:47:59,670][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/ucf__current.pkl.
[2024-03-09 01:48:17,806][infer.py][line:94][INFO] offline AUC:0.8933 Anomaly-AUC:0.7339 AP:0.4013 FAR:0.0165 | Complete in 0m 18s

[2024-03-09 01:50:58,158][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 01:50:58,343][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 01:50:58,343][main.py][line:268][INFO] Test Mode
[2024-03-09 01:50:58,343][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/ucf__current.pkl.
[2024-03-09 01:51:14,324][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 01:51:17,464][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 01:51:17,464][main.py][line:262][INFO] Training Mode
[2024-03-09 01:51:17,464][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 01:51:17,465][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 01:51:35,081][main.py][line:76][INFO] Random initialize AUCAUC:0.5662 Anomaly AUC:0.52848
[2024-03-09 01:52:40,963][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.0444 loss2:0.7821 loss3:0.3868 | AUC:0.7746 Anomaly AUC:0.6217
[2024-03-09 01:53:00,588][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.0417 loss2:0.8282 loss3:0.3864 | AUC:0.7730 Anomaly AUC:0.6206
[2024-03-09 01:53:19,845][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.0353 loss2:0.8654 loss3:0.3861 | AUC:0.7733 Anomaly AUC:0.6193
[2024-03-09 01:53:38,950][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.0393 loss2:0.8395 loss3:0.3857 | AUC:0.7755 Anomaly AUC:0.6236
[2024-03-09 01:53:58,042][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.0329 loss2:0.8665 loss3:0.3857 | AUC:0.7747 Anomaly AUC:0.6254
[2024-03-09 01:54:16,861][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.0339 loss2:0.7728 loss3:0.3856 | AUC:0.7752 Anomaly AUC:0.6261
[2024-03-09 01:54:35,893][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0375 loss2:0.8619 loss3:0.3855 | AUC:0.7775 Anomaly AUC:0.6264
[2024-03-09 01:54:54,951][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.0386 loss2:0.8999 loss3:0.3849 | AUC:0.7781 Anomaly AUC:0.6283
[2024-03-09 01:55:14,041][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0270 loss2:0.8581 loss3:0.3852 | AUC:0.7789 Anomaly AUC:0.6308
[2024-03-09 01:55:33,219][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.0274 loss2:0.8320 loss3:0.3848 | AUC:0.7817 Anomaly AUC:0.6342
[2024-03-09 01:55:52,498][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.0534 loss2:0.7456 loss3:0.3847 | AUC:0.7815 Anomaly AUC:0.6356
[2024-03-09 01:56:11,496][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.0292 loss2:0.7537 loss3:0.3843 | AUC:0.7773 Anomaly AUC:0.6316
[2024-03-09 01:56:30,438][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.0274 loss2:0.7616 loss3:0.3849 | AUC:0.7804 Anomaly AUC:0.6367
[2024-03-09 01:56:49,713][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.0246 loss2:0.7550 loss3:0.3841 | AUC:0.7808 Anomaly AUC:0.6392
[2024-03-09 01:57:08,945][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.0344 loss2:0.7806 loss3:0.3841 | AUC:0.7837 Anomaly AUC:0.6414
[2024-03-09 01:57:28,055][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0242 loss2:0.8544 loss3:0.3842 | AUC:0.7831 Anomaly AUC:0.6380
[2024-03-09 01:57:47,210][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.0210 loss2:0.8166 loss3:0.3842 | AUC:0.7852 Anomaly AUC:0.6433
[2024-03-09 01:58:06,138][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.0238 loss2:0.7942 loss3:0.3838 | AUC:0.7851 Anomaly AUC:0.6464
[2024-03-09 01:58:25,302][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0218 loss2:0.7496 loss3:0.3840 | AUC:0.7831 Anomaly AUC:0.6429
[2024-03-09 01:58:44,364][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.0233 loss2:0.7288 loss3:0.3839 | AUC:0.7842 Anomaly AUC:0.6436
[2024-03-09 01:59:03,502][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.0202 loss2:0.8150 loss3:0.3836 | AUC:0.7839 Anomaly AUC:0.6428
[2024-03-09 01:59:22,534][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0206 loss2:0.7395 loss3:0.3832 | AUC:0.7852 Anomaly AUC:0.6461
[2024-03-09 01:59:41,596][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.0195 loss2:0.8476 loss3:0.3837 | AUC:0.7860 Anomaly AUC:0.6469
[2024-03-09 02:00:00,778][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0190 loss2:0.8350 loss3:0.3836 | AUC:0.7879 Anomaly AUC:0.6482
[2024-03-09 02:00:19,882][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0180 loss2:0.7729 loss3:0.3837 | AUC:0.7875 Anomaly AUC:0.6482
[2024-03-09 02:00:39,046][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.0190 loss2:0.7665 loss3:0.3833 | AUC:0.7891 Anomaly AUC:0.6518
[2024-03-09 02:00:58,103][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 02:01:01,173][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 02:01:01,174][main.py][line:262][INFO] Training Mode
[2024-03-09 02:01:01,174][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 02:01:01,174][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 02:01:18,935][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-09 02:02:24,919][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2292 loss2:0.8658 loss3:0.5250 | AUC:0.8703 Anomaly AUC:0.6929
[2024-03-09 02:02:44,754][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.1923 loss2:0.8876 loss3:0.4779 | AUC:0.8649 Anomaly AUC:0.6874
[2024-03-09 02:03:03,961][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1853 loss2:0.9184 loss3:0.5116 | AUC:0.8658 Anomaly AUC:0.6907
[2024-03-09 02:03:22,746][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1594 loss2:0.9077 loss3:0.4875 | AUC:0.8701 Anomaly AUC:0.6899
[2024-03-09 02:03:41,766][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1293 loss2:0.8975 loss3:0.4681 | AUC:0.8683 Anomaly AUC:0.6860
[2024-03-09 02:04:00,830][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.1705 loss2:0.8445 loss3:0.5018 | AUC:0.8708 Anomaly AUC:0.6931
[2024-03-09 02:04:20,009][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0687 loss2:0.8590 loss3:0.4880 | AUC:0.8665 Anomaly AUC:0.6854
[2024-03-09 02:04:38,973][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1815 loss2:0.9835 loss3:0.4692 | AUC:0.8682 Anomaly AUC:0.6868
[2024-03-09 02:04:58,029][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0654 loss2:0.8817 loss3:0.4646 | AUC:0.8697 Anomaly AUC:0.6867
[2024-03-09 02:05:17,140][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1423 loss2:0.8800 loss3:0.4642 | AUC:0.8668 Anomaly AUC:0.6844
[2024-03-09 02:05:36,260][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.1770 loss2:0.7918 loss3:0.4391 | AUC:0.8731 Anomaly AUC:0.6997
[2024-03-09 02:05:55,124][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1349 loss2:0.7857 loss3:0.4656 | AUC:0.8771 Anomaly AUC:0.7068
[2024-03-09 02:06:14,194][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.0727 loss2:0.8002 loss3:0.4776 | AUC:0.8757 Anomaly AUC:0.7028
[2024-03-09 02:06:33,258][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.3176 loss2:0.8524 loss3:0.4328 | AUC:0.8743 Anomaly AUC:0.6969
[2024-03-09 02:06:52,488][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.2219 loss2:0.8464 loss3:0.4645 | AUC:0.8754 Anomaly AUC:0.6980
[2024-03-09 02:07:11,518][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0950 loss2:0.8828 loss3:0.4480 | AUC:0.8715 Anomaly AUC:0.6872
[2024-03-09 02:07:30,756][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.2026 loss2:0.9135 loss3:0.4312 | AUC:0.8743 Anomaly AUC:0.6897
[2024-03-09 02:07:50,138][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.0987 loss2:0.8251 loss3:0.4379 | AUC:0.8817 Anomaly AUC:0.7056
[2024-03-09 02:08:09,263][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0444 loss2:0.7496 loss3:0.4463 | AUC:0.8807 Anomaly AUC:0.7081
[2024-03-09 02:08:28,535][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.1285 loss2:0.7996 loss3:0.4391 | AUC:0.8863 Anomaly AUC:0.7262
[2024-03-09 02:08:47,396][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1262 loss2:0.8307 loss3:0.4369 | AUC:0.8788 Anomaly AUC:0.7153
[2024-03-09 02:09:06,394][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0386 loss2:0.7383 loss3:0.4471 | AUC:0.8801 Anomaly AUC:0.7146
[2024-03-09 02:09:25,534][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.1337 loss2:0.9037 loss3:0.4224 | AUC:0.8845 Anomaly AUC:0.7116
[2024-03-09 02:09:44,638][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0700 loss2:0.8645 loss3:0.4369 | AUC:0.8796 Anomaly AUC:0.7042
[2024-03-09 02:10:03,634][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0783 loss2:0.7898 loss3:0.4390 | AUC:0.8815 Anomaly AUC:0.7118
[2024-03-09 02:10:22,765][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.0746 loss2:0.7716 loss3:0.4336 | AUC:0.8818 Anomaly AUC:0.7138
[2024-03-09 02:10:39,965][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.0746 loss2:0.7716 loss3:0.4336 | AUC:0.8819 Anomaly AUC:0.7140
[2024-03-09 02:10:59,510][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0615 loss2:0.7796 loss3:0.4218 | AUC:0.8842 Anomaly AUC:0.7222
[2024-03-09 02:11:18,880][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.1170 loss2:0.7607 loss3:0.4216 | AUC:0.8820 Anomaly AUC:0.7248
[2024-03-09 02:11:37,860][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1344 loss2:0.8445 loss3:0.4316 | AUC:0.8868 Anomaly AUC:0.7274
[2024-03-09 02:11:56,896][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0486 loss2:0.7680 loss3:0.4321 | AUC:0.8851 Anomaly AUC:0.7238
[2024-03-09 02:12:15,949][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.0377 loss2:0.7716 loss3:0.4157 | AUC:0.8848 Anomaly AUC:0.7212
[2024-03-09 02:12:34,941][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0414 loss2:0.7819 loss3:0.4118 | AUC:0.8880 Anomaly AUC:0.7294
[2024-03-09 02:12:54,053][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0412 loss2:0.7407 loss3:0.4088 | AUC:0.8868 Anomaly AUC:0.7310
[2024-03-09 02:13:13,029][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0365 loss2:0.7935 loss3:0.4167 | AUC:0.8827 Anomaly AUC:0.7186
[2024-03-09 02:13:31,996][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0521 loss2:0.7851 loss3:0.4124 | AUC:0.8827 Anomaly AUC:0.7196
[2024-03-09 02:13:51,333][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0348 loss2:0.7764 loss3:0.4063 | AUC:0.8877 Anomaly AUC:0.7297
[2024-03-09 02:14:10,310][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0248 loss2:0.7254 loss3:0.4180 | AUC:0.8895 Anomaly AUC:0.7266
[2024-03-09 02:14:29,417][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0316 loss2:0.7375 loss3:0.4060 | AUC:0.8818 Anomaly AUC:0.7110
[2024-03-09 02:14:48,429][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0176 loss2:0.7518 loss3:0.4061 | AUC:0.8793 Anomaly AUC:0.7085
[2024-03-09 02:15:07,506][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0430 loss2:0.7686 loss3:0.3981 | AUC:0.8867 Anomaly AUC:0.7255
[2024-03-09 02:15:26,595][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.0978 loss2:0.7692 loss3:0.4018 | AUC:0.8878 Anomaly AUC:0.7273
[2024-03-09 02:15:45,660][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0342 loss2:0.7855 loss3:0.4024 | AUC:0.8868 Anomaly AUC:0.7265
[2024-03-09 02:16:04,425][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0243 loss2:0.7036 loss3:0.4002 | AUC:0.8849 Anomaly AUC:0.7271
[2024-03-09 02:16:23,430][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0268 loss2:0.6870 loss3:0.3990 | AUC:0.8861 Anomaly AUC:0.7269
[2024-03-09 02:16:42,165][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0212 loss2:0.6586 loss3:0.3944 | AUC:0.8862 Anomaly AUC:0.7235
[2024-03-09 02:17:00,896][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1104 loss2:0.7587 loss3:0.3991 | AUC:0.8845 Anomaly AUC:0.7190
[2024-03-09 02:17:19,687][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0161 loss2:0.6455 loss3:0.3949 | AUC:0.8872 Anomaly AUC:0.7250
[2024-03-09 02:17:38,846][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.1346 loss2:0.7814 loss3:0.3942 | AUC:0.8884 Anomaly AUC:0.7280
[2024-03-09 02:17:57,936][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0722 loss2:0.6252 loss3:0.3889 | AUC:0.8849 Anomaly AUC:0.7153
[2024-03-09 02:18:16,814][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0131 loss2:0.6913 loss3:0.3957 | AUC:0.8829 Anomaly AUC:0.7091
[2024-03-09 02:18:35,723][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0188 loss2:0.6429 loss3:0.3910 | AUC:0.8866 Anomaly AUC:0.7232
[2024-03-09 02:18:53,124][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0188 loss2:0.6429 loss3:0.3910 | AUC:0.8866 Anomaly AUC:0.7232
[2024-03-09 02:19:12,433][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0495 loss2:0.7940 loss3:0.3827 | AUC:0.8804 Anomaly AUC:0.7176
[2024-03-09 02:19:31,443][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0288 loss2:0.6921 loss3:0.3885 | AUC:0.8860 Anomaly AUC:0.7275
[2024-03-09 02:19:50,504][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0264 loss2:0.6826 loss3:0.3936 | AUC:0.8809 Anomaly AUC:0.7206
[2024-03-09 02:20:09,583][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0172 loss2:0.4791 loss3:0.3932 | AUC:0.8814 Anomaly AUC:0.7166
[2024-03-09 02:20:28,554][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0320 loss2:0.6111 loss3:0.3875 | AUC:0.8825 Anomaly AUC:0.7184
[2024-03-09 02:20:47,417][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0401 loss2:0.7602 loss3:0.3849 | AUC:0.8770 Anomaly AUC:0.7107
[2024-03-09 02:21:06,542][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0383 loss2:0.5954 loss3:0.3879 | AUC:0.8666 Anomaly AUC:0.6950
[2024-03-09 02:21:25,614][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0277 loss2:0.6490 loss3:0.3855 | AUC:0.8655 Anomaly AUC:0.6972
[2024-03-09 02:21:44,707][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0523 loss2:0.6053 loss3:0.3802 | AUC:0.8669 Anomaly AUC:0.6993
[2024-03-09 02:22:03,795][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0118 loss2:0.6711 loss3:0.3810 | AUC:0.8764 Anomaly AUC:0.7154
[2024-03-09 02:22:22,566][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.0328 loss2:0.5803 loss3:0.3824 | AUC:0.8731 Anomaly AUC:0.7011
[2024-03-09 02:22:41,582][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0158 loss2:0.6808 loss3:0.3780 | AUC:0.8714 Anomaly AUC:0.6934
[2024-03-09 02:23:00,590][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.0260 loss2:0.5952 loss3:0.3848 | AUC:0.8746 Anomaly AUC:0.6927
[2024-03-09 02:23:19,348][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0648 loss2:0.6013 loss3:0.3753 | AUC:0.8763 Anomaly AUC:0.6936
[2024-03-09 02:23:38,399][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0470 loss2:0.6037 loss3:0.3763 | AUC:0.8767 Anomaly AUC:0.6976
[2024-03-09 02:23:57,412][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0908 loss2:0.6389 loss3:0.3729 | AUC:0.8682 Anomaly AUC:0.6867
[2024-03-09 02:24:16,372][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.2260 loss2:0.7684 loss3:0.3741 | AUC:0.8736 Anomaly AUC:0.6925
[2024-03-09 02:24:35,404][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0331 loss2:0.6313 loss3:0.3754 | AUC:0.8764 Anomaly AUC:0.6936
[2024-03-09 02:24:54,534][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0297 loss2:0.6522 loss3:0.3783 | AUC:0.8718 Anomaly AUC:0.6873
[2024-03-09 02:25:13,506][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0151 loss2:0.6384 loss3:0.3800 | AUC:0.8769 Anomaly AUC:0.7020
[2024-03-09 02:25:32,582][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.0167 loss2:0.6080 loss3:0.3729 | AUC:0.8799 Anomaly AUC:0.7086
[2024-03-09 02:25:51,236][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0191 loss2:0.5843 loss3:0.3746 | AUC:0.8764 Anomaly AUC:0.6954
[2024-03-09 02:26:10,165][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0183 loss2:0.5753 loss3:0.3705 | AUC:0.8775 Anomaly AUC:0.6978
[2024-03-09 02:26:29,132][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0169 loss2:0.6433 loss3:0.3718 | AUC:0.8600 Anomaly AUC:0.6598
[2024-03-09 02:26:48,041][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0176 loss2:0.6570 loss3:0.3667 | AUC:0.8678 Anomaly AUC:0.6776
[2024-03-09 02:27:05,030][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0176 loss2:0.6570 loss3:0.3667 | AUC:0.8678 Anomaly AUC:0.6775
[2024-03-09 02:27:24,429][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.0213 loss2:0.5779 loss3:0.3679 | AUC:0.8743 Anomaly AUC:0.6900
[2024-03-09 02:27:43,437][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0396 loss2:0.6052 loss3:0.3697 | AUC:0.8542 Anomaly AUC:0.6452
[2024-03-09 02:28:02,486][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.0140 loss2:0.6066 loss3:0.3685 | AUC:0.8581 Anomaly AUC:0.6457
[2024-03-09 02:28:21,527][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0203 loss2:0.4868 loss3:0.3700 | AUC:0.8613 Anomaly AUC:0.6540
[2024-03-09 02:28:40,538][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.0321 loss2:0.6351 loss3:0.3638 | AUC:0.8711 Anomaly AUC:0.6806
[2024-03-09 02:28:59,590][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.0091 loss2:0.6027 loss3:0.3736 | AUC:0.8747 Anomaly AUC:0.6931
[2024-03-09 02:29:18,563][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.0106 loss2:0.5790 loss3:0.3687 | AUC:0.8727 Anomaly AUC:0.6947
[2024-03-09 02:29:37,497][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0271 loss2:0.6188 loss3:0.3724 | AUC:0.8707 Anomaly AUC:0.6943
[2024-03-09 02:29:56,592][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0101 loss2:0.5668 loss3:0.3667 | AUC:0.8798 Anomaly AUC:0.7095
[2024-03-09 02:30:15,424][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.0161 loss2:0.6026 loss3:0.3633 | AUC:0.8724 Anomaly AUC:0.6887
[2024-03-09 02:30:34,544][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.0133 loss2:0.5333 loss3:0.3621 | AUC:0.8771 Anomaly AUC:0.6988
[2024-03-09 02:30:53,503][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.1058 loss2:0.5536 loss3:0.3706 | AUC:0.8468 Anomaly AUC:0.6282
[2024-03-09 02:31:12,541][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0118 loss2:0.6518 loss3:0.3677 | AUC:0.8699 Anomaly AUC:0.6844
[2024-03-09 02:31:31,441][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0115 loss2:0.5587 loss3:0.3766 | AUC:0.8587 Anomaly AUC:0.6596
[2024-03-09 02:31:50,436][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.0114 loss2:0.5017 loss3:0.3690 | AUC:0.8540 Anomaly AUC:0.6428
[2024-03-09 02:32:09,333][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.0150 loss2:0.5195 loss3:0.3680 | AUC:0.8636 Anomaly AUC:0.6679
[2024-03-09 02:32:28,282][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0259 loss2:0.5646 loss3:0.3630 | AUC:0.8584 Anomaly AUC:0.6599
[2024-03-09 02:32:47,247][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.1491 loss2:0.6669 loss3:0.3625 | AUC:0.8670 Anomaly AUC:0.6790
[2024-03-09 02:33:06,372][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0196 loss2:0.5526 loss3:0.3655 | AUC:0.8656 Anomaly AUC:0.6724
[2024-03-09 02:33:25,459][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0134 loss2:0.5176 loss3:0.3606 | AUC:0.8645 Anomaly AUC:0.6696
[2024-03-09 02:33:44,564][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0162 loss2:0.4766 loss3:0.3615 | AUC:0.8644 Anomaly AUC:0.6771
[2024-03-09 02:34:03,427][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0272 loss2:0.5516 loss3:0.3686 | AUC:0.8457 Anomaly AUC:0.6323
[2024-03-09 02:34:22,667][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0113 loss2:0.4875 loss3:0.3626 | AUC:0.8684 Anomaly AUC:0.6856
[2024-03-09 02:34:41,825][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.0159 loss2:0.4212 loss3:0.3639 | AUC:0.8552 Anomaly AUC:0.6520
[2024-03-09 02:35:00,762][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0114 loss2:0.5021 loss3:0.3625 | AUC:0.8619 Anomaly AUC:0.6662
[2024-03-09 02:35:17,955][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0114 loss2:0.5021 loss3:0.3625 | AUC:0.8619 Anomaly AUC:0.6661
[2024-03-09 02:35:37,300][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0128 loss2:0.4284 loss3:0.3644 | AUC:0.8578 Anomaly AUC:0.6554
[2024-03-09 02:35:56,301][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.0151 loss2:0.4760 loss3:0.3604 | AUC:0.8716 Anomaly AUC:0.6879
[2024-03-09 02:36:15,286][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0210 loss2:0.5163 loss3:0.3652 | AUC:0.8441 Anomaly AUC:0.6142
[2024-03-09 02:36:34,015][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0334 loss2:0.5165 loss3:0.3635 | AUC:0.8575 Anomaly AUC:0.6528
[2024-03-09 02:36:53,086][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0316 loss2:0.4796 loss3:0.3650 | AUC:0.8568 Anomaly AUC:0.6554
[2024-03-09 02:37:12,253][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0145 loss2:0.5372 loss3:0.3610 | AUC:0.8557 Anomaly AUC:0.6519
[2024-03-09 02:37:31,355][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0137 loss2:0.5024 loss3:0.3588 | AUC:0.8636 Anomaly AUC:0.6722
[2024-03-09 02:37:50,475][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0089 loss2:0.5033 loss3:0.3620 | AUC:0.8551 Anomaly AUC:0.6439
[2024-03-09 02:38:09,606][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0128 loss2:0.5580 loss3:0.3556 | AUC:0.8675 Anomaly AUC:0.6762
[2024-03-09 02:38:28,561][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0138 loss2:0.4711 loss3:0.3624 | AUC:0.8310 Anomaly AUC:0.5874
[2024-03-09 02:38:47,525][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0170 loss2:0.5299 loss3:0.3594 | AUC:0.8608 Anomaly AUC:0.6613
[2024-03-09 02:39:06,525][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0100 loss2:0.4464 loss3:0.3610 | AUC:0.8571 Anomaly AUC:0.6479
[2024-03-09 02:39:25,581][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0097 loss2:0.3936 loss3:0.3591 | AUC:0.8704 Anomaly AUC:0.6928
[2024-03-09 02:39:44,630][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0134 loss2:0.5001 loss3:0.3594 | AUC:0.8460 Anomaly AUC:0.6261
[2024-03-09 02:40:03,801][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0131 loss2:0.4306 loss3:0.3592 | AUC:0.8598 Anomaly AUC:0.6555
[2024-03-09 02:40:22,887][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0121 loss2:0.4406 loss3:0.3561 | AUC:0.8669 Anomaly AUC:0.6806
[2024-03-09 02:40:42,006][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0139 loss2:0.3601 loss3:0.3584 | AUC:0.8484 Anomaly AUC:0.6272
[2024-03-09 02:41:00,995][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0118 loss2:0.4325 loss3:0.3564 | AUC:0.8585 Anomaly AUC:0.6518
[2024-03-09 02:41:20,051][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0096 loss2:0.4389 loss3:0.3549 | AUC:0.8601 Anomaly AUC:0.6554
[2024-03-09 02:41:39,121][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.0106 loss2:0.4762 loss3:0.3558 | AUC:0.8609 Anomaly AUC:0.6593
[2024-03-09 02:41:58,160][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0142 loss2:0.4608 loss3:0.3560 | AUC:0.8580 Anomaly AUC:0.6504
[2024-03-09 02:42:17,309][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0107 loss2:0.4688 loss3:0.3591 | AUC:0.8435 Anomaly AUC:0.6194
[2024-03-09 02:42:36,400][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0163 loss2:0.3906 loss3:0.3616 | AUC:0.8463 Anomaly AUC:0.6295
[2024-03-09 02:42:55,423][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0149 loss2:0.4515 loss3:0.3553 | AUC:0.8546 Anomaly AUC:0.6505
[2024-03-09 02:43:14,382][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0116 loss2:0.3787 loss3:0.3586 | AUC:0.8442 Anomaly AUC:0.6243
[2024-03-09 02:43:31,483][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0116 loss2:0.3787 loss3:0.3586 | AUC:0.8443 Anomaly AUC:0.6246
[2024-03-09 02:43:31,519][main.py][line:171][INFO] Training completes in 42m 13s | best AUCAUC:0.8895 Anomaly AUC:0.7266

[2024-03-09 14:22:49,119][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 14:22:49,311][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 14:22:49,311][main.py][line:268][INFO] Test Mode
[2024-03-09 14:22:49,311][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/ucf__current.pkl.
[2024-03-09 14:23:07,776][infer.py][line:94][INFO] offline AUC:0.8933 Anomaly-AUC:0.7339 AP:0.4013 FAR:0.0165 | Complete in 0m 18s

[2024-03-09 14:23:47,942][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 14:23:51,083][main.py][line:259][INFO] total params:8.3467M
[2024-03-09 14:23:51,083][main.py][line:262][INFO] Training Mode
[2024-03-09 14:23:51,083][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 14:23:51,083][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 14:24:04,189][main.py][line:76][INFO] Random initialize AUCAP:0.2849 Anomaly AUC:0.60659
[2024-03-09 14:25:14,813][main.py][line:153][INFO] [Epoch:1/20]: lr:0.00010 | loss1:0.3193 loss2:0.6647 loss3:0.4335 | AUC:0.7884 Anomaly AUC:0.9339
[2024-03-09 14:25:29,856][main.py][line:124][INFO] [Epoch:2/20, Batch:9/320]: loss1:0.1771 loss2:0.6006 loss3:0.4688 | AUC:0.7607 Anomaly AUC:0.9286
[2024-03-09 14:25:44,269][main.py][line:124][INFO] [Epoch:2/20, Batch:19/320]: loss1:0.2868 loss2:0.6758 loss3:0.4294 | AUC:0.7642 Anomaly AUC:0.9307
[2024-03-09 14:25:58,614][main.py][line:124][INFO] [Epoch:2/20, Batch:29/320]: loss1:0.2024 loss2:0.6002 loss3:0.4339 | AUC:0.7514 Anomaly AUC:0.9268
[2024-03-09 14:26:13,005][main.py][line:124][INFO] [Epoch:2/20, Batch:39/320]: loss1:0.1585 loss2:0.5512 loss3:0.4572 | AUC:0.7768 Anomaly AUC:0.9320
[2024-03-09 14:26:27,653][main.py][line:124][INFO] [Epoch:2/20, Batch:49/320]: loss1:0.1608 loss2:0.5903 loss3:0.4482 | AUC:0.7889 Anomaly AUC:0.9349
[2024-03-09 14:26:42,378][main.py][line:124][INFO] [Epoch:2/20, Batch:59/320]: loss1:0.2079 loss2:0.6471 loss3:0.4356 | AUC:0.7629 Anomaly AUC:0.9268
[2024-03-09 14:26:56,911][main.py][line:124][INFO] [Epoch:2/20, Batch:69/320]: loss1:0.1927 loss2:0.5693 loss3:0.4510 | AUC:0.7477 Anomaly AUC:0.9251
[2024-03-09 14:27:11,582][main.py][line:124][INFO] [Epoch:2/20, Batch:79/320]: loss1:0.1875 loss2:0.6026 loss3:0.4390 | AUC:0.7831 Anomaly AUC:0.9329
[2024-03-09 14:27:26,378][main.py][line:124][INFO] [Epoch:2/20, Batch:89/320]: loss1:0.2179 loss2:0.5988 loss3:0.4371 | AUC:0.7052 Anomaly AUC:0.9175
[2024-03-09 14:27:41,063][main.py][line:124][INFO] [Epoch:2/20, Batch:99/320]: loss1:0.1884 loss2:0.5261 loss3:0.4323 | AUC:0.7935 Anomaly AUC:0.9358
[2024-03-09 14:27:55,889][main.py][line:124][INFO] [Epoch:2/20, Batch:109/320]: loss1:0.2882 loss2:0.6597 loss3:0.4262 | AUC:0.7735 Anomaly AUC:0.9319
[2024-03-09 14:28:10,593][main.py][line:124][INFO] [Epoch:2/20, Batch:119/320]: loss1:0.1776 loss2:0.4965 loss3:0.4242 | AUC:0.7427 Anomaly AUC:0.9246
[2024-03-09 14:28:25,515][main.py][line:124][INFO] [Epoch:2/20, Batch:129/320]: loss1:0.2643 loss2:0.5166 loss3:0.4350 | AUC:0.7700 Anomaly AUC:0.9325
[2024-03-09 14:28:40,160][main.py][line:124][INFO] [Epoch:2/20, Batch:139/320]: loss1:0.1917 loss2:0.5699 loss3:0.4220 | AUC:0.7551 Anomaly AUC:0.9256
[2024-03-09 14:28:54,984][main.py][line:124][INFO] [Epoch:2/20, Batch:149/320]: loss1:0.1495 loss2:0.4628 loss3:0.4278 | AUC:0.7811 Anomaly AUC:0.9372
[2024-03-09 14:29:09,705][main.py][line:124][INFO] [Epoch:2/20, Batch:159/320]: loss1:0.1888 loss2:0.5203 loss3:0.4290 | AUC:0.7231 Anomaly AUC:0.9196
[2024-03-09 14:29:24,356][main.py][line:124][INFO] [Epoch:2/20, Batch:169/320]: loss1:0.0976 loss2:0.4336 loss3:0.4333 | AUC:0.7335 Anomaly AUC:0.9248
[2024-03-09 14:29:39,030][main.py][line:124][INFO] [Epoch:2/20, Batch:179/320]: loss1:0.1902 loss2:0.5241 loss3:0.4385 | AUC:0.7741 Anomaly AUC:0.9341
[2024-03-09 14:29:53,532][main.py][line:124][INFO] [Epoch:2/20, Batch:189/320]: loss1:0.1441 loss2:0.4349 loss3:0.4286 | AUC:0.7770 Anomaly AUC:0.9355
[2024-03-09 14:30:08,171][main.py][line:124][INFO] [Epoch:2/20, Batch:199/320]: loss1:0.1583 loss2:0.4398 loss3:0.4156 | AUC:0.7296 Anomaly AUC:0.9250
[2024-03-09 14:30:22,728][main.py][line:124][INFO] [Epoch:2/20, Batch:209/320]: loss1:0.1378 loss2:0.4561 loss3:0.4246 | AUC:0.7491 Anomaly AUC:0.9232
[2024-03-09 14:30:37,194][main.py][line:124][INFO] [Epoch:2/20, Batch:219/320]: loss1:0.1045 loss2:0.4481 loss3:0.4382 | AUC:0.7831 Anomaly AUC:0.9368
[2024-03-09 14:30:51,649][main.py][line:124][INFO] [Epoch:2/20, Batch:229/320]: loss1:0.2238 loss2:0.4534 loss3:0.4329 | AUC:0.6995 Anomaly AUC:0.9079
[2024-03-09 14:31:06,359][main.py][line:124][INFO] [Epoch:2/20, Batch:239/320]: loss1:0.1933 loss2:0.4517 loss3:0.4089 | AUC:0.7467 Anomaly AUC:0.9297
[2024-03-09 14:31:20,898][main.py][line:124][INFO] [Epoch:2/20, Batch:249/320]: loss1:0.1229 loss2:0.3676 loss3:0.4137 | AUC:0.7471 Anomaly AUC:0.9243
[2024-03-09 14:31:35,505][main.py][line:124][INFO] [Epoch:2/20, Batch:259/320]: loss1:0.3583 loss2:0.4851 loss3:0.4291 | AUC:0.7191 Anomaly AUC:0.9245
[2024-03-09 14:31:50,403][main.py][line:124][INFO] [Epoch:2/20, Batch:269/320]: loss1:0.1558 loss2:0.4599 loss3:0.3995 | AUC:0.8214 Anomaly AUC:0.9426
[2024-03-09 14:32:05,086][main.py][line:124][INFO] [Epoch:2/20, Batch:279/320]: loss1:0.1107 loss2:0.4303 loss3:0.4131 | AUC:0.7709 Anomaly AUC:0.9347
[2024-03-09 14:32:19,805][main.py][line:124][INFO] [Epoch:2/20, Batch:289/320]: loss1:0.2145 loss2:0.4647 loss3:0.4260 | AUC:0.7028 Anomaly AUC:0.9213
[2024-03-09 14:32:33,965][main.py][line:153][INFO] [Epoch:2/20]: lr:0.00010 | loss1:0.2234 loss2:0.4344 loss3:0.4173 | AUC:0.7361 Anomaly AUC:0.9257
[2024-03-09 14:32:48,946][main.py][line:124][INFO] [Epoch:3/20, Batch:9/320]: loss1:0.1646 loss2:0.4667 loss3:0.4132 | AUC:0.7585 Anomaly AUC:0.9285
[2024-03-09 14:33:03,347][main.py][line:124][INFO] [Epoch:3/20, Batch:19/320]: loss1:0.1122 loss2:0.4126 loss3:0.4187 | AUC:0.7746 Anomaly AUC:0.9329
[2024-03-09 14:33:18,044][main.py][line:124][INFO] [Epoch:3/20, Batch:29/320]: loss1:0.0891 loss2:0.3947 loss3:0.4081 | AUC:0.7642 Anomaly AUC:0.9319
[2024-03-09 14:33:32,710][main.py][line:124][INFO] [Epoch:3/20, Batch:39/320]: loss1:0.1497 loss2:0.4207 loss3:0.4066 | AUC:0.7422 Anomaly AUC:0.9288
[2024-03-09 14:33:47,398][main.py][line:124][INFO] [Epoch:3/20, Batch:49/320]: loss1:0.0902 loss2:0.3644 loss3:0.4187 | AUC:0.7506 Anomaly AUC:0.9264
[2024-03-09 14:34:02,085][main.py][line:124][INFO] [Epoch:3/20, Batch:59/320]: loss1:0.0898 loss2:0.3595 loss3:0.4105 | AUC:0.7699 Anomaly AUC:0.9343
[2024-03-09 14:34:16,571][main.py][line:124][INFO] [Epoch:3/20, Batch:69/320]: loss1:0.0868 loss2:0.3958 loss3:0.4138 | AUC:0.7455 Anomaly AUC:0.9237
[2024-03-09 14:34:30,917][main.py][line:124][INFO] [Epoch:3/20, Batch:79/320]: loss1:0.2112 loss2:0.4444 loss3:0.4055 | AUC:0.7678 Anomaly AUC:0.9307
[2024-03-09 14:34:45,443][main.py][line:124][INFO] [Epoch:3/20, Batch:89/320]: loss1:0.0912 loss2:0.3643 loss3:0.4027 | AUC:0.7793 Anomaly AUC:0.9345
[2024-03-09 14:35:00,028][main.py][line:124][INFO] [Epoch:3/20, Batch:99/320]: loss1:0.0491 loss2:0.2948 loss3:0.4111 | AUC:0.7512 Anomaly AUC:0.9274
[2024-03-09 14:35:14,502][main.py][line:124][INFO] [Epoch:3/20, Batch:109/320]: loss1:0.1327 loss2:0.3949 loss3:0.4101 | AUC:0.7550 Anomaly AUC:0.9275
[2024-03-09 14:35:29,052][main.py][line:124][INFO] [Epoch:3/20, Batch:119/320]: loss1:0.0314 loss2:0.3088 loss3:0.4086 | AUC:0.7468 Anomaly AUC:0.9274
[2024-03-09 14:35:43,682][main.py][line:124][INFO] [Epoch:3/20, Batch:129/320]: loss1:0.1081 loss2:0.3910 loss3:0.4200 | AUC:0.7901 Anomaly AUC:0.9384
[2024-03-09 14:35:58,124][main.py][line:124][INFO] [Epoch:3/20, Batch:139/320]: loss1:0.0793 loss2:0.3025 loss3:0.4018 | AUC:0.7405 Anomaly AUC:0.9242
[2024-03-09 14:36:12,466][main.py][line:124][INFO] [Epoch:3/20, Batch:149/320]: loss1:0.0576 loss2:0.3407 loss3:0.3974 | AUC:0.7706 Anomaly AUC:0.9378
[2024-03-09 14:36:26,963][main.py][line:124][INFO] [Epoch:3/20, Batch:159/320]: loss1:0.1248 loss2:0.3364 loss3:0.3986 | AUC:0.7681 Anomaly AUC:0.9285
[2024-03-09 14:36:41,476][main.py][line:124][INFO] [Epoch:3/20, Batch:169/320]: loss1:0.0931 loss2:0.3505 loss3:0.4043 | AUC:0.7525 Anomaly AUC:0.9306
[2024-03-09 14:36:55,905][main.py][line:124][INFO] [Epoch:3/20, Batch:179/320]: loss1:0.0947 loss2:0.2852 loss3:0.4080 | AUC:0.7490 Anomaly AUC:0.9302
[2024-03-09 14:37:10,761][main.py][line:124][INFO] [Epoch:3/20, Batch:189/320]: loss1:0.1149 loss2:0.3874 loss3:0.3978 | AUC:0.7867 Anomaly AUC:0.9337
[2024-03-09 14:37:25,589][main.py][line:124][INFO] [Epoch:3/20, Batch:199/320]: loss1:0.0649 loss2:0.3150 loss3:0.4030 | AUC:0.7514 Anomaly AUC:0.9289
[2024-03-09 14:37:40,340][main.py][line:124][INFO] [Epoch:3/20, Batch:209/320]: loss1:0.0713 loss2:0.3390 loss3:0.3962 | AUC:0.7549 Anomaly AUC:0.9277
[2024-03-09 14:37:55,148][main.py][line:124][INFO] [Epoch:3/20, Batch:219/320]: loss1:0.0811 loss2:0.3454 loss3:0.4002 | AUC:0.7736 Anomaly AUC:0.9347
[2024-03-09 14:38:09,800][main.py][line:124][INFO] [Epoch:3/20, Batch:229/320]: loss1:0.0829 loss2:0.3337 loss3:0.3976 | AUC:0.7796 Anomaly AUC:0.9333
[2024-03-09 14:38:24,475][main.py][line:124][INFO] [Epoch:3/20, Batch:239/320]: loss1:0.0627 loss2:0.2775 loss3:0.3969 | AUC:0.6830 Anomaly AUC:0.9118
[2024-03-09 14:38:39,168][main.py][line:124][INFO] [Epoch:3/20, Batch:249/320]: loss1:0.1082 loss2:0.3120 loss3:0.3925 | AUC:0.7477 Anomaly AUC:0.9202
[2024-03-09 14:38:53,787][main.py][line:124][INFO] [Epoch:3/20, Batch:259/320]: loss1:0.0286 loss2:0.2774 loss3:0.4031 | AUC:0.7759 Anomaly AUC:0.9310
[2024-03-09 14:39:08,251][main.py][line:124][INFO] [Epoch:3/20, Batch:269/320]: loss1:0.0504 loss2:0.3385 loss3:0.3990 | AUC:0.7913 Anomaly AUC:0.9303
[2024-03-09 14:39:22,927][main.py][line:124][INFO] [Epoch:3/20, Batch:279/320]: loss1:0.1126 loss2:0.3007 loss3:0.3950 | AUC:0.7189 Anomaly AUC:0.9209
[2024-03-09 14:39:37,800][main.py][line:124][INFO] [Epoch:3/20, Batch:289/320]: loss1:0.2880 loss2:0.4837 loss3:0.3844 | AUC:0.7777 Anomaly AUC:0.9284
[2024-03-09 14:39:52,021][main.py][line:153][INFO] [Epoch:3/20]: lr:0.00010 | loss1:0.1187 loss2:0.3501 loss3:0.3835 | AUC:0.7530 Anomaly AUC:0.9300
[2024-03-09 14:40:07,173][main.py][line:124][INFO] [Epoch:4/20, Batch:9/320]: loss1:0.0272 loss2:0.2721 loss3:0.3888 | AUC:0.7515 Anomaly AUC:0.9295
[2024-03-09 14:40:21,798][main.py][line:124][INFO] [Epoch:4/20, Batch:19/320]: loss1:0.0448 loss2:0.2781 loss3:0.3950 | AUC:0.7124 Anomaly AUC:0.9154
[2024-03-09 14:40:36,493][main.py][line:124][INFO] [Epoch:4/20, Batch:29/320]: loss1:0.0346 loss2:0.3235 loss3:0.3920 | AUC:0.7026 Anomaly AUC:0.9147
[2024-03-09 14:40:51,032][main.py][line:124][INFO] [Epoch:4/20, Batch:39/320]: loss1:0.1690 loss2:0.3925 loss3:0.3881 | AUC:0.7870 Anomaly AUC:0.9353
[2024-03-09 14:41:05,548][main.py][line:124][INFO] [Epoch:4/20, Batch:49/320]: loss1:0.0362 loss2:0.2679 loss3:0.3886 | AUC:0.7427 Anomaly AUC:0.9263
[2024-03-09 14:41:19,932][main.py][line:124][INFO] [Epoch:4/20, Batch:59/320]: loss1:0.0419 loss2:0.2563 loss3:0.3880 | AUC:0.7585 Anomaly AUC:0.9269
[2024-03-09 14:41:34,421][main.py][line:124][INFO] [Epoch:4/20, Batch:69/320]: loss1:0.0720 loss2:0.2845 loss3:0.3917 | AUC:0.7568 Anomaly AUC:0.9278
[2024-03-09 14:41:48,889][main.py][line:124][INFO] [Epoch:4/20, Batch:79/320]: loss1:0.0555 loss2:0.3143 loss3:0.3905 | AUC:0.7992 Anomaly AUC:0.9332
[2024-03-09 14:42:03,500][main.py][line:124][INFO] [Epoch:4/20, Batch:89/320]: loss1:0.0582 loss2:0.2702 loss3:0.3974 | AUC:0.7721 Anomaly AUC:0.9259
[2024-03-09 14:42:18,205][main.py][line:124][INFO] [Epoch:4/20, Batch:99/320]: loss1:0.0625 loss2:0.2791 loss3:0.3899 | AUC:0.7843 Anomaly AUC:0.9296
[2024-03-09 14:42:32,713][main.py][line:124][INFO] [Epoch:4/20, Batch:109/320]: loss1:0.0495 loss2:0.2300 loss3:0.3927 | AUC:0.7336 Anomaly AUC:0.9248
[2024-03-09 14:42:47,296][main.py][line:124][INFO] [Epoch:4/20, Batch:119/320]: loss1:0.1638 loss2:0.4798 loss3:0.4006 | AUC:0.8009 Anomaly AUC:0.9360
[2024-03-09 14:43:02,034][main.py][line:124][INFO] [Epoch:4/20, Batch:129/320]: loss1:0.1037 loss2:0.3021 loss3:0.3823 | AUC:0.7191 Anomaly AUC:0.9220
[2024-03-09 14:43:16,488][main.py][line:124][INFO] [Epoch:4/20, Batch:139/320]: loss1:0.0963 loss2:0.2675 loss3:0.3917 | AUC:0.6659 Anomaly AUC:0.9114
[2024-03-09 14:43:31,119][main.py][line:124][INFO] [Epoch:4/20, Batch:149/320]: loss1:0.0741 loss2:0.2575 loss3:0.3918 | AUC:0.8160 Anomaly AUC:0.9347
[2024-03-09 14:43:45,569][main.py][line:124][INFO] [Epoch:4/20, Batch:159/320]: loss1:0.1112 loss2:0.4244 loss3:0.3968 | AUC:0.7763 Anomaly AUC:0.9329
[2024-03-09 14:44:00,224][main.py][line:124][INFO] [Epoch:4/20, Batch:169/320]: loss1:0.0971 loss2:0.2883 loss3:0.3850 | AUC:0.8018 Anomaly AUC:0.9339
[2024-03-09 14:44:14,923][main.py][line:124][INFO] [Epoch:4/20, Batch:179/320]: loss1:0.0265 loss2:0.2720 loss3:0.3858 | AUC:0.8027 Anomaly AUC:0.9345
[2024-03-09 14:44:29,528][main.py][line:124][INFO] [Epoch:4/20, Batch:189/320]: loss1:0.0392 loss2:0.2487 loss3:0.3838 | AUC:0.7725 Anomaly AUC:0.9325
[2024-03-09 14:44:44,177][main.py][line:124][INFO] [Epoch:4/20, Batch:199/320]: loss1:0.0781 loss2:0.2157 loss3:0.3834 | AUC:0.7038 Anomaly AUC:0.9228
[2024-03-09 14:44:58,658][main.py][line:124][INFO] [Epoch:4/20, Batch:209/320]: loss1:0.0260 loss2:0.2252 loss3:0.3899 | AUC:0.7247 Anomaly AUC:0.9251
[2024-03-09 14:45:13,111][main.py][line:124][INFO] [Epoch:4/20, Batch:219/320]: loss1:0.0776 loss2:0.2461 loss3:0.3853 | AUC:0.7409 Anomaly AUC:0.9232
[2024-03-09 14:45:27,765][main.py][line:124][INFO] [Epoch:4/20, Batch:229/320]: loss1:0.0683 loss2:0.3171 loss3:0.3786 | AUC:0.7708 Anomaly AUC:0.9238
[2024-03-09 14:45:42,202][main.py][line:124][INFO] [Epoch:4/20, Batch:239/320]: loss1:0.1148 loss2:0.4226 loss3:0.3826 | AUC:0.7312 Anomaly AUC:0.9147
[2024-03-09 14:45:56,816][main.py][line:124][INFO] [Epoch:4/20, Batch:249/320]: loss1:0.0559 loss2:0.2537 loss3:0.3887 | AUC:0.8048 Anomaly AUC:0.9349
[2024-03-09 14:46:11,460][main.py][line:124][INFO] [Epoch:4/20, Batch:259/320]: loss1:0.0781 loss2:0.2869 loss3:0.3819 | AUC:0.7827 Anomaly AUC:0.9301
[2024-03-09 14:46:26,107][main.py][line:124][INFO] [Epoch:4/20, Batch:269/320]: loss1:0.0329 loss2:0.2460 loss3:0.3833 | AUC:0.7475 Anomaly AUC:0.9275
[2024-03-09 14:46:40,555][main.py][line:124][INFO] [Epoch:4/20, Batch:279/320]: loss1:0.0490 loss2:0.2814 loss3:0.3829 | AUC:0.7605 Anomaly AUC:0.9315
[2024-03-09 14:46:55,029][main.py][line:124][INFO] [Epoch:4/20, Batch:289/320]: loss1:0.0107 loss2:0.1930 loss3:0.3939 | AUC:0.7365 Anomaly AUC:0.9266
[2024-03-09 14:47:08,981][main.py][line:153][INFO] [Epoch:4/20]: lr:0.00010 | loss1:0.0563 loss2:0.2498 loss3:0.3885 | AUC:0.7588 Anomaly AUC:0.9285
[2024-03-09 14:47:24,051][main.py][line:124][INFO] [Epoch:5/20, Batch:9/320]: loss1:0.0322 loss2:0.1652 loss3:0.3822 | AUC:0.7739 Anomaly AUC:0.9326
[2024-03-09 14:47:38,650][main.py][line:124][INFO] [Epoch:5/20, Batch:19/320]: loss1:0.0142 loss2:0.2358 loss3:0.3885 | AUC:0.7193 Anomaly AUC:0.9220
[2024-03-09 14:47:53,062][main.py][line:124][INFO] [Epoch:5/20, Batch:29/320]: loss1:0.0154 loss2:0.2042 loss3:0.3849 | AUC:0.7262 Anomaly AUC:0.9197
[2024-03-09 14:48:07,618][main.py][line:124][INFO] [Epoch:5/20, Batch:39/320]: loss1:0.2485 loss2:0.4287 loss3:0.3900 | AUC:0.7173 Anomaly AUC:0.9223
[2024-03-09 14:48:22,241][main.py][line:124][INFO] [Epoch:5/20, Batch:49/320]: loss1:0.0827 loss2:0.3560 loss3:0.3976 | AUC:0.7649 Anomaly AUC:0.9151
[2024-03-09 14:48:36,742][main.py][line:124][INFO] [Epoch:5/20, Batch:59/320]: loss1:0.0386 loss2:0.2427 loss3:0.3919 | AUC:0.7822 Anomaly AUC:0.9267
[2024-03-09 14:48:51,268][main.py][line:124][INFO] [Epoch:5/20, Batch:69/320]: loss1:0.0669 loss2:0.2581 loss3:0.3832 | AUC:0.7845 Anomaly AUC:0.9320
[2024-03-09 14:49:05,899][main.py][line:124][INFO] [Epoch:5/20, Batch:79/320]: loss1:0.0327 loss2:0.2205 loss3:0.3855 | AUC:0.7334 Anomaly AUC:0.9269
[2024-03-09 14:49:20,449][main.py][line:124][INFO] [Epoch:5/20, Batch:89/320]: loss1:0.0333 loss2:0.2162 loss3:0.3794 | AUC:0.8121 Anomaly AUC:0.9387
[2024-03-09 14:49:35,235][main.py][line:124][INFO] [Epoch:5/20, Batch:99/320]: loss1:0.0301 loss2:0.2422 loss3:0.3927 | AUC:0.7557 Anomaly AUC:0.9265
[2024-03-09 14:49:49,723][main.py][line:124][INFO] [Epoch:5/20, Batch:109/320]: loss1:0.0730 loss2:0.2744 loss3:0.3815 | AUC:0.7955 Anomaly AUC:0.9329
[2024-03-09 14:50:04,561][main.py][line:124][INFO] [Epoch:5/20, Batch:119/320]: loss1:0.0666 loss2:0.3228 loss3:0.3744 | AUC:0.7837 Anomaly AUC:0.9298
[2024-03-09 14:50:19,058][main.py][line:124][INFO] [Epoch:5/20, Batch:129/320]: loss1:0.0231 loss2:0.2217 loss3:0.3817 | AUC:0.7374 Anomaly AUC:0.9247
[2024-03-09 14:50:33,658][main.py][line:124][INFO] [Epoch:5/20, Batch:139/320]: loss1:0.1499 loss2:0.3628 loss3:0.3750 | AUC:0.7719 Anomaly AUC:0.9288
[2024-03-09 14:50:48,153][main.py][line:124][INFO] [Epoch:5/20, Batch:149/320]: loss1:0.0225 loss2:0.2368 loss3:0.3839 | AUC:0.7806 Anomaly AUC:0.9305
[2024-03-09 14:51:02,671][main.py][line:124][INFO] [Epoch:5/20, Batch:159/320]: loss1:0.0155 loss2:0.2380 loss3:0.3786 | AUC:0.8349 Anomaly AUC:0.9450
[2024-03-09 14:51:17,209][main.py][line:124][INFO] [Epoch:5/20, Batch:169/320]: loss1:0.1304 loss2:0.2712 loss3:0.3788 | AUC:0.7595 Anomaly AUC:0.9322
[2024-03-09 14:51:31,807][main.py][line:124][INFO] [Epoch:5/20, Batch:179/320]: loss1:0.0151 loss2:0.2641 loss3:0.3770 | AUC:0.7353 Anomaly AUC:0.9197
[2024-03-09 14:51:46,222][main.py][line:124][INFO] [Epoch:5/20, Batch:189/320]: loss1:0.0146 loss2:0.1571 loss3:0.3743 | AUC:0.7878 Anomaly AUC:0.9316
[2024-03-09 14:52:00,911][main.py][line:124][INFO] [Epoch:5/20, Batch:199/320]: loss1:0.0205 loss2:0.2091 loss3:0.3762 | AUC:0.7931 Anomaly AUC:0.9313
[2024-03-09 14:52:15,458][main.py][line:124][INFO] [Epoch:5/20, Batch:209/320]: loss1:0.0353 loss2:0.1973 loss3:0.3746 | AUC:0.8002 Anomaly AUC:0.9334
[2024-03-09 14:52:29,906][main.py][line:124][INFO] [Epoch:5/20, Batch:219/320]: loss1:0.0968 loss2:0.2868 loss3:0.3818 | AUC:0.7996 Anomaly AUC:0.9333
[2024-03-09 14:52:44,307][main.py][line:124][INFO] [Epoch:5/20, Batch:229/320]: loss1:0.0569 loss2:0.2130 loss3:0.3704 | AUC:0.7788 Anomaly AUC:0.9338
[2024-03-09 14:52:58,869][main.py][line:124][INFO] [Epoch:5/20, Batch:239/320]: loss1:0.0595 loss2:0.2851 loss3:0.3901 | AUC:0.7785 Anomaly AUC:0.9335
[2024-03-09 14:53:13,320][main.py][line:124][INFO] [Epoch:5/20, Batch:249/320]: loss1:0.0215 loss2:0.1886 loss3:0.3754 | AUC:0.7414 Anomaly AUC:0.9238
[2024-03-09 14:53:27,732][main.py][line:124][INFO] [Epoch:5/20, Batch:259/320]: loss1:0.0237 loss2:0.1539 loss3:0.3750 | AUC:0.7972 Anomaly AUC:0.9368
[2024-03-09 14:53:41,993][main.py][line:124][INFO] [Epoch:5/20, Batch:269/320]: loss1:0.1864 loss2:0.3191 loss3:0.3779 | AUC:0.7523 Anomaly AUC:0.9329
[2024-03-09 14:53:56,444][main.py][line:124][INFO] [Epoch:5/20, Batch:279/320]: loss1:0.0357 loss2:0.2553 loss3:0.3765 | AUC:0.7260 Anomaly AUC:0.9120
[2024-03-09 14:54:10,978][main.py][line:124][INFO] [Epoch:5/20, Batch:289/320]: loss1:0.0410 loss2:0.1910 loss3:0.3802 | AUC:0.7521 Anomaly AUC:0.9219
[2024-03-09 14:54:24,975][main.py][line:153][INFO] [Epoch:5/20]: lr:0.00010 | loss1:0.0140 loss2:0.2473 loss3:0.3721 | AUC:0.7511 Anomaly AUC:0.9236
[2024-03-09 14:54:40,077][main.py][line:124][INFO] [Epoch:6/20, Batch:9/320]: loss1:0.0572 loss2:0.2209 loss3:0.3739 | AUC:0.7759 Anomaly AUC:0.9229
[2024-03-09 14:54:54,680][main.py][line:124][INFO] [Epoch:6/20, Batch:19/320]: loss1:0.0183 loss2:0.2415 loss3:0.3703 | AUC:0.7800 Anomaly AUC:0.9303
[2024-03-09 14:55:09,223][main.py][line:124][INFO] [Epoch:6/20, Batch:29/320]: loss1:0.0160 loss2:0.1688 loss3:0.3710 | AUC:0.7439 Anomaly AUC:0.9265
[2024-03-09 14:55:23,703][main.py][line:124][INFO] [Epoch:6/20, Batch:39/320]: loss1:0.0887 loss2:0.1909 loss3:0.3726 | AUC:0.7743 Anomaly AUC:0.9290
[2024-03-09 14:55:38,168][main.py][line:124][INFO] [Epoch:6/20, Batch:49/320]: loss1:0.0509 loss2:0.1934 loss3:0.3702 | AUC:0.7684 Anomaly AUC:0.9224
[2024-03-09 14:55:52,588][main.py][line:124][INFO] [Epoch:6/20, Batch:59/320]: loss1:0.0354 loss2:0.2249 loss3:0.3793 | AUC:0.7754 Anomaly AUC:0.9285
[2024-03-09 14:56:07,080][main.py][line:124][INFO] [Epoch:6/20, Batch:69/320]: loss1:0.0273 loss2:0.2005 loss3:0.3638 | AUC:0.7724 Anomaly AUC:0.9307
[2024-03-09 14:56:21,659][main.py][line:124][INFO] [Epoch:6/20, Batch:79/320]: loss1:0.0567 loss2:0.2514 loss3:0.3675 | AUC:0.7877 Anomaly AUC:0.9341
[2024-03-09 14:56:36,273][main.py][line:124][INFO] [Epoch:6/20, Batch:89/320]: loss1:0.0426 loss2:0.2271 loss3:0.3712 | AUC:0.7666 Anomaly AUC:0.9293
[2024-03-09 14:56:50,756][main.py][line:124][INFO] [Epoch:6/20, Batch:99/320]: loss1:0.0187 loss2:0.1608 loss3:0.3754 | AUC:0.7829 Anomaly AUC:0.9307
[2024-03-09 14:57:05,327][main.py][line:124][INFO] [Epoch:6/20, Batch:109/320]: loss1:0.0300 loss2:0.1843 loss3:0.3687 | AUC:0.7524 Anomaly AUC:0.9239
[2024-03-09 14:57:19,703][main.py][line:124][INFO] [Epoch:6/20, Batch:119/320]: loss1:0.0506 loss2:0.2967 loss3:0.3675 | AUC:0.7951 Anomaly AUC:0.9405
[2024-03-09 14:57:34,381][main.py][line:124][INFO] [Epoch:6/20, Batch:129/320]: loss1:0.0659 loss2:0.2241 loss3:0.3674 | AUC:0.7825 Anomaly AUC:0.9362
[2024-03-09 14:57:48,877][main.py][line:124][INFO] [Epoch:6/20, Batch:139/320]: loss1:0.0122 loss2:0.2379 loss3:0.3718 | AUC:0.7356 Anomaly AUC:0.9323
[2024-03-09 14:58:03,536][main.py][line:124][INFO] [Epoch:6/20, Batch:149/320]: loss1:0.1164 loss2:0.2316 loss3:0.3734 | AUC:0.7324 Anomaly AUC:0.9276
[2024-03-09 14:58:18,080][main.py][line:124][INFO] [Epoch:6/20, Batch:159/320]: loss1:0.0164 loss2:0.1642 loss3:0.3717 | AUC:0.7719 Anomaly AUC:0.9298
[2024-03-09 14:58:32,619][main.py][line:124][INFO] [Epoch:6/20, Batch:169/320]: loss1:0.0122 loss2:0.1899 loss3:0.3651 | AUC:0.7363 Anomaly AUC:0.9207
[2024-03-09 14:58:47,118][main.py][line:124][INFO] [Epoch:6/20, Batch:179/320]: loss1:0.0338 loss2:0.1574 loss3:0.3691 | AUC:0.7127 Anomaly AUC:0.9070
[2024-03-09 14:59:01,829][main.py][line:124][INFO] [Epoch:6/20, Batch:189/320]: loss1:0.0096 loss2:0.1882 loss3:0.3650 | AUC:0.7830 Anomaly AUC:0.9282
[2024-03-09 14:59:16,210][main.py][line:124][INFO] [Epoch:6/20, Batch:199/320]: loss1:0.0168 loss2:0.3024 loss3:0.3619 | AUC:0.7638 Anomaly AUC:0.9241
[2024-03-09 14:59:30,556][main.py][line:124][INFO] [Epoch:6/20, Batch:209/320]: loss1:0.0333 loss2:0.2067 loss3:0.3606 | AUC:0.7400 Anomaly AUC:0.9204
[2024-03-09 14:59:45,291][main.py][line:124][INFO] [Epoch:6/20, Batch:219/320]: loss1:0.0090 loss2:0.1665 loss3:0.3595 | AUC:0.7547 Anomaly AUC:0.9251
[2024-03-09 14:59:59,773][main.py][line:124][INFO] [Epoch:6/20, Batch:229/320]: loss1:0.0100 loss2:0.1834 loss3:0.3576 | AUC:0.7486 Anomaly AUC:0.9266
[2024-03-09 15:00:14,310][main.py][line:124][INFO] [Epoch:6/20, Batch:239/320]: loss1:0.0471 loss2:0.2316 loss3:0.3577 | AUC:0.7330 Anomaly AUC:0.9198
[2024-03-09 15:00:28,909][main.py][line:124][INFO] [Epoch:6/20, Batch:249/320]: loss1:0.0269 loss2:0.1987 loss3:0.3612 | AUC:0.7913 Anomaly AUC:0.9352
[2024-03-09 15:00:43,432][main.py][line:124][INFO] [Epoch:6/20, Batch:259/320]: loss1:0.0364 loss2:0.2421 loss3:0.3548 | AUC:0.7377 Anomaly AUC:0.9272
[2024-03-09 15:00:57,900][main.py][line:124][INFO] [Epoch:6/20, Batch:269/320]: loss1:0.0837 loss2:0.2752 loss3:0.3704 | AUC:0.7621 Anomaly AUC:0.9340
[2024-03-09 15:01:12,434][main.py][line:124][INFO] [Epoch:6/20, Batch:279/320]: loss1:0.0126 loss2:0.1262 loss3:0.3564 | AUC:0.7146 Anomaly AUC:0.9221
[2024-03-09 15:01:26,963][main.py][line:124][INFO] [Epoch:6/20, Batch:289/320]: loss1:0.0147 loss2:0.1339 loss3:0.3523 | AUC:0.7151 Anomaly AUC:0.9216
[2024-03-09 15:01:40,951][main.py][line:153][INFO] [Epoch:6/20]: lr:0.00010 | loss1:0.0184 loss2:0.1902 loss3:0.3547 | AUC:0.7804 Anomaly AUC:0.9287
[2024-03-09 15:01:55,874][main.py][line:124][INFO] [Epoch:7/20, Batch:9/320]: loss1:0.0227 loss2:0.1862 loss3:0.3535 | AUC:0.7317 Anomaly AUC:0.9217
[2024-03-09 15:02:10,502][main.py][line:124][INFO] [Epoch:7/20, Batch:19/320]: loss1:0.0276 loss2:0.2337 loss3:0.3643 | AUC:0.7463 Anomaly AUC:0.9211
[2024-03-09 15:02:25,009][main.py][line:124][INFO] [Epoch:7/20, Batch:29/320]: loss1:0.0163 loss2:0.1261 loss3:0.3640 | AUC:0.7353 Anomaly AUC:0.9156
[2024-03-09 15:02:39,464][main.py][line:124][INFO] [Epoch:7/20, Batch:39/320]: loss1:0.0155 loss2:0.1878 loss3:0.3576 | AUC:0.7513 Anomaly AUC:0.9245
[2024-03-09 15:02:53,963][main.py][line:124][INFO] [Epoch:7/20, Batch:49/320]: loss1:0.0125 loss2:0.1311 loss3:0.3544 | AUC:0.7373 Anomaly AUC:0.9272
[2024-03-09 15:03:08,449][main.py][line:124][INFO] [Epoch:7/20, Batch:59/320]: loss1:0.0460 loss2:0.1957 loss3:0.3549 | AUC:0.7304 Anomaly AUC:0.9202
[2024-03-09 15:03:23,090][main.py][line:124][INFO] [Epoch:7/20, Batch:69/320]: loss1:0.0298 loss2:0.1794 loss3:0.3566 | AUC:0.7561 Anomaly AUC:0.9212
[2024-03-09 15:03:37,575][main.py][line:124][INFO] [Epoch:7/20, Batch:79/320]: loss1:0.0191 loss2:0.2435 loss3:0.3640 | AUC:0.8117 Anomaly AUC:0.9346
[2024-03-09 15:03:51,989][main.py][line:124][INFO] [Epoch:7/20, Batch:89/320]: loss1:0.0221 loss2:0.1441 loss3:0.3812 | AUC:0.8370 Anomaly AUC:0.9456
[2024-03-09 15:04:06,515][main.py][line:124][INFO] [Epoch:7/20, Batch:99/320]: loss1:0.0247 loss2:0.1555 loss3:0.3644 | AUC:0.8102 Anomaly AUC:0.9386
[2024-03-09 15:04:21,082][main.py][line:124][INFO] [Epoch:7/20, Batch:109/320]: loss1:0.0361 loss2:0.2261 loss3:0.3626 | AUC:0.7771 Anomaly AUC:0.9306
[2024-03-09 15:04:35,554][main.py][line:124][INFO] [Epoch:7/20, Batch:119/320]: loss1:0.0386 loss2:0.2416 loss3:0.3753 | AUC:0.7529 Anomaly AUC:0.9270
[2024-03-09 15:04:49,939][main.py][line:124][INFO] [Epoch:7/20, Batch:129/320]: loss1:0.2057 loss2:0.2776 loss3:0.3817 | AUC:0.7993 Anomaly AUC:0.9320
[2024-03-09 15:05:04,473][main.py][line:124][INFO] [Epoch:7/20, Batch:139/320]: loss1:0.0401 loss2:0.2300 loss3:0.3774 | AUC:0.7326 Anomaly AUC:0.9210
[2024-03-09 15:05:18,803][main.py][line:124][INFO] [Epoch:7/20, Batch:149/320]: loss1:0.0506 loss2:0.2199 loss3:0.3676 | AUC:0.7640 Anomaly AUC:0.9214
[2024-03-09 15:05:33,166][main.py][line:124][INFO] [Epoch:7/20, Batch:159/320]: loss1:0.0310 loss2:0.1495 loss3:0.3598 | AUC:0.7666 Anomaly AUC:0.9260
[2024-03-09 15:05:47,550][main.py][line:124][INFO] [Epoch:7/20, Batch:169/320]: loss1:0.0695 loss2:0.2484 loss3:0.3568 | AUC:0.7982 Anomaly AUC:0.9369
[2024-03-09 15:06:02,117][main.py][line:124][INFO] [Epoch:7/20, Batch:179/320]: loss1:0.0194 loss2:0.2178 loss3:0.3627 | AUC:0.7964 Anomaly AUC:0.9320
[2024-03-09 15:06:16,624][main.py][line:124][INFO] [Epoch:7/20, Batch:189/320]: loss1:0.0115 loss2:0.1604 loss3:0.3570 | AUC:0.7808 Anomaly AUC:0.9315
[2024-03-09 15:06:31,066][main.py][line:124][INFO] [Epoch:7/20, Batch:199/320]: loss1:0.0138 loss2:0.1919 loss3:0.3521 | AUC:0.7330 Anomaly AUC:0.9248
[2024-03-09 15:06:45,619][main.py][line:124][INFO] [Epoch:7/20, Batch:209/320]: loss1:0.0219 loss2:0.1646 loss3:0.3522 | AUC:0.7871 Anomaly AUC:0.9319
[2024-03-09 15:07:00,046][main.py][line:124][INFO] [Epoch:7/20, Batch:219/320]: loss1:0.0147 loss2:0.1212 loss3:0.3555 | AUC:0.7846 Anomaly AUC:0.9322
[2024-03-09 15:07:14,387][main.py][line:124][INFO] [Epoch:7/20, Batch:229/320]: loss1:0.0143 loss2:0.1338 loss3:0.3556 | AUC:0.8152 Anomaly AUC:0.9378
[2024-03-09 15:07:29,023][main.py][line:124][INFO] [Epoch:7/20, Batch:239/320]: loss1:0.0094 loss2:0.1961 loss3:0.3527 | AUC:0.7989 Anomaly AUC:0.9361
[2024-03-09 15:07:43,542][main.py][line:124][INFO] [Epoch:7/20, Batch:249/320]: loss1:0.0770 loss2:0.1617 loss3:0.3549 | AUC:0.7820 Anomaly AUC:0.9349
[2024-03-09 15:07:58,157][main.py][line:124][INFO] [Epoch:7/20, Batch:259/320]: loss1:0.0816 loss2:0.2485 loss3:0.3591 | AUC:0.7696 Anomaly AUC:0.9306
[2024-03-09 15:08:12,844][main.py][line:124][INFO] [Epoch:7/20, Batch:269/320]: loss1:0.0397 loss2:0.2603 loss3:0.3615 | AUC:0.6973 Anomaly AUC:0.9117
[2024-03-09 15:08:27,416][main.py][line:124][INFO] [Epoch:7/20, Batch:279/320]: loss1:0.0425 loss2:0.1825 loss3:0.3596 | AUC:0.7580 Anomaly AUC:0.9300
[2024-03-09 15:08:41,844][main.py][line:124][INFO] [Epoch:7/20, Batch:289/320]: loss1:0.0234 loss2:0.1884 loss3:0.3857 | AUC:0.7733 Anomaly AUC:0.9306
[2024-03-09 15:08:55,833][main.py][line:153][INFO] [Epoch:7/20]: lr:0.00010 | loss1:0.0184 loss2:0.1723 loss3:0.3760 | AUC:0.7927 Anomaly AUC:0.9336
[2024-03-09 15:09:10,806][main.py][line:124][INFO] [Epoch:8/20, Batch:9/320]: loss1:0.0777 loss2:0.2906 loss3:0.3865 | AUC:0.7698 Anomaly AUC:0.9299
[2024-03-09 15:09:25,517][main.py][line:124][INFO] [Epoch:8/20, Batch:19/320]: loss1:0.0524 loss2:0.1624 loss3:0.3692 | AUC:0.7874 Anomaly AUC:0.9308
[2024-03-09 15:09:39,996][main.py][line:124][INFO] [Epoch:8/20, Batch:29/320]: loss1:0.0844 loss2:0.1630 loss3:0.3699 | AUC:0.8157 Anomaly AUC:0.9369
[2024-03-09 15:09:54,456][main.py][line:124][INFO] [Epoch:8/20, Batch:39/320]: loss1:0.0294 loss2:0.1702 loss3:0.3802 | AUC:0.8099 Anomaly AUC:0.9388
[2024-03-09 15:10:09,035][main.py][line:124][INFO] [Epoch:8/20, Batch:49/320]: loss1:0.0264 loss2:0.1992 loss3:0.3670 | AUC:0.7772 Anomaly AUC:0.9339
[2024-03-09 15:10:23,744][main.py][line:124][INFO] [Epoch:8/20, Batch:59/320]: loss1:0.0288 loss2:0.1953 loss3:0.3672 | AUC:0.7936 Anomaly AUC:0.9344
[2024-03-09 15:10:38,278][main.py][line:124][INFO] [Epoch:8/20, Batch:69/320]: loss1:0.0791 loss2:0.2268 loss3:0.3623 | AUC:0.7675 Anomaly AUC:0.9284
[2024-03-09 15:10:52,575][main.py][line:124][INFO] [Epoch:8/20, Batch:79/320]: loss1:0.0570 loss2:0.1754 loss3:0.3795 | AUC:0.7509 Anomaly AUC:0.9195
[2024-03-09 15:11:06,968][main.py][line:124][INFO] [Epoch:8/20, Batch:89/320]: loss1:0.0374 loss2:0.2022 loss3:0.3838 | AUC:0.8141 Anomaly AUC:0.9374
[2024-03-09 15:11:21,393][main.py][line:124][INFO] [Epoch:8/20, Batch:99/320]: loss1:0.0384 loss2:0.1992 loss3:0.3719 | AUC:0.8081 Anomaly AUC:0.9409
[2024-03-09 15:11:36,057][main.py][line:124][INFO] [Epoch:8/20, Batch:109/320]: loss1:0.1763 loss2:0.3142 loss3:0.4030 | AUC:0.8519 Anomaly AUC:0.9483
[2024-03-09 15:11:50,495][main.py][line:124][INFO] [Epoch:8/20, Batch:119/320]: loss1:0.0453 loss2:0.1398 loss3:0.3914 | AUC:0.8000 Anomaly AUC:0.9391
[2024-03-09 15:12:05,061][main.py][line:124][INFO] [Epoch:8/20, Batch:129/320]: loss1:0.0322 loss2:0.2158 loss3:0.3969 | AUC:0.7777 Anomaly AUC:0.9329
[2024-03-09 15:12:19,363][main.py][line:124][INFO] [Epoch:8/20, Batch:139/320]: loss1:0.0678 loss2:0.1621 loss3:0.3950 | AUC:0.7829 Anomaly AUC:0.9330
[2024-03-09 15:12:33,952][main.py][line:124][INFO] [Epoch:8/20, Batch:149/320]: loss1:0.0188 loss2:0.1474 loss3:0.3935 | AUC:0.8005 Anomaly AUC:0.9358
[2024-03-09 15:12:48,585][main.py][line:124][INFO] [Epoch:8/20, Batch:159/320]: loss1:0.0140 loss2:0.1782 loss3:0.3919 | AUC:0.8048 Anomaly AUC:0.9370
[2024-03-09 15:13:02,982][main.py][line:124][INFO] [Epoch:8/20, Batch:169/320]: loss1:0.0119 loss2:0.1968 loss3:0.3961 | AUC:0.7854 Anomaly AUC:0.9341
[2024-03-09 15:13:17,459][main.py][line:124][INFO] [Epoch:8/20, Batch:179/320]: loss1:0.0136 loss2:0.1433 loss3:0.3852 | AUC:0.8063 Anomaly AUC:0.9388
[2024-03-09 15:13:32,075][main.py][line:124][INFO] [Epoch:8/20, Batch:189/320]: loss1:0.0103 loss2:0.1706 loss3:0.3842 | AUC:0.8055 Anomaly AUC:0.9388
[2024-03-09 15:13:46,485][main.py][line:124][INFO] [Epoch:8/20, Batch:199/320]: loss1:0.0104 loss2:0.1787 loss3:0.3943 | AUC:0.7936 Anomaly AUC:0.9365
[2024-03-09 15:14:01,019][main.py][line:124][INFO] [Epoch:8/20, Batch:209/320]: loss1:0.0206 loss2:0.1414 loss3:0.3907 | AUC:0.7779 Anomaly AUC:0.9349
[2024-03-09 15:14:15,616][main.py][line:124][INFO] [Epoch:8/20, Batch:219/320]: loss1:0.0443 loss2:0.1512 loss3:0.3872 | AUC:0.7822 Anomaly AUC:0.9340
[2024-03-09 15:14:30,074][main.py][line:124][INFO] [Epoch:8/20, Batch:229/320]: loss1:0.0204 loss2:0.1440 loss3:0.3998 | AUC:0.7845 Anomaly AUC:0.9381
[2024-03-09 15:14:44,466][main.py][line:124][INFO] [Epoch:8/20, Batch:239/320]: loss1:0.0979 loss2:0.2177 loss3:0.3956 | AUC:0.7625 Anomaly AUC:0.9329
[2024-03-09 15:14:58,889][main.py][line:124][INFO] [Epoch:8/20, Batch:249/320]: loss1:0.0215 loss2:0.1115 loss3:0.4109 | AUC:0.7785 Anomaly AUC:0.9328
[2024-03-09 15:15:13,556][main.py][line:124][INFO] [Epoch:8/20, Batch:259/320]: loss1:0.0300 loss2:0.1142 loss3:0.3993 | AUC:0.7548 Anomaly AUC:0.9300
[2024-03-09 15:15:27,940][main.py][line:124][INFO] [Epoch:8/20, Batch:269/320]: loss1:0.0176 loss2:0.1324 loss3:0.3817 | AUC:0.7723 Anomaly AUC:0.9325
[2024-03-09 15:15:42,526][main.py][line:124][INFO] [Epoch:8/20, Batch:279/320]: loss1:0.0375 loss2:0.1562 loss3:0.4151 | AUC:0.7927 Anomaly AUC:0.9379
[2024-03-09 15:15:57,120][main.py][line:124][INFO] [Epoch:8/20, Batch:289/320]: loss1:0.0663 loss2:0.1933 loss3:0.4027 | AUC:0.8074 Anomaly AUC:0.9450
[2024-03-09 15:16:11,366][main.py][line:153][INFO] [Epoch:8/20]: lr:0.00010 | loss1:0.1194 loss2:0.2410 loss3:0.4256 | AUC:0.7775 Anomaly AUC:0.9366
[2024-03-09 15:16:26,336][main.py][line:124][INFO] [Epoch:9/20, Batch:9/320]: loss1:0.0280 loss2:0.1719 loss3:0.4219 | AUC:0.7248 Anomaly AUC:0.9265
[2024-03-09 15:16:40,707][main.py][line:124][INFO] [Epoch:9/20, Batch:19/320]: loss1:0.0129 loss2:0.1339 loss3:0.4089 | AUC:0.7862 Anomaly AUC:0.9369
[2024-03-09 15:16:55,305][main.py][line:124][INFO] [Epoch:9/20, Batch:29/320]: loss1:0.0316 loss2:0.1395 loss3:0.4038 | AUC:0.7801 Anomaly AUC:0.9339
[2024-03-09 15:17:09,665][main.py][line:124][INFO] [Epoch:9/20, Batch:39/320]: loss1:0.1149 loss2:0.1944 loss3:0.4050 | AUC:0.7559 Anomaly AUC:0.9282
[2024-03-09 15:17:24,127][main.py][line:124][INFO] [Epoch:9/20, Batch:49/320]: loss1:0.0154 loss2:0.1520 loss3:0.3947 | AUC:0.7541 Anomaly AUC:0.9308
[2024-03-09 15:17:38,676][main.py][line:124][INFO] [Epoch:9/20, Batch:59/320]: loss1:0.0185 loss2:0.1410 loss3:0.3995 | AUC:0.7672 Anomaly AUC:0.9308
[2024-03-09 15:17:53,088][main.py][line:124][INFO] [Epoch:9/20, Batch:69/320]: loss1:0.0674 loss2:0.1886 loss3:0.4026 | AUC:0.6886 Anomaly AUC:0.9219
[2024-03-09 15:18:07,607][main.py][line:124][INFO] [Epoch:9/20, Batch:79/320]: loss1:0.0171 loss2:0.1843 loss3:0.3971 | AUC:0.6988 Anomaly AUC:0.9230
[2024-03-09 15:18:22,230][main.py][line:124][INFO] [Epoch:9/20, Batch:89/320]: loss1:0.0123 loss2:0.1564 loss3:0.3928 | AUC:0.7339 Anomaly AUC:0.9274
[2024-03-09 15:18:36,838][main.py][line:124][INFO] [Epoch:9/20, Batch:99/320]: loss1:0.0121 loss2:0.1006 loss3:0.3913 | AUC:0.7503 Anomaly AUC:0.9327
[2024-03-09 15:18:51,432][main.py][line:124][INFO] [Epoch:9/20, Batch:109/320]: loss1:0.0065 loss2:0.1258 loss3:0.4068 | AUC:0.7773 Anomaly AUC:0.9360
[2024-03-09 15:19:05,979][main.py][line:124][INFO] [Epoch:9/20, Batch:119/320]: loss1:0.0476 loss2:0.1543 loss3:0.3740 | AUC:0.7419 Anomaly AUC:0.9313
[2024-03-09 15:19:20,376][main.py][line:124][INFO] [Epoch:9/20, Batch:129/320]: loss1:0.0198 loss2:0.1520 loss3:0.3867 | AUC:0.7481 Anomaly AUC:0.9345
[2024-03-09 15:19:34,749][main.py][line:124][INFO] [Epoch:9/20, Batch:139/320]: loss1:0.0202 loss2:0.1228 loss3:0.3995 | AUC:0.7600 Anomaly AUC:0.9288
[2024-03-09 15:19:49,410][main.py][line:124][INFO] [Epoch:9/20, Batch:149/320]: loss1:0.0074 loss2:0.1732 loss3:0.4116 | AUC:0.7343 Anomaly AUC:0.9285
[2024-03-09 15:20:04,123][main.py][line:124][INFO] [Epoch:9/20, Batch:159/320]: loss1:0.0387 loss2:0.1330 loss3:0.3806 | AUC:0.7708 Anomaly AUC:0.9361
[2024-03-09 15:20:18,726][main.py][line:124][INFO] [Epoch:9/20, Batch:169/320]: loss1:0.0553 loss2:0.2048 loss3:0.4230 | AUC:0.7727 Anomaly AUC:0.9336
[2024-03-09 15:20:33,219][main.py][line:124][INFO] [Epoch:9/20, Batch:179/320]: loss1:0.0075 loss2:0.2002 loss3:0.3935 | AUC:0.7554 Anomaly AUC:0.9369
[2024-03-09 15:20:47,812][main.py][line:124][INFO] [Epoch:9/20, Batch:189/320]: loss1:0.0427 loss2:0.2025 loss3:0.3966 | AUC:0.7179 Anomaly AUC:0.9293
[2024-03-09 15:21:02,362][main.py][line:124][INFO] [Epoch:9/20, Batch:199/320]: loss1:0.0099 loss2:0.1003 loss3:0.3862 | AUC:0.7363 Anomaly AUC:0.9303
[2024-03-09 15:21:16,757][main.py][line:124][INFO] [Epoch:9/20, Batch:209/320]: loss1:0.0312 loss2:0.1182 loss3:0.3951 | AUC:0.7265 Anomaly AUC:0.9308
[2024-03-09 15:21:31,180][main.py][line:124][INFO] [Epoch:9/20, Batch:219/320]: loss1:0.0157 loss2:0.1670 loss3:0.3783 | AUC:0.7301 Anomaly AUC:0.9312
[2024-03-09 15:21:45,599][main.py][line:124][INFO] [Epoch:9/20, Batch:229/320]: loss1:0.0106 loss2:0.1546 loss3:0.4016 | AUC:0.7168 Anomaly AUC:0.9289
[2024-03-09 15:22:00,017][main.py][line:124][INFO] [Epoch:9/20, Batch:239/320]: loss1:0.0106 loss2:0.1145 loss3:0.4066 | AUC:0.7397 Anomaly AUC:0.9298
[2024-03-09 15:22:14,639][main.py][line:124][INFO] [Epoch:9/20, Batch:249/320]: loss1:0.0100 loss2:0.1372 loss3:0.3989 | AUC:0.7163 Anomaly AUC:0.9297
[2024-03-09 15:22:29,169][main.py][line:124][INFO] [Epoch:9/20, Batch:259/320]: loss1:0.0082 loss2:0.1644 loss3:0.3833 | AUC:0.7597 Anomaly AUC:0.9343
[2024-03-09 15:22:43,603][main.py][line:124][INFO] [Epoch:9/20, Batch:269/320]: loss1:0.0106 loss2:0.1229 loss3:0.3944 | AUC:0.7153 Anomaly AUC:0.9292
[2024-03-09 15:22:58,100][main.py][line:124][INFO] [Epoch:9/20, Batch:279/320]: loss1:0.0128 loss2:0.1669 loss3:0.4053 | AUC:0.7297 Anomaly AUC:0.9287
[2024-03-09 15:23:12,667][main.py][line:124][INFO] [Epoch:9/20, Batch:289/320]: loss1:0.0149 loss2:0.1358 loss3:0.3941 | AUC:0.7558 Anomaly AUC:0.9319
[2024-03-09 15:23:26,639][main.py][line:153][INFO] [Epoch:9/20]: lr:0.00010 | loss1:0.0197 loss2:0.1509 loss3:0.3892 | AUC:0.7802 Anomaly AUC:0.9366
[2024-03-09 15:23:41,711][main.py][line:124][INFO] [Epoch:10/20, Batch:9/320]: loss1:0.0141 loss2:0.1454 loss3:0.3978 | AUC:0.7192 Anomaly AUC:0.9310
[2024-03-09 15:23:56,019][main.py][line:124][INFO] [Epoch:10/20, Batch:19/320]: loss1:0.0062 loss2:0.1179 loss3:0.4023 | AUC:0.7835 Anomaly AUC:0.9383
[2024-03-09 15:24:10,494][main.py][line:124][INFO] [Epoch:10/20, Batch:29/320]: loss1:0.0215 loss2:0.1233 loss3:0.3995 | AUC:0.7604 Anomaly AUC:0.9349
[2024-03-09 15:24:25,082][main.py][line:124][INFO] [Epoch:10/20, Batch:39/320]: loss1:0.0089 loss2:0.1352 loss3:0.3896 | AUC:0.7655 Anomaly AUC:0.9359
[2024-03-09 15:24:39,567][main.py][line:124][INFO] [Epoch:10/20, Batch:49/320]: loss1:0.0096 loss2:0.1549 loss3:0.3851 | AUC:0.7768 Anomaly AUC:0.9378
[2024-03-09 15:24:53,990][main.py][line:124][INFO] [Epoch:10/20, Batch:59/320]: loss1:0.0188 loss2:0.0845 loss3:0.3922 | AUC:0.7567 Anomaly AUC:0.9350
[2024-03-09 15:25:08,633][main.py][line:124][INFO] [Epoch:10/20, Batch:69/320]: loss1:0.0312 loss2:0.1011 loss3:0.3975 | AUC:0.7568 Anomaly AUC:0.9314
[2024-03-09 15:25:22,986][main.py][line:124][INFO] [Epoch:10/20, Batch:79/320]: loss1:0.0071 loss2:0.0856 loss3:0.3860 | AUC:0.7445 Anomaly AUC:0.9303
[2024-03-09 15:25:37,532][main.py][line:124][INFO] [Epoch:10/20, Batch:89/320]: loss1:0.0110 loss2:0.1055 loss3:0.3881 | AUC:0.7212 Anomaly AUC:0.9240
[2024-03-09 15:25:51,919][main.py][line:124][INFO] [Epoch:10/20, Batch:99/320]: loss1:0.0309 loss2:0.1542 loss3:0.3815 | AUC:0.7399 Anomaly AUC:0.9289
[2024-03-09 15:26:06,304][main.py][line:124][INFO] [Epoch:10/20, Batch:109/320]: loss1:0.0174 loss2:0.1799 loss3:0.4236 | AUC:0.7792 Anomaly AUC:0.9365
[2024-03-09 15:26:20,747][main.py][line:124][INFO] [Epoch:10/20, Batch:119/320]: loss1:0.0972 loss2:0.1555 loss3:0.4055 | AUC:0.6960 Anomaly AUC:0.9242
[2024-03-09 15:26:35,156][main.py][line:124][INFO] [Epoch:10/20, Batch:129/320]: loss1:0.0116 loss2:0.1568 loss3:0.3942 | AUC:0.7460 Anomaly AUC:0.9278
[2024-03-09 15:26:49,714][main.py][line:124][INFO] [Epoch:10/20, Batch:139/320]: loss1:0.0159 loss2:0.1098 loss3:0.3817 | AUC:0.7775 Anomaly AUC:0.9379
[2024-03-09 15:27:04,202][main.py][line:124][INFO] [Epoch:10/20, Batch:149/320]: loss1:0.0288 loss2:0.1176 loss3:0.3963 | AUC:0.7758 Anomaly AUC:0.9368
[2024-03-09 15:27:18,779][main.py][line:124][INFO] [Epoch:10/20, Batch:159/320]: loss1:0.0343 loss2:0.0997 loss3:0.3867 | AUC:0.6880 Anomaly AUC:0.9210
[2024-03-09 15:27:33,272][main.py][line:124][INFO] [Epoch:10/20, Batch:169/320]: loss1:0.0233 loss2:0.1211 loss3:0.4030 | AUC:0.7195 Anomaly AUC:0.9274
[2024-03-09 15:27:47,823][main.py][line:124][INFO] [Epoch:10/20, Batch:179/320]: loss1:0.0085 loss2:0.1934 loss3:0.3884 | AUC:0.7369 Anomaly AUC:0.9328
[2024-03-09 15:28:02,419][main.py][line:124][INFO] [Epoch:10/20, Batch:189/320]: loss1:0.0357 loss2:0.1756 loss3:0.3800 | AUC:0.7273 Anomaly AUC:0.9286
[2024-03-09 15:28:17,068][main.py][line:124][INFO] [Epoch:10/20, Batch:199/320]: loss1:0.0245 loss2:0.0945 loss3:0.3887 | AUC:0.7025 Anomaly AUC:0.9215
[2024-03-09 15:28:31,735][main.py][line:124][INFO] [Epoch:10/20, Batch:209/320]: loss1:0.0137 loss2:0.1114 loss3:0.3900 | AUC:0.7092 Anomaly AUC:0.9243
[2024-03-09 15:28:46,285][main.py][line:124][INFO] [Epoch:10/20, Batch:219/320]: loss1:0.0070 loss2:0.0793 loss3:0.3851 | AUC:0.7193 Anomaly AUC:0.9290
[2024-03-09 15:29:00,944][main.py][line:124][INFO] [Epoch:10/20, Batch:229/320]: loss1:0.0174 loss2:0.1990 loss3:0.3891 | AUC:0.7238 Anomaly AUC:0.9287
[2024-03-09 15:29:15,257][main.py][line:124][INFO] [Epoch:10/20, Batch:239/320]: loss1:0.0098 loss2:0.1820 loss3:0.3861 | AUC:0.7411 Anomaly AUC:0.9320
[2024-03-09 15:29:29,694][main.py][line:124][INFO] [Epoch:10/20, Batch:249/320]: loss1:0.0168 loss2:0.1847 loss3:0.3785 | AUC:0.7396 Anomaly AUC:0.9312
[2024-03-09 15:29:44,212][main.py][line:124][INFO] [Epoch:10/20, Batch:259/320]: loss1:0.0309 loss2:0.0921 loss3:0.3877 | AUC:0.7248 Anomaly AUC:0.9261
[2024-03-09 15:29:58,850][main.py][line:124][INFO] [Epoch:10/20, Batch:269/320]: loss1:0.0441 loss2:0.1807 loss3:0.3823 | AUC:0.7377 Anomaly AUC:0.9302
[2024-03-09 15:30:13,226][main.py][line:124][INFO] [Epoch:10/20, Batch:279/320]: loss1:0.0105 loss2:0.0816 loss3:0.3827 | AUC:0.6838 Anomaly AUC:0.9255
[2024-03-09 15:30:27,846][main.py][line:124][INFO] [Epoch:10/20, Batch:289/320]: loss1:0.0109 loss2:0.1209 loss3:0.3795 | AUC:0.7168 Anomaly AUC:0.9320
[2024-03-09 15:30:41,934][main.py][line:153][INFO] [Epoch:10/20]: lr:0.00010 | loss1:0.1322 loss2:0.1793 loss3:0.3902 | AUC:0.7425 Anomaly AUC:0.9283
[2024-03-09 15:30:56,945][main.py][line:124][INFO] [Epoch:11/20, Batch:9/320]: loss1:0.0607 loss2:0.1525 loss3:0.3952 | AUC:0.7595 Anomaly AUC:0.9325
[2024-03-09 15:31:11,507][main.py][line:124][INFO] [Epoch:11/20, Batch:19/320]: loss1:0.0776 loss2:0.2296 loss3:0.3935 | AUC:0.7562 Anomaly AUC:0.9264
[2024-03-09 15:31:25,896][main.py][line:124][INFO] [Epoch:11/20, Batch:29/320]: loss1:0.0962 loss2:0.1379 loss3:0.3901 | AUC:0.7405 Anomaly AUC:0.9245
[2024-03-09 15:31:40,319][main.py][line:124][INFO] [Epoch:11/20, Batch:39/320]: loss1:0.0290 loss2:0.1315 loss3:0.3947 | AUC:0.8014 Anomaly AUC:0.9357
[2024-03-09 15:31:54,822][main.py][line:124][INFO] [Epoch:11/20, Batch:49/320]: loss1:0.0223 loss2:0.1374 loss3:0.3886 | AUC:0.8191 Anomaly AUC:0.9471
[2024-03-09 15:32:09,392][main.py][line:124][INFO] [Epoch:11/20, Batch:59/320]: loss1:0.0257 loss2:0.1378 loss3:0.3865 | AUC:0.7714 Anomaly AUC:0.9405
[2024-03-09 15:32:24,018][main.py][line:124][INFO] [Epoch:11/20, Batch:69/320]: loss1:0.0267 loss2:0.0920 loss3:0.3840 | AUC:0.7347 Anomaly AUC:0.9334
[2024-03-09 15:32:38,378][main.py][line:124][INFO] [Epoch:11/20, Batch:79/320]: loss1:0.0332 loss2:0.0986 loss3:0.3845 | AUC:0.7116 Anomaly AUC:0.9294
[2024-03-09 15:32:53,014][main.py][line:124][INFO] [Epoch:11/20, Batch:89/320]: loss1:0.0111 loss2:0.1428 loss3:0.3853 | AUC:0.7127 Anomaly AUC:0.9285
[2024-03-09 15:33:07,568][main.py][line:124][INFO] [Epoch:11/20, Batch:99/320]: loss1:0.0194 loss2:0.1242 loss3:0.3900 | AUC:0.6864 Anomaly AUC:0.9264
[2024-03-09 15:33:22,089][main.py][line:124][INFO] [Epoch:11/20, Batch:109/320]: loss1:0.0095 loss2:0.0973 loss3:0.3934 | AUC:0.7147 Anomaly AUC:0.9253
[2024-03-09 15:33:36,635][main.py][line:124][INFO] [Epoch:11/20, Batch:119/320]: loss1:0.0136 loss2:0.1011 loss3:0.3852 | AUC:0.7160 Anomaly AUC:0.9236
[2024-03-09 15:33:51,102][main.py][line:124][INFO] [Epoch:11/20, Batch:129/320]: loss1:0.0735 loss2:0.1334 loss3:0.3928 | AUC:0.7456 Anomaly AUC:0.9270
[2024-03-09 15:34:05,637][main.py][line:124][INFO] [Epoch:11/20, Batch:139/320]: loss1:0.0181 loss2:0.1001 loss3:0.3893 | AUC:0.7058 Anomaly AUC:0.9251
[2024-03-09 15:34:20,139][main.py][line:124][INFO] [Epoch:11/20, Batch:149/320]: loss1:0.0129 loss2:0.0918 loss3:0.3948 | AUC:0.7471 Anomaly AUC:0.9333
[2024-03-09 15:34:34,853][main.py][line:124][INFO] [Epoch:11/20, Batch:159/320]: loss1:0.0205 loss2:0.1156 loss3:0.3807 | AUC:0.7521 Anomaly AUC:0.9349
[2024-03-09 15:34:49,368][main.py][line:124][INFO] [Epoch:11/20, Batch:169/320]: loss1:0.0272 loss2:0.0888 loss3:0.3920 | AUC:0.7672 Anomaly AUC:0.9348
[2024-03-09 15:35:03,775][main.py][line:124][INFO] [Epoch:11/20, Batch:179/320]: loss1:0.0099 loss2:0.1215 loss3:0.3854 | AUC:0.7686 Anomaly AUC:0.9355
[2024-03-09 15:35:18,154][main.py][line:124][INFO] [Epoch:11/20, Batch:189/320]: loss1:0.0105 loss2:0.0876 loss3:0.3874 | AUC:0.7355 Anomaly AUC:0.9324
[2024-03-09 15:35:32,718][main.py][line:124][INFO] [Epoch:11/20, Batch:199/320]: loss1:0.0134 loss2:0.1060 loss3:0.3854 | AUC:0.7406 Anomaly AUC:0.9348
[2024-03-09 15:35:47,170][main.py][line:124][INFO] [Epoch:11/20, Batch:209/320]: loss1:0.0102 loss2:0.1136 loss3:0.3892 | AUC:0.7459 Anomaly AUC:0.9353
[2024-03-09 15:36:01,611][main.py][line:124][INFO] [Epoch:11/20, Batch:219/320]: loss1:0.0115 loss2:0.1020 loss3:0.3797 | AUC:0.7505 Anomaly AUC:0.9353
[2024-03-09 15:36:16,020][main.py][line:124][INFO] [Epoch:11/20, Batch:229/320]: loss1:0.0091 loss2:0.0829 loss3:0.3823 | AUC:0.7453 Anomaly AUC:0.9342
[2024-03-09 15:36:30,576][main.py][line:124][INFO] [Epoch:11/20, Batch:239/320]: loss1:0.0080 loss2:0.0949 loss3:0.3906 | AUC:0.7322 Anomaly AUC:0.9328
[2024-03-09 15:36:45,108][main.py][line:124][INFO] [Epoch:11/20, Batch:249/320]: loss1:0.0145 loss2:0.0964 loss3:0.3793 | AUC:0.7474 Anomaly AUC:0.9325
[2024-03-09 15:36:59,624][main.py][line:124][INFO] [Epoch:11/20, Batch:259/320]: loss1:0.0086 loss2:0.0836 loss3:0.3813 | AUC:0.7371 Anomaly AUC:0.9318
[2024-03-09 15:37:14,120][main.py][line:124][INFO] [Epoch:11/20, Batch:269/320]: loss1:0.0120 loss2:0.0996 loss3:0.3870 | AUC:0.7533 Anomaly AUC:0.9342
[2024-03-09 15:37:28,644][main.py][line:124][INFO] [Epoch:11/20, Batch:279/320]: loss1:0.0134 loss2:0.0745 loss3:0.3914 | AUC:0.7416 Anomaly AUC:0.9331
[2024-03-09 15:37:43,229][main.py][line:124][INFO] [Epoch:11/20, Batch:289/320]: loss1:0.0159 loss2:0.0838 loss3:0.4020 | AUC:0.6953 Anomaly AUC:0.9241
[2024-03-09 15:37:57,131][main.py][line:153][INFO] [Epoch:11/20]: lr:0.00010 | loss1:0.0141 loss2:0.0734 loss3:0.3838 | AUC:0.7149 Anomaly AUC:0.9271
[2024-03-09 15:38:12,452][main.py][line:124][INFO] [Epoch:12/20, Batch:9/320]: loss1:0.0258 loss2:0.0571 loss3:0.3739 | AUC:0.7638 Anomaly AUC:0.9339
[2024-03-09 15:38:27,029][main.py][line:124][INFO] [Epoch:12/20, Batch:19/320]: loss1:0.0283 loss2:0.0779 loss3:0.3868 | AUC:0.7697 Anomaly AUC:0.9343
[2024-03-09 15:38:41,755][main.py][line:124][INFO] [Epoch:12/20, Batch:29/320]: loss1:0.0242 loss2:0.0687 loss3:0.3827 | AUC:0.7112 Anomaly AUC:0.9253
[2024-03-09 15:38:56,305][main.py][line:124][INFO] [Epoch:12/20, Batch:39/320]: loss1:0.0083 loss2:0.1054 loss3:0.3851 | AUC:0.6972 Anomaly AUC:0.9235
[2024-03-09 15:39:10,919][main.py][line:124][INFO] [Epoch:12/20, Batch:49/320]: loss1:0.0115 loss2:0.0691 loss3:0.3870 | AUC:0.7226 Anomaly AUC:0.9271
[2024-03-09 15:39:25,330][main.py][line:124][INFO] [Epoch:12/20, Batch:59/320]: loss1:0.0149 loss2:0.0773 loss3:0.3813 | AUC:0.7363 Anomaly AUC:0.9299
[2024-03-09 15:39:39,711][main.py][line:124][INFO] [Epoch:12/20, Batch:69/320]: loss1:0.0062 loss2:0.0737 loss3:0.3822 | AUC:0.7467 Anomaly AUC:0.9314
[2024-03-09 15:39:54,056][main.py][line:124][INFO] [Epoch:12/20, Batch:79/320]: loss1:0.0153 loss2:0.0592 loss3:0.3779 | AUC:0.7862 Anomaly AUC:0.9383
[2024-03-09 15:40:08,570][main.py][line:124][INFO] [Epoch:12/20, Batch:89/320]: loss1:0.0268 loss2:0.0732 loss3:0.3718 | AUC:0.7681 Anomaly AUC:0.9364
[2024-03-09 15:40:23,112][main.py][line:124][INFO] [Epoch:12/20, Batch:99/320]: loss1:0.0093 loss2:0.1150 loss3:0.3788 | AUC:0.7154 Anomaly AUC:0.9262
[2024-03-09 15:40:37,491][main.py][line:124][INFO] [Epoch:12/20, Batch:109/320]: loss1:0.0142 loss2:0.1025 loss3:0.3856 | AUC:0.7696 Anomaly AUC:0.9361
[2024-03-09 15:40:51,838][main.py][line:124][INFO] [Epoch:12/20, Batch:119/320]: loss1:0.0268 loss2:0.0916 loss3:0.3810 | AUC:0.6445 Anomaly AUC:0.9098
[2024-03-09 15:41:06,378][main.py][line:124][INFO] [Epoch:12/20, Batch:129/320]: loss1:0.0119 loss2:0.0893 loss3:0.3854 | AUC:0.6942 Anomaly AUC:0.9260
[2024-03-09 15:41:20,952][main.py][line:124][INFO] [Epoch:12/20, Batch:139/320]: loss1:0.0092 loss2:0.0664 loss3:0.3804 | AUC:0.6872 Anomaly AUC:0.9248
[2024-03-09 15:41:35,601][main.py][line:124][INFO] [Epoch:12/20, Batch:149/320]: loss1:0.0134 loss2:0.0991 loss3:0.3900 | AUC:0.6966 Anomaly AUC:0.9259
[2024-03-09 15:41:50,187][main.py][line:124][INFO] [Epoch:12/20, Batch:159/320]: loss1:0.0208 loss2:0.0608 loss3:0.3868 | AUC:0.6877 Anomaly AUC:0.9213
[2024-03-09 15:42:04,769][main.py][line:124][INFO] [Epoch:12/20, Batch:169/320]: loss1:0.0618 loss2:0.1289 loss3:0.3802 | AUC:0.6985 Anomaly AUC:0.9246
[2024-03-09 15:42:19,279][main.py][line:124][INFO] [Epoch:12/20, Batch:179/320]: loss1:0.3279 loss2:0.2808 loss3:0.3914 | AUC:0.6822 Anomaly AUC:0.9189
[2024-03-09 15:42:33,687][main.py][line:124][INFO] [Epoch:12/20, Batch:189/320]: loss1:0.0312 loss2:0.1730 loss3:0.4163 | AUC:0.7030 Anomaly AUC:0.9261
[2024-03-09 15:42:48,079][main.py][line:124][INFO] [Epoch:12/20, Batch:199/320]: loss1:0.0345 loss2:0.1235 loss3:0.3841 | AUC:0.7471 Anomaly AUC:0.9281
[2024-03-09 15:43:02,451][main.py][line:124][INFO] [Epoch:12/20, Batch:209/320]: loss1:0.0601 loss2:0.1364 loss3:0.3849 | AUC:0.7675 Anomaly AUC:0.9310
[2024-03-09 15:43:16,979][main.py][line:124][INFO] [Epoch:12/20, Batch:219/320]: loss1:0.0751 loss2:0.1614 loss3:0.3921 | AUC:0.7701 Anomaly AUC:0.9350
[2024-03-09 15:43:31,772][main.py][line:124][INFO] [Epoch:12/20, Batch:229/320]: loss1:0.0331 loss2:0.1040 loss3:0.3837 | AUC:0.7085 Anomaly AUC:0.9202
[2024-03-09 15:43:46,458][main.py][line:124][INFO] [Epoch:12/20, Batch:239/320]: loss1:0.0231 loss2:0.1564 loss3:0.3824 | AUC:0.7444 Anomaly AUC:0.9198
[2024-03-09 15:44:01,051][main.py][line:124][INFO] [Epoch:12/20, Batch:249/320]: loss1:0.0219 loss2:0.1187 loss3:0.3916 | AUC:0.7559 Anomaly AUC:0.9366
[2024-03-09 15:44:15,974][main.py][line:124][INFO] [Epoch:12/20, Batch:259/320]: loss1:0.0410 loss2:0.0856 loss3:0.3873 | AUC:0.6946 Anomaly AUC:0.9179
[2024-03-09 15:44:31,153][main.py][line:124][INFO] [Epoch:12/20, Batch:269/320]: loss1:0.0265 loss2:0.0620 loss3:0.3974 | AUC:0.7799 Anomaly AUC:0.9266
[2024-03-09 15:44:45,798][main.py][line:124][INFO] [Epoch:12/20, Batch:279/320]: loss1:0.0656 loss2:0.1294 loss3:0.3812 | AUC:0.7964 Anomaly AUC:0.9308
[2024-03-09 15:45:00,462][main.py][line:124][INFO] [Epoch:12/20, Batch:289/320]: loss1:0.0105 loss2:0.1552 loss3:0.3905 | AUC:0.8344 Anomaly AUC:0.9459
[2024-03-09 15:45:15,330][main.py][line:153][INFO] [Epoch:12/20]: lr:0.00010 | loss1:0.0383 loss2:0.0800 loss3:0.3887 | AUC:0.8257 Anomaly AUC:0.9459
[2024-03-09 15:45:30,648][main.py][line:124][INFO] [Epoch:13/20, Batch:9/320]: loss1:0.0055 loss2:0.0868 loss3:0.3842 | AUC:0.7557 Anomaly AUC:0.9369
[2024-03-09 15:45:45,194][main.py][line:124][INFO] [Epoch:13/20, Batch:19/320]: loss1:0.0134 loss2:0.0975 loss3:0.3757 | AUC:0.7159 Anomaly AUC:0.9316
[2024-03-09 15:46:00,012][main.py][line:124][INFO] [Epoch:13/20, Batch:29/320]: loss1:0.0093 loss2:0.0544 loss3:0.3770 | AUC:0.7039 Anomaly AUC:0.9278
[2024-03-09 15:46:14,450][main.py][line:124][INFO] [Epoch:13/20, Batch:39/320]: loss1:0.0162 loss2:0.0828 loss3:0.3856 | AUC:0.7364 Anomaly AUC:0.9304
[2024-03-09 15:46:29,220][main.py][line:124][INFO] [Epoch:13/20, Batch:49/320]: loss1:0.0105 loss2:0.0880 loss3:0.3842 | AUC:0.7321 Anomaly AUC:0.9307
[2024-03-09 15:46:43,934][main.py][line:124][INFO] [Epoch:13/20, Batch:59/320]: loss1:0.0094 loss2:0.0687 loss3:0.3848 | AUC:0.7551 Anomaly AUC:0.9337
[2024-03-09 15:46:58,444][main.py][line:124][INFO] [Epoch:13/20, Batch:69/320]: loss1:0.0606 loss2:0.1093 loss3:0.3869 | AUC:0.7775 Anomaly AUC:0.9358
[2024-03-09 15:47:13,137][main.py][line:124][INFO] [Epoch:13/20, Batch:79/320]: loss1:0.0299 loss2:0.0810 loss3:0.3823 | AUC:0.6844 Anomaly AUC:0.9210
[2024-03-09 15:47:27,743][main.py][line:124][INFO] [Epoch:13/20, Batch:89/320]: loss1:0.0085 loss2:0.0849 loss3:0.3858 | AUC:0.6904 Anomaly AUC:0.9223
[2024-03-09 15:47:42,361][main.py][line:124][INFO] [Epoch:13/20, Batch:99/320]: loss1:0.0077 loss2:0.0864 loss3:0.3778 | AUC:0.7174 Anomaly AUC:0.9254
[2024-03-09 15:47:56,935][main.py][line:124][INFO] [Epoch:13/20, Batch:109/320]: loss1:0.0120 loss2:0.0671 loss3:0.3822 | AUC:0.7063 Anomaly AUC:0.9248
[2024-03-09 15:48:11,436][main.py][line:124][INFO] [Epoch:13/20, Batch:119/320]: loss1:0.0099 loss2:0.0630 loss3:0.3860 | AUC:0.7175 Anomaly AUC:0.9254
[2024-03-09 15:48:26,226][main.py][line:124][INFO] [Epoch:13/20, Batch:129/320]: loss1:0.0263 loss2:0.0790 loss3:0.3826 | AUC:0.7262 Anomaly AUC:0.9252
[2024-03-09 15:48:40,706][main.py][line:124][INFO] [Epoch:13/20, Batch:139/320]: loss1:0.0085 loss2:0.0705 loss3:0.3782 | AUC:0.7232 Anomaly AUC:0.9246
[2024-03-09 15:48:55,110][main.py][line:124][INFO] [Epoch:13/20, Batch:149/320]: loss1:0.0163 loss2:0.0929 loss3:0.3830 | AUC:0.7302 Anomaly AUC:0.9274
[2024-03-09 15:49:09,548][main.py][line:124][INFO] [Epoch:13/20, Batch:159/320]: loss1:0.0119 loss2:0.0710 loss3:0.3781 | AUC:0.7362 Anomaly AUC:0.9311
[2024-03-09 15:49:24,015][main.py][line:124][INFO] [Epoch:13/20, Batch:169/320]: loss1:0.0122 loss2:0.0895 loss3:0.3765 | AUC:0.7587 Anomaly AUC:0.9338
[2024-03-09 15:49:38,703][main.py][line:124][INFO] [Epoch:13/20, Batch:179/320]: loss1:0.1306 loss2:0.1398 loss3:0.3871 | AUC:0.7615 Anomaly AUC:0.9354
[2024-03-09 15:49:53,302][main.py][line:124][INFO] [Epoch:13/20, Batch:189/320]: loss1:0.0120 loss2:0.0564 loss3:0.3829 | AUC:0.7844 Anomaly AUC:0.9388
[2024-03-09 15:50:07,843][main.py][line:124][INFO] [Epoch:13/20, Batch:199/320]: loss1:0.0102 loss2:0.0845 loss3:0.3835 | AUC:0.7397 Anomaly AUC:0.9318
[2024-03-09 15:50:22,288][main.py][line:124][INFO] [Epoch:13/20, Batch:209/320]: loss1:0.0191 loss2:0.1027 loss3:0.3911 | AUC:0.7452 Anomaly AUC:0.9303
[2024-03-09 15:50:36,662][main.py][line:124][INFO] [Epoch:13/20, Batch:219/320]: loss1:0.0077 loss2:0.0544 loss3:0.3894 | AUC:0.7549 Anomaly AUC:0.9322
[2024-03-09 15:50:51,124][main.py][line:124][INFO] [Epoch:13/20, Batch:229/320]: loss1:0.0097 loss2:0.0844 loss3:0.3873 | AUC:0.7576 Anomaly AUC:0.9323
[2024-03-09 15:51:05,601][main.py][line:124][INFO] [Epoch:13/20, Batch:239/320]: loss1:0.0081 loss2:0.0561 loss3:0.3866 | AUC:0.7408 Anomaly AUC:0.9311
[2024-03-09 15:51:19,920][main.py][line:124][INFO] [Epoch:13/20, Batch:249/320]: loss1:0.0115 loss2:0.0713 loss3:0.3898 | AUC:0.7429 Anomaly AUC:0.9312
[2024-03-09 15:51:34,631][main.py][line:124][INFO] [Epoch:13/20, Batch:259/320]: loss1:0.0062 loss2:0.0706 loss3:0.3872 | AUC:0.7564 Anomaly AUC:0.9320
[2024-03-09 15:51:49,084][main.py][line:124][INFO] [Epoch:13/20, Batch:269/320]: loss1:0.0399 loss2:0.0866 loss3:0.3766 | AUC:0.7462 Anomaly AUC:0.9251
[2024-03-09 15:52:03,839][main.py][line:124][INFO] [Epoch:13/20, Batch:279/320]: loss1:0.0044 loss2:0.0575 loss3:0.3893 | AUC:0.7047 Anomaly AUC:0.9264
[2024-03-09 15:52:18,219][main.py][line:124][INFO] [Epoch:13/20, Batch:289/320]: loss1:0.0066 loss2:0.0968 loss3:0.3928 | AUC:0.7147 Anomaly AUC:0.9285
[2024-03-09 15:52:32,572][main.py][line:153][INFO] [Epoch:13/20]: lr:0.00010 | loss1:0.0136 loss2:0.1394 loss3:0.3754 | AUC:0.6893 Anomaly AUC:0.9245
[2024-03-09 15:52:47,284][main.py][line:124][INFO] [Epoch:14/20, Batch:9/320]: loss1:0.0092 loss2:0.0874 loss3:0.3754 | AUC:0.7119 Anomaly AUC:0.9273
[2024-03-09 15:53:01,579][main.py][line:124][INFO] [Epoch:14/20, Batch:19/320]: loss1:0.0078 loss2:0.0977 loss3:0.3761 | AUC:0.7295 Anomaly AUC:0.9301
[2024-03-09 15:53:16,109][main.py][line:124][INFO] [Epoch:14/20, Batch:29/320]: loss1:0.0163 loss2:0.0752 loss3:0.3735 | AUC:0.7373 Anomaly AUC:0.9307
[2024-03-09 15:53:30,681][main.py][line:124][INFO] [Epoch:14/20, Batch:39/320]: loss1:0.0081 loss2:0.0682 loss3:0.3775 | AUC:0.7264 Anomaly AUC:0.9299
[2024-03-09 15:53:45,199][main.py][line:124][INFO] [Epoch:14/20, Batch:49/320]: loss1:0.0118 loss2:0.0536 loss3:0.3783 | AUC:0.7098 Anomaly AUC:0.9268
[2024-03-09 15:53:59,766][main.py][line:124][INFO] [Epoch:14/20, Batch:59/320]: loss1:0.0068 loss2:0.0652 loss3:0.3759 | AUC:0.7239 Anomaly AUC:0.9294
[2024-03-09 15:54:14,461][main.py][line:124][INFO] [Epoch:14/20, Batch:69/320]: loss1:0.0133 loss2:0.1035 loss3:0.3761 | AUC:0.7362 Anomaly AUC:0.9300
[2024-03-09 15:54:29,121][main.py][line:124][INFO] [Epoch:14/20, Batch:79/320]: loss1:0.0162 loss2:0.0692 loss3:0.3747 | AUC:0.7461 Anomaly AUC:0.9306
[2024-03-09 15:54:43,749][main.py][line:124][INFO] [Epoch:14/20, Batch:89/320]: loss1:0.0094 loss2:0.0945 loss3:0.3746 | AUC:0.7238 Anomaly AUC:0.9292
[2024-03-09 15:54:58,361][main.py][line:124][INFO] [Epoch:14/20, Batch:99/320]: loss1:0.0065 loss2:0.0461 loss3:0.3750 | AUC:0.7256 Anomaly AUC:0.9287
[2024-03-09 15:55:13,024][main.py][line:124][INFO] [Epoch:14/20, Batch:109/320]: loss1:0.0086 loss2:0.0454 loss3:0.3730 | AUC:0.6975 Anomaly AUC:0.9259
[2024-03-09 15:55:27,602][main.py][line:124][INFO] [Epoch:14/20, Batch:119/320]: loss1:0.0089 loss2:0.0906 loss3:0.3735 | AUC:0.7023 Anomaly AUC:0.9254
[2024-03-09 15:55:42,367][main.py][line:124][INFO] [Epoch:14/20, Batch:129/320]: loss1:0.0077 loss2:0.0660 loss3:0.3721 | AUC:0.6929 Anomaly AUC:0.9243
[2024-03-09 15:55:56,985][main.py][line:124][INFO] [Epoch:14/20, Batch:139/320]: loss1:0.0091 loss2:0.0820 loss3:0.3828 | AUC:0.7136 Anomaly AUC:0.9269
[2024-03-09 15:56:11,307][main.py][line:124][INFO] [Epoch:14/20, Batch:149/320]: loss1:0.0089 loss2:0.0545 loss3:0.3780 | AUC:0.7646 Anomaly AUC:0.9295
[2024-03-09 15:56:25,626][main.py][line:124][INFO] [Epoch:14/20, Batch:159/320]: loss1:0.0093 loss2:0.0625 loss3:0.3711 | AUC:0.7269 Anomaly AUC:0.9209
[2024-03-09 15:56:40,018][main.py][line:124][INFO] [Epoch:14/20, Batch:169/320]: loss1:0.0102 loss2:0.0791 loss3:0.3754 | AUC:0.7385 Anomaly AUC:0.9222
[2024-03-09 15:56:54,522][main.py][line:124][INFO] [Epoch:14/20, Batch:179/320]: loss1:0.0081 loss2:0.0459 loss3:0.3797 | AUC:0.7402 Anomaly AUC:0.9236
[2024-03-09 15:57:08,954][main.py][line:124][INFO] [Epoch:14/20, Batch:189/320]: loss1:0.0082 loss2:0.0529 loss3:0.3780 | AUC:0.7315 Anomaly AUC:0.9238
[2024-03-09 15:57:23,608][main.py][line:124][INFO] [Epoch:14/20, Batch:199/320]: loss1:0.0105 loss2:0.0655 loss3:0.3775 | AUC:0.7616 Anomaly AUC:0.9319
[2024-03-09 15:57:38,099][main.py][line:124][INFO] [Epoch:14/20, Batch:209/320]: loss1:0.0122 loss2:0.0898 loss3:0.3727 | AUC:0.7412 Anomaly AUC:0.9289
[2024-03-09 15:57:52,599][main.py][line:124][INFO] [Epoch:14/20, Batch:219/320]: loss1:0.0262 loss2:0.0643 loss3:0.3743 | AUC:0.7359 Anomaly AUC:0.9290
[2024-03-09 15:58:06,980][main.py][line:124][INFO] [Epoch:14/20, Batch:229/320]: loss1:0.0074 loss2:0.0359 loss3:0.3807 | AUC:0.7354 Anomaly AUC:0.9272
[2024-03-09 15:58:21,380][main.py][line:124][INFO] [Epoch:14/20, Batch:239/320]: loss1:0.0159 loss2:0.0476 loss3:0.3806 | AUC:0.7259 Anomaly AUC:0.9266
[2024-03-09 15:58:35,829][main.py][line:124][INFO] [Epoch:14/20, Batch:249/320]: loss1:0.0357 loss2:0.0460 loss3:0.3763 | AUC:0.7592 Anomaly AUC:0.9316
[2024-03-09 15:58:50,318][main.py][line:124][INFO] [Epoch:14/20, Batch:259/320]: loss1:0.0119 loss2:0.1243 loss3:0.3845 | AUC:0.7533 Anomaly AUC:0.9338
[2024-03-09 15:59:04,901][main.py][line:124][INFO] [Epoch:14/20, Batch:269/320]: loss1:0.0543 loss2:0.0842 loss3:0.3728 | AUC:0.6810 Anomaly AUC:0.9266
[2024-03-09 15:59:19,265][main.py][line:124][INFO] [Epoch:14/20, Batch:279/320]: loss1:0.0604 loss2:0.1590 loss3:0.3809 | AUC:0.6943 Anomaly AUC:0.9260
[2024-03-09 15:59:33,737][main.py][line:124][INFO] [Epoch:14/20, Batch:289/320]: loss1:0.0607 loss2:0.0691 loss3:0.3800 | AUC:0.7471 Anomaly AUC:0.9300
[2024-03-09 15:59:47,645][main.py][line:153][INFO] [Epoch:14/20]: lr:0.00010 | loss1:0.0109 loss2:0.1192 loss3:0.3978 | AUC:0.7620 Anomaly AUC:0.9340
[2024-03-09 16:00:02,554][main.py][line:124][INFO] [Epoch:15/20, Batch:9/320]: loss1:0.0525 loss2:0.1590 loss3:0.3824 | AUC:0.8148 Anomaly AUC:0.9441
[2024-03-09 16:00:17,147][main.py][line:124][INFO] [Epoch:15/20, Batch:19/320]: loss1:0.0281 loss2:0.0857 loss3:0.3770 | AUC:0.7563 Anomaly AUC:0.9360
[2024-03-09 16:00:31,747][main.py][line:124][INFO] [Epoch:15/20, Batch:29/320]: loss1:0.0195 loss2:0.0632 loss3:0.3739 | AUC:0.7335 Anomaly AUC:0.9290
[2024-03-09 16:00:46,435][main.py][line:124][INFO] [Epoch:15/20, Batch:39/320]: loss1:0.0145 loss2:0.0561 loss3:0.3795 | AUC:0.7654 Anomaly AUC:0.9328
[2024-03-09 16:01:00,885][main.py][line:124][INFO] [Epoch:15/20, Batch:49/320]: loss1:0.0098 loss2:0.0610 loss3:0.3787 | AUC:0.7466 Anomaly AUC:0.9282
[2024-03-09 16:01:15,434][main.py][line:124][INFO] [Epoch:15/20, Batch:59/320]: loss1:0.0351 loss2:0.0850 loss3:0.3821 | AUC:0.7108 Anomaly AUC:0.9121
[2024-03-09 16:01:29,975][main.py][line:124][INFO] [Epoch:15/20, Batch:69/320]: loss1:0.0239 loss2:0.1227 loss3:0.3817 | AUC:0.7448 Anomaly AUC:0.9302
[2024-03-09 16:01:44,523][main.py][line:124][INFO] [Epoch:15/20, Batch:79/320]: loss1:0.1382 loss2:0.1520 loss3:0.3773 | AUC:0.7890 Anomaly AUC:0.9436
[2024-03-09 16:01:59,106][main.py][line:124][INFO] [Epoch:15/20, Batch:89/320]: loss1:0.0383 loss2:0.1295 loss3:0.3809 | AUC:0.7696 Anomaly AUC:0.9397
[2024-03-09 16:02:13,772][main.py][line:124][INFO] [Epoch:15/20, Batch:99/320]: loss1:0.0630 loss2:0.1173 loss3:0.3854 | AUC:0.7302 Anomaly AUC:0.9241
[2024-03-09 16:02:28,369][main.py][line:124][INFO] [Epoch:15/20, Batch:109/320]: loss1:0.0110 loss2:0.0849 loss3:0.3850 | AUC:0.7484 Anomaly AUC:0.9321
[2024-03-09 16:02:43,061][main.py][line:124][INFO] [Epoch:15/20, Batch:119/320]: loss1:0.0371 loss2:0.0666 loss3:0.3792 | AUC:0.7156 Anomaly AUC:0.9215
[2024-03-09 16:02:57,589][main.py][line:124][INFO] [Epoch:15/20, Batch:129/320]: loss1:0.0350 loss2:0.0864 loss3:0.3779 | AUC:0.7378 Anomaly AUC:0.9248
[2024-03-09 16:03:11,977][main.py][line:124][INFO] [Epoch:15/20, Batch:139/320]: loss1:0.0092 loss2:0.0873 loss3:0.3888 | AUC:0.7620 Anomaly AUC:0.9314
[2024-03-09 16:03:26,354][main.py][line:124][INFO] [Epoch:15/20, Batch:149/320]: loss1:0.0332 loss2:0.0810 loss3:0.3786 | AUC:0.7632 Anomaly AUC:0.9307
[2024-03-09 16:03:40,727][main.py][line:124][INFO] [Epoch:15/20, Batch:159/320]: loss1:0.0166 loss2:0.0677 loss3:0.3880 | AUC:0.7744 Anomaly AUC:0.9325
[2024-03-09 16:03:55,201][main.py][line:124][INFO] [Epoch:15/20, Batch:169/320]: loss1:0.0134 loss2:0.0771 loss3:0.3850 | AUC:0.7857 Anomaly AUC:0.9351
[2024-03-09 16:04:09,660][main.py][line:124][INFO] [Epoch:15/20, Batch:179/320]: loss1:0.0075 loss2:0.0697 loss3:0.3791 | AUC:0.7900 Anomaly AUC:0.9366
[2024-03-09 16:04:24,197][main.py][line:124][INFO] [Epoch:15/20, Batch:189/320]: loss1:0.0115 loss2:0.0511 loss3:0.3988 | AUC:0.7289 Anomaly AUC:0.9239
[2024-03-09 16:04:38,657][main.py][line:124][INFO] [Epoch:15/20, Batch:199/320]: loss1:0.0147 loss2:0.0796 loss3:0.3786 | AUC:0.6881 Anomaly AUC:0.9187
[2024-03-09 16:04:53,061][main.py][line:124][INFO] [Epoch:15/20, Batch:209/320]: loss1:0.0083 loss2:0.1019 loss3:0.3850 | AUC:0.6957 Anomaly AUC:0.9203
[2024-03-09 16:05:07,574][main.py][line:124][INFO] [Epoch:15/20, Batch:219/320]: loss1:0.0079 loss2:0.0817 loss3:0.3823 | AUC:0.7129 Anomaly AUC:0.9240
[2024-03-09 16:05:22,077][main.py][line:124][INFO] [Epoch:15/20, Batch:229/320]: loss1:0.0098 loss2:0.0796 loss3:0.3808 | AUC:0.7394 Anomaly AUC:0.9290
[2024-03-09 16:05:36,642][main.py][line:124][INFO] [Epoch:15/20, Batch:239/320]: loss1:0.0150 loss2:0.0701 loss3:0.3809 | AUC:0.7607 Anomaly AUC:0.9324
[2024-03-09 16:05:51,016][main.py][line:124][INFO] [Epoch:15/20, Batch:249/320]: loss1:0.0179 loss2:0.0609 loss3:0.3929 | AUC:0.7600 Anomaly AUC:0.9352
[2024-03-09 16:06:05,820][main.py][line:124][INFO] [Epoch:15/20, Batch:259/320]: loss1:0.1501 loss2:0.2992 loss3:0.4023 | AUC:0.7450 Anomaly AUC:0.9365
[2024-03-09 16:06:20,310][main.py][line:124][INFO] [Epoch:15/20, Batch:269/320]: loss1:0.0553 loss2:0.0825 loss3:0.3811 | AUC:0.7490 Anomaly AUC:0.9360
[2024-03-09 16:06:34,908][main.py][line:124][INFO] [Epoch:15/20, Batch:279/320]: loss1:0.0213 loss2:0.0847 loss3:0.3840 | AUC:0.7272 Anomaly AUC:0.9306
[2024-03-09 16:06:49,372][main.py][line:124][INFO] [Epoch:15/20, Batch:289/320]: loss1:0.0298 loss2:0.1146 loss3:0.3808 | AUC:0.6835 Anomaly AUC:0.9217
[2024-03-09 16:07:03,545][main.py][line:153][INFO] [Epoch:15/20]: lr:0.00010 | loss1:0.0123 loss2:0.0657 loss3:0.3775 | AUC:0.6769 Anomaly AUC:0.9204
[2024-03-09 16:07:18,580][main.py][line:124][INFO] [Epoch:16/20, Batch:9/320]: loss1:0.0377 loss2:0.0578 loss3:0.3775 | AUC:0.6773 Anomaly AUC:0.9205
[2024-03-09 16:07:33,064][main.py][line:124][INFO] [Epoch:16/20, Batch:19/320]: loss1:0.0104 loss2:0.0768 loss3:0.3815 | AUC:0.6923 Anomaly AUC:0.9210
[2024-03-09 16:07:47,518][main.py][line:124][INFO] [Epoch:16/20, Batch:29/320]: loss1:0.0153 loss2:0.0730 loss3:0.3820 | AUC:0.6870 Anomaly AUC:0.9201
[2024-03-09 16:08:01,985][main.py][line:124][INFO] [Epoch:16/20, Batch:39/320]: loss1:0.0143 loss2:0.1137 loss3:0.3824 | AUC:0.6997 Anomaly AUC:0.9200
[2024-03-09 16:08:16,379][main.py][line:124][INFO] [Epoch:16/20, Batch:49/320]: loss1:0.0123 loss2:0.0592 loss3:0.3818 | AUC:0.7117 Anomaly AUC:0.9218
[2024-03-09 16:08:30,818][main.py][line:124][INFO] [Epoch:16/20, Batch:59/320]: loss1:0.0123 loss2:0.0585 loss3:0.3835 | AUC:0.7037 Anomaly AUC:0.9213
[2024-03-09 16:08:45,165][main.py][line:124][INFO] [Epoch:16/20, Batch:69/320]: loss1:0.0077 loss2:0.1252 loss3:0.3896 | AUC:0.7179 Anomaly AUC:0.9230
[2024-03-09 16:08:59,591][main.py][line:124][INFO] [Epoch:16/20, Batch:79/320]: loss1:0.0139 loss2:0.0461 loss3:0.3838 | AUC:0.7286 Anomaly AUC:0.9225
[2024-03-09 16:09:14,184][main.py][line:124][INFO] [Epoch:16/20, Batch:89/320]: loss1:0.0084 loss2:0.0462 loss3:0.3817 | AUC:0.7298 Anomaly AUC:0.9257
[2024-03-09 16:09:28,733][main.py][line:124][INFO] [Epoch:16/20, Batch:99/320]: loss1:0.0088 loss2:0.0689 loss3:0.3859 | AUC:0.7425 Anomaly AUC:0.9290
[2024-03-09 16:09:43,109][main.py][line:124][INFO] [Epoch:16/20, Batch:109/320]: loss1:0.0066 loss2:0.0447 loss3:0.3861 | AUC:0.7253 Anomaly AUC:0.9278
[2024-03-09 16:09:57,749][main.py][line:124][INFO] [Epoch:16/20, Batch:119/320]: loss1:0.0098 loss2:0.0639 loss3:0.3845 | AUC:0.7250 Anomaly AUC:0.9278
[2024-03-09 16:10:12,216][main.py][line:124][INFO] [Epoch:16/20, Batch:129/320]: loss1:0.0079 loss2:0.0408 loss3:0.3754 | AUC:0.7185 Anomaly AUC:0.9283
[2024-03-09 16:10:26,655][main.py][line:124][INFO] [Epoch:16/20, Batch:139/320]: loss1:0.0065 loss2:0.0519 loss3:0.3777 | AUC:0.7113 Anomaly AUC:0.9269
[2024-03-09 16:10:41,290][main.py][line:124][INFO] [Epoch:16/20, Batch:149/320]: loss1:0.0148 loss2:0.0424 loss3:0.3802 | AUC:0.7077 Anomaly AUC:0.9263
[2024-03-09 16:10:55,762][main.py][line:124][INFO] [Epoch:16/20, Batch:159/320]: loss1:0.0109 loss2:0.0372 loss3:0.3826 | AUC:0.7297 Anomaly AUC:0.9280
[2024-03-09 16:11:10,190][main.py][line:124][INFO] [Epoch:16/20, Batch:169/320]: loss1:0.0110 loss2:0.0334 loss3:0.3835 | AUC:0.7065 Anomaly AUC:0.9249
[2024-03-09 16:11:24,816][main.py][line:124][INFO] [Epoch:16/20, Batch:179/320]: loss1:0.0103 loss2:0.0477 loss3:0.3800 | AUC:0.7157 Anomaly AUC:0.9260
[2024-03-09 16:11:39,393][main.py][line:124][INFO] [Epoch:16/20, Batch:189/320]: loss1:0.0079 loss2:0.0585 loss3:0.3789 | AUC:0.7014 Anomaly AUC:0.9249
[2024-03-09 16:11:54,031][main.py][line:124][INFO] [Epoch:16/20, Batch:199/320]: loss1:0.0114 loss2:0.0616 loss3:0.3805 | AUC:0.7110 Anomaly AUC:0.9233
[2024-03-09 16:12:08,586][main.py][line:124][INFO] [Epoch:16/20, Batch:209/320]: loss1:0.0105 loss2:0.0696 loss3:0.3851 | AUC:0.7229 Anomaly AUC:0.9273
[2024-03-09 16:12:23,083][main.py][line:124][INFO] [Epoch:16/20, Batch:219/320]: loss1:0.0078 loss2:0.0313 loss3:0.3824 | AUC:0.7028 Anomaly AUC:0.9247
[2024-03-09 16:12:37,418][main.py][line:124][INFO] [Epoch:16/20, Batch:229/320]: loss1:0.0234 loss2:0.0623 loss3:0.3800 | AUC:0.7417 Anomaly AUC:0.9308
[2024-03-09 16:12:51,929][main.py][line:124][INFO] [Epoch:16/20, Batch:239/320]: loss1:0.0067 loss2:0.0598 loss3:0.3819 | AUC:0.7300 Anomaly AUC:0.9293
[2024-03-09 16:13:06,227][main.py][line:124][INFO] [Epoch:16/20, Batch:249/320]: loss1:0.0379 loss2:0.0393 loss3:0.3750 | AUC:0.7387 Anomaly AUC:0.9293
[2024-03-09 16:13:20,487][main.py][line:124][INFO] [Epoch:16/20, Batch:259/320]: loss1:0.1334 loss2:0.0774 loss3:0.3827 | AUC:0.7126 Anomaly AUC:0.9224
[2024-03-09 16:13:35,002][main.py][line:124][INFO] [Epoch:16/20, Batch:269/320]: loss1:0.0085 loss2:0.0614 loss3:0.3849 | AUC:0.7301 Anomaly AUC:0.9278
[2024-03-09 16:13:49,448][main.py][line:124][INFO] [Epoch:16/20, Batch:279/320]: loss1:0.0065 loss2:0.0785 loss3:0.3882 | AUC:0.7208 Anomaly AUC:0.9257
[2024-03-09 16:14:03,873][main.py][line:124][INFO] [Epoch:16/20, Batch:289/320]: loss1:0.0666 loss2:0.1339 loss3:0.3910 | AUC:0.7466 Anomaly AUC:0.9209
[2024-03-09 16:14:17,839][main.py][line:153][INFO] [Epoch:16/20]: lr:0.00010 | loss1:0.0144 loss2:0.0696 loss3:0.3788 | AUC:0.7509 Anomaly AUC:0.9239
[2024-03-09 16:14:32,652][main.py][line:124][INFO] [Epoch:17/20, Batch:9/320]: loss1:0.0192 loss2:0.1172 loss3:0.3797 | AUC:0.7428 Anomaly AUC:0.9261
[2024-03-09 16:14:47,232][main.py][line:124][INFO] [Epoch:17/20, Batch:19/320]: loss1:0.0100 loss2:0.0371 loss3:0.3905 | AUC:0.7367 Anomaly AUC:0.9282
[2024-03-09 16:15:01,696][main.py][line:124][INFO] [Epoch:17/20, Batch:29/320]: loss1:0.0125 loss2:0.0615 loss3:0.3817 | AUC:0.6943 Anomaly AUC:0.9254
[2024-03-09 16:15:16,147][main.py][line:124][INFO] [Epoch:17/20, Batch:39/320]: loss1:0.0084 loss2:0.0582 loss3:0.3851 | AUC:0.7092 Anomaly AUC:0.9264
[2024-03-09 16:15:30,566][main.py][line:124][INFO] [Epoch:17/20, Batch:49/320]: loss1:0.0076 loss2:0.0736 loss3:0.3851 | AUC:0.7118 Anomaly AUC:0.9267
[2024-03-09 16:15:45,208][main.py][line:124][INFO] [Epoch:17/20, Batch:59/320]: loss1:0.0282 loss2:0.0612 loss3:0.3847 | AUC:0.7255 Anomaly AUC:0.9260
[2024-03-09 16:15:59,667][main.py][line:124][INFO] [Epoch:17/20, Batch:69/320]: loss1:0.0111 loss2:0.0488 loss3:0.3882 | AUC:0.7235 Anomaly AUC:0.9247
[2024-03-09 16:16:14,143][main.py][line:124][INFO] [Epoch:17/20, Batch:79/320]: loss1:0.0081 loss2:0.0396 loss3:0.3829 | AUC:0.6999 Anomaly AUC:0.9202
[2024-03-09 16:16:28,681][main.py][line:124][INFO] [Epoch:17/20, Batch:89/320]: loss1:0.0173 loss2:0.0720 loss3:0.3855 | AUC:0.7278 Anomaly AUC:0.9258
[2024-03-09 16:16:43,334][main.py][line:124][INFO] [Epoch:17/20, Batch:99/320]: loss1:0.0132 loss2:0.0576 loss3:0.3779 | AUC:0.7068 Anomaly AUC:0.9255
[2024-03-09 16:16:57,782][main.py][line:124][INFO] [Epoch:17/20, Batch:109/320]: loss1:0.0087 loss2:0.0599 loss3:0.3785 | AUC:0.7156 Anomaly AUC:0.9268
[2024-03-09 16:17:12,381][main.py][line:124][INFO] [Epoch:17/20, Batch:119/320]: loss1:0.0115 loss2:0.0470 loss3:0.3844 | AUC:0.7050 Anomaly AUC:0.9262
[2024-03-09 16:17:27,105][main.py][line:124][INFO] [Epoch:17/20, Batch:129/320]: loss1:0.0132 loss2:0.0771 loss3:0.3807 | AUC:0.7105 Anomaly AUC:0.9262
[2024-03-09 16:17:41,691][main.py][line:124][INFO] [Epoch:17/20, Batch:139/320]: loss1:0.0346 loss2:0.0529 loss3:0.3765 | AUC:0.7341 Anomaly AUC:0.9332
[2024-03-09 16:17:56,202][main.py][line:124][INFO] [Epoch:17/20, Batch:149/320]: loss1:0.0055 loss2:0.1133 loss3:0.3910 | AUC:0.7109 Anomaly AUC:0.9299
[2024-03-09 16:18:10,736][main.py][line:124][INFO] [Epoch:17/20, Batch:159/320]: loss1:0.0098 loss2:0.0407 loss3:0.3854 | AUC:0.6956 Anomaly AUC:0.9279
[2024-03-09 16:18:25,232][main.py][line:124][INFO] [Epoch:17/20, Batch:169/320]: loss1:0.0081 loss2:0.0458 loss3:0.3799 | AUC:0.6792 Anomaly AUC:0.9258
[2024-03-09 16:18:39,653][main.py][line:124][INFO] [Epoch:17/20, Batch:179/320]: loss1:0.0075 loss2:0.0477 loss3:0.3901 | AUC:0.7006 Anomaly AUC:0.9285
[2024-03-09 16:18:54,094][main.py][line:124][INFO] [Epoch:17/20, Batch:189/320]: loss1:0.0076 loss2:0.0339 loss3:0.3748 | AUC:0.7187 Anomaly AUC:0.9301
[2024-03-09 16:19:08,662][main.py][line:124][INFO] [Epoch:17/20, Batch:199/320]: loss1:0.0103 loss2:0.0577 loss3:0.3751 | AUC:0.7229 Anomaly AUC:0.9307
[2024-03-09 16:19:23,018][main.py][line:124][INFO] [Epoch:17/20, Batch:209/320]: loss1:0.0077 loss2:0.0573 loss3:0.3782 | AUC:0.7115 Anomaly AUC:0.9301
[2024-03-09 16:19:37,649][main.py][line:124][INFO] [Epoch:17/20, Batch:219/320]: loss1:0.0136 loss2:0.0489 loss3:0.3797 | AUC:0.7206 Anomaly AUC:0.9308
[2024-03-09 16:19:52,347][main.py][line:124][INFO] [Epoch:17/20, Batch:229/320]: loss1:0.0162 loss2:0.0313 loss3:0.3808 | AUC:0.7154 Anomaly AUC:0.9300
[2024-03-09 16:20:06,598][main.py][line:124][INFO] [Epoch:17/20, Batch:239/320]: loss1:0.0092 loss2:0.0466 loss3:0.3821 | AUC:0.7084 Anomaly AUC:0.9285
[2024-03-09 16:20:21,048][main.py][line:124][INFO] [Epoch:17/20, Batch:249/320]: loss1:0.0101 loss2:0.0762 loss3:0.3742 | AUC:0.7094 Anomaly AUC:0.9281
[2024-03-09 16:20:35,550][main.py][line:124][INFO] [Epoch:17/20, Batch:259/320]: loss1:0.0180 loss2:0.0493 loss3:0.3857 | AUC:0.7023 Anomaly AUC:0.9262
[2024-03-09 16:20:49,926][main.py][line:124][INFO] [Epoch:17/20, Batch:269/320]: loss1:0.0094 loss2:0.0491 loss3:0.3863 | AUC:0.6977 Anomaly AUC:0.9239
[2024-03-09 16:21:04,444][main.py][line:124][INFO] [Epoch:17/20, Batch:279/320]: loss1:0.0101 loss2:0.0429 loss3:0.3798 | AUC:0.6914 Anomaly AUC:0.9219
[2024-03-09 16:21:18,797][main.py][line:124][INFO] [Epoch:17/20, Batch:289/320]: loss1:0.0060 loss2:0.0384 loss3:0.3813 | AUC:0.6999 Anomaly AUC:0.9254
[2024-03-09 16:21:32,858][main.py][line:153][INFO] [Epoch:17/20]: lr:0.00010 | loss1:0.0085 loss2:0.0473 loss3:0.3803 | AUC:0.7142 Anomaly AUC:0.9278
[2024-03-09 16:21:47,926][main.py][line:124][INFO] [Epoch:18/20, Batch:9/320]: loss1:0.0104 loss2:0.0500 loss3:0.3777 | AUC:0.7121 Anomaly AUC:0.9280
[2024-03-09 16:22:02,500][main.py][line:124][INFO] [Epoch:18/20, Batch:19/320]: loss1:0.0076 loss2:0.0243 loss3:0.3824 | AUC:0.7186 Anomaly AUC:0.9289
[2024-03-09 16:22:17,043][main.py][line:124][INFO] [Epoch:18/20, Batch:29/320]: loss1:0.0108 loss2:0.0246 loss3:0.3837 | AUC:0.7118 Anomaly AUC:0.9278
[2024-03-09 16:22:31,502][main.py][line:124][INFO] [Epoch:18/20, Batch:39/320]: loss1:0.0068 loss2:0.0277 loss3:0.3788 | AUC:0.7197 Anomaly AUC:0.9286
[2024-03-09 16:22:45,870][main.py][line:124][INFO] [Epoch:18/20, Batch:49/320]: loss1:0.0089 loss2:0.0504 loss3:0.3771 | AUC:0.7241 Anomaly AUC:0.9296
[2024-03-09 16:23:00,495][main.py][line:124][INFO] [Epoch:18/20, Batch:59/320]: loss1:0.0103 loss2:0.0583 loss3:0.3816 | AUC:0.7218 Anomaly AUC:0.9279
[2024-03-09 16:23:15,016][main.py][line:124][INFO] [Epoch:18/20, Batch:69/320]: loss1:0.0097 loss2:0.0366 loss3:0.3776 | AUC:0.7424 Anomaly AUC:0.9313
[2024-03-09 16:23:29,601][main.py][line:124][INFO] [Epoch:18/20, Batch:79/320]: loss1:0.0074 loss2:0.0749 loss3:0.3757 | AUC:0.7261 Anomaly AUC:0.9287
[2024-03-09 16:23:44,113][main.py][line:124][INFO] [Epoch:18/20, Batch:89/320]: loss1:0.0106 loss2:0.0495 loss3:0.3919 | AUC:0.7149 Anomaly AUC:0.9271
[2024-03-09 16:23:58,712][main.py][line:124][INFO] [Epoch:18/20, Batch:99/320]: loss1:0.0076 loss2:0.0262 loss3:0.3852 | AUC:0.7127 Anomaly AUC:0.9267
[2024-03-09 16:24:13,354][main.py][line:124][INFO] [Epoch:18/20, Batch:109/320]: loss1:0.0074 loss2:0.0510 loss3:0.3792 | AUC:0.7189 Anomaly AUC:0.9287
[2024-03-09 16:24:28,036][main.py][line:124][INFO] [Epoch:18/20, Batch:119/320]: loss1:0.0064 loss2:0.0582 loss3:0.3801 | AUC:0.7149 Anomaly AUC:0.9276
[2024-03-09 16:24:42,417][main.py][line:124][INFO] [Epoch:18/20, Batch:129/320]: loss1:0.0127 loss2:0.0285 loss3:0.3782 | AUC:0.7064 Anomaly AUC:0.9251
[2024-03-09 16:24:56,956][main.py][line:124][INFO] [Epoch:18/20, Batch:139/320]: loss1:0.0071 loss2:0.0495 loss3:0.3812 | AUC:0.6943 Anomaly AUC:0.9244
[2024-03-09 16:25:11,401][main.py][line:124][INFO] [Epoch:18/20, Batch:149/320]: loss1:0.0093 loss2:0.0559 loss3:0.3889 | AUC:0.7129 Anomaly AUC:0.9251
[2024-03-09 16:25:25,971][main.py][line:124][INFO] [Epoch:18/20, Batch:159/320]: loss1:0.0114 loss2:0.0498 loss3:0.3822 | AUC:0.7158 Anomaly AUC:0.9270
[2024-03-09 16:25:40,361][main.py][line:124][INFO] [Epoch:18/20, Batch:169/320]: loss1:0.0105 loss2:0.0434 loss3:0.3795 | AUC:0.7451 Anomaly AUC:0.9332
[2024-03-09 16:25:54,771][main.py][line:124][INFO] [Epoch:18/20, Batch:179/320]: loss1:0.0098 loss2:0.0501 loss3:0.3793 | AUC:0.7499 Anomaly AUC:0.9361
[2024-03-09 16:26:09,299][main.py][line:124][INFO] [Epoch:18/20, Batch:189/320]: loss1:0.0472 loss2:0.0956 loss3:0.3803 | AUC:0.7371 Anomaly AUC:0.9341
[2024-03-09 16:26:23,659][main.py][line:124][INFO] [Epoch:18/20, Batch:199/320]: loss1:0.0105 loss2:0.0712 loss3:0.3876 | AUC:0.7204 Anomaly AUC:0.9310
[2024-03-09 16:26:38,028][main.py][line:124][INFO] [Epoch:18/20, Batch:209/320]: loss1:0.0318 loss2:0.0667 loss3:0.3832 | AUC:0.7265 Anomaly AUC:0.9310
[2024-03-09 16:26:52,431][main.py][line:124][INFO] [Epoch:18/20, Batch:219/320]: loss1:0.0083 loss2:0.0454 loss3:0.3822 | AUC:0.6818 Anomaly AUC:0.9215
[2024-03-09 16:27:06,829][main.py][line:124][INFO] [Epoch:18/20, Batch:229/320]: loss1:0.0093 loss2:0.0347 loss3:0.3757 | AUC:0.6617 Anomaly AUC:0.9184
[2024-03-09 16:27:21,317][main.py][line:124][INFO] [Epoch:18/20, Batch:239/320]: loss1:0.0222 loss2:0.0524 loss3:0.3747 | AUC:0.7466 Anomaly AUC:0.9339
[2024-03-09 16:27:35,913][main.py][line:124][INFO] [Epoch:18/20, Batch:249/320]: loss1:0.0371 loss2:0.0379 loss3:0.3796 | AUC:0.7107 Anomaly AUC:0.9304
[2024-03-09 16:27:50,522][main.py][line:124][INFO] [Epoch:18/20, Batch:259/320]: loss1:0.0541 loss2:0.0567 loss3:0.3909 | AUC:0.6592 Anomaly AUC:0.9201
[2024-03-09 16:28:05,138][main.py][line:124][INFO] [Epoch:18/20, Batch:269/320]: loss1:0.0145 loss2:0.0476 loss3:0.3834 | AUC:0.7080 Anomaly AUC:0.9260
[2024-03-09 16:28:19,726][main.py][line:124][INFO] [Epoch:18/20, Batch:279/320]: loss1:0.0119 loss2:0.0511 loss3:0.3894 | AUC:0.6957 Anomaly AUC:0.9295
[2024-03-09 16:28:34,277][main.py][line:124][INFO] [Epoch:18/20, Batch:289/320]: loss1:0.0235 loss2:0.0266 loss3:0.3804 | AUC:0.6889 Anomaly AUC:0.9282
[2024-03-09 16:28:48,306][main.py][line:153][INFO] [Epoch:18/20]: lr:0.00010 | loss1:0.0788 loss2:0.1071 loss3:0.3849 | AUC:0.6880 Anomaly AUC:0.9295
[2024-03-09 16:29:03,217][main.py][line:124][INFO] [Epoch:19/20, Batch:9/320]: loss1:0.1885 loss2:0.1948 loss3:0.3756 | AUC:0.6879 Anomaly AUC:0.9242
[2024-03-09 16:29:17,732][main.py][line:124][INFO] [Epoch:19/20, Batch:19/320]: loss1:0.0074 loss2:0.1073 loss3:0.3946 | AUC:0.6814 Anomaly AUC:0.9187
[2024-03-09 16:29:32,407][main.py][line:124][INFO] [Epoch:19/20, Batch:29/320]: loss1:0.0374 loss2:0.0986 loss3:0.3835 | AUC:0.6919 Anomaly AUC:0.9214
[2024-03-09 16:29:47,074][main.py][line:124][INFO] [Epoch:19/20, Batch:39/320]: loss1:0.0224 loss2:0.0384 loss3:0.3889 | AUC:0.7130 Anomaly AUC:0.9290
[2024-03-09 16:30:01,607][main.py][line:124][INFO] [Epoch:19/20, Batch:49/320]: loss1:0.0457 loss2:0.0823 loss3:0.3963 | AUC:0.6932 Anomaly AUC:0.9223
[2024-03-09 16:30:16,317][main.py][line:124][INFO] [Epoch:19/20, Batch:59/320]: loss1:0.0672 loss2:0.1158 loss3:0.3881 | AUC:0.6697 Anomaly AUC:0.9206
[2024-03-09 16:30:30,831][main.py][line:124][INFO] [Epoch:19/20, Batch:69/320]: loss1:0.0440 loss2:0.0820 loss3:0.3908 | AUC:0.6983 Anomaly AUC:0.9277
[2024-03-09 16:30:45,554][main.py][line:124][INFO] [Epoch:19/20, Batch:79/320]: loss1:0.0276 loss2:0.0855 loss3:0.3903 | AUC:0.6472 Anomaly AUC:0.9210
[2024-03-09 16:31:00,142][main.py][line:124][INFO] [Epoch:19/20, Batch:89/320]: loss1:0.0410 loss2:0.0331 loss3:0.3793 | AUC:0.7740 Anomaly AUC:0.9415
[2024-03-09 16:31:14,740][main.py][line:124][INFO] [Epoch:19/20, Batch:99/320]: loss1:0.0283 loss2:0.0867 loss3:0.3904 | AUC:0.7234 Anomaly AUC:0.9308
[2024-03-09 16:31:29,414][main.py][line:124][INFO] [Epoch:19/20, Batch:109/320]: loss1:0.0350 loss2:0.0702 loss3:0.3947 | AUC:0.6446 Anomaly AUC:0.9195
[2024-03-09 16:31:43,930][main.py][line:124][INFO] [Epoch:19/20, Batch:119/320]: loss1:0.0458 loss2:0.0418 loss3:0.3949 | AUC:0.6406 Anomaly AUC:0.9161
[2024-03-09 16:31:58,219][main.py][line:124][INFO] [Epoch:19/20, Batch:129/320]: loss1:0.0540 loss2:0.1349 loss3:0.3944 | AUC:0.6860 Anomaly AUC:0.9217
[2024-03-09 16:32:12,668][main.py][line:124][INFO] [Epoch:19/20, Batch:139/320]: loss1:0.0755 loss2:0.0550 loss3:0.3845 | AUC:0.7017 Anomaly AUC:0.9225
[2024-03-09 16:32:27,126][main.py][line:124][INFO] [Epoch:19/20, Batch:149/320]: loss1:0.0262 loss2:0.1189 loss3:0.4033 | AUC:0.7257 Anomaly AUC:0.9233
[2024-03-09 16:32:41,720][main.py][line:124][INFO] [Epoch:19/20, Batch:159/320]: loss1:0.0162 loss2:0.1076 loss3:0.3863 | AUC:0.7031 Anomaly AUC:0.9216
[2024-03-09 16:32:56,046][main.py][line:124][INFO] [Epoch:19/20, Batch:169/320]: loss1:0.0355 loss2:0.0890 loss3:0.3983 | AUC:0.6779 Anomaly AUC:0.9188
[2024-03-09 16:33:10,568][main.py][line:124][INFO] [Epoch:19/20, Batch:179/320]: loss1:0.0217 loss2:0.0826 loss3:0.3821 | AUC:0.6888 Anomaly AUC:0.9186
[2024-03-09 16:33:25,014][main.py][line:124][INFO] [Epoch:19/20, Batch:189/320]: loss1:0.0086 loss2:0.0547 loss3:0.3871 | AUC:0.7666 Anomaly AUC:0.9330
[2024-03-09 16:33:39,382][main.py][line:124][INFO] [Epoch:19/20, Batch:199/320]: loss1:0.0154 loss2:0.0779 loss3:0.3839 | AUC:0.7698 Anomaly AUC:0.9339
[2024-03-09 16:33:53,835][main.py][line:124][INFO] [Epoch:19/20, Batch:209/320]: loss1:0.0094 loss2:0.0297 loss3:0.3870 | AUC:0.7062 Anomaly AUC:0.9257
[2024-03-09 16:34:08,354][main.py][line:124][INFO] [Epoch:19/20, Batch:219/320]: loss1:0.0133 loss2:0.0443 loss3:0.3859 | AUC:0.7063 Anomaly AUC:0.9259
[2024-03-09 16:34:23,077][main.py][line:124][INFO] [Epoch:19/20, Batch:229/320]: loss1:0.0214 loss2:0.0813 loss3:0.3959 | AUC:0.7117 Anomaly AUC:0.9263
[2024-03-09 16:34:37,508][main.py][line:124][INFO] [Epoch:19/20, Batch:239/320]: loss1:0.0162 loss2:0.0542 loss3:0.3818 | AUC:0.7322 Anomaly AUC:0.9295
[2024-03-09 16:34:52,072][main.py][line:124][INFO] [Epoch:19/20, Batch:249/320]: loss1:0.0312 loss2:0.0485 loss3:0.3861 | AUC:0.7299 Anomaly AUC:0.9269
[2024-03-09 16:35:06,484][main.py][line:124][INFO] [Epoch:19/20, Batch:259/320]: loss1:0.0303 loss2:0.0279 loss3:0.3834 | AUC:0.7470 Anomaly AUC:0.9282
[2024-03-09 16:35:20,836][main.py][line:124][INFO] [Epoch:19/20, Batch:269/320]: loss1:0.0081 loss2:0.0379 loss3:0.3904 | AUC:0.7456 Anomaly AUC:0.9310
[2024-03-09 16:35:35,334][main.py][line:124][INFO] [Epoch:19/20, Batch:279/320]: loss1:0.0300 loss2:0.0948 loss3:0.3859 | AUC:0.7310 Anomaly AUC:0.9284
[2024-03-09 16:35:49,811][main.py][line:124][INFO] [Epoch:19/20, Batch:289/320]: loss1:0.0278 loss2:0.0603 loss3:0.3847 | AUC:0.6976 Anomaly AUC:0.9250
[2024-03-09 16:36:03,877][main.py][line:153][INFO] [Epoch:19/20]: lr:0.00010 | loss1:0.0154 loss2:0.0329 loss3:0.3964 | AUC:0.7019 Anomaly AUC:0.9214
[2024-03-09 16:36:18,668][main.py][line:124][INFO] [Epoch:20/20, Batch:9/320]: loss1:0.0363 loss2:0.0534 loss3:0.3819 | AUC:0.6676 Anomaly AUC:0.9221
[2024-03-09 16:36:33,036][main.py][line:124][INFO] [Epoch:20/20, Batch:19/320]: loss1:0.0083 loss2:0.0372 loss3:0.3856 | AUC:0.6900 Anomaly AUC:0.9274
[2024-03-09 16:36:47,558][main.py][line:124][INFO] [Epoch:20/20, Batch:29/320]: loss1:0.0104 loss2:0.0685 loss3:0.3907 | AUC:0.6805 Anomaly AUC:0.9221
[2024-03-09 16:37:01,987][main.py][line:124][INFO] [Epoch:20/20, Batch:39/320]: loss1:0.0210 loss2:0.0414 loss3:0.3858 | AUC:0.6905 Anomaly AUC:0.9210
[2024-03-09 16:37:16,534][main.py][line:124][INFO] [Epoch:20/20, Batch:49/320]: loss1:0.0101 loss2:0.0288 loss3:0.3831 | AUC:0.7075 Anomaly AUC:0.9250
[2024-03-09 16:37:31,044][main.py][line:124][INFO] [Epoch:20/20, Batch:59/320]: loss1:0.0092 loss2:0.0555 loss3:0.3814 | AUC:0.7849 Anomaly AUC:0.9392
[2024-03-09 16:37:45,630][main.py][line:124][INFO] [Epoch:20/20, Batch:69/320]: loss1:0.0402 loss2:0.0444 loss3:0.3914 | AUC:0.6909 Anomaly AUC:0.9203
[2024-03-09 16:38:00,150][main.py][line:124][INFO] [Epoch:20/20, Batch:79/320]: loss1:0.0174 loss2:0.0588 loss3:0.3889 | AUC:0.7681 Anomaly AUC:0.9307
[2024-03-09 16:38:14,819][main.py][line:124][INFO] [Epoch:20/20, Batch:89/320]: loss1:0.0085 loss2:0.0697 loss3:0.3896 | AUC:0.7677 Anomaly AUC:0.9321
[2024-03-09 16:38:29,358][main.py][line:124][INFO] [Epoch:20/20, Batch:99/320]: loss1:0.0104 loss2:0.0648 loss3:0.3809 | AUC:0.7949 Anomaly AUC:0.9377
[2024-03-09 16:38:43,940][main.py][line:124][INFO] [Epoch:20/20, Batch:109/320]: loss1:0.0134 loss2:0.0516 loss3:0.3852 | AUC:0.7866 Anomaly AUC:0.9369
[2024-03-09 16:38:58,588][main.py][line:124][INFO] [Epoch:20/20, Batch:119/320]: loss1:0.0097 loss2:0.0421 loss3:0.3822 | AUC:0.7787 Anomaly AUC:0.9363
[2024-03-09 16:39:13,076][main.py][line:124][INFO] [Epoch:20/20, Batch:129/320]: loss1:0.0100 loss2:0.0376 loss3:0.3826 | AUC:0.7630 Anomaly AUC:0.9336
[2024-03-09 16:39:27,574][main.py][line:124][INFO] [Epoch:20/20, Batch:139/320]: loss1:0.0064 loss2:0.0337 loss3:0.3845 | AUC:0.7569 Anomaly AUC:0.9330
[2024-03-09 16:39:42,227][main.py][line:124][INFO] [Epoch:20/20, Batch:149/320]: loss1:0.0158 loss2:0.0712 loss3:0.3835 | AUC:0.7505 Anomaly AUC:0.9314
[2024-03-09 16:39:56,768][main.py][line:124][INFO] [Epoch:20/20, Batch:159/320]: loss1:0.0099 loss2:0.0404 loss3:0.3819 | AUC:0.7363 Anomaly AUC:0.9308
[2024-03-09 16:40:11,477][main.py][line:124][INFO] [Epoch:20/20, Batch:169/320]: loss1:0.0101 loss2:0.0518 loss3:0.3807 | AUC:0.7411 Anomaly AUC:0.9326
[2024-03-09 16:40:26,036][main.py][line:124][INFO] [Epoch:20/20, Batch:179/320]: loss1:0.0070 loss2:0.0399 loss3:0.3883 | AUC:0.7205 Anomaly AUC:0.9302
[2024-03-09 16:40:40,475][main.py][line:124][INFO] [Epoch:20/20, Batch:189/320]: loss1:0.0204 loss2:0.0408 loss3:0.3826 | AUC:0.7393 Anomaly AUC:0.9319
[2024-03-09 16:40:55,079][main.py][line:124][INFO] [Epoch:20/20, Batch:199/320]: loss1:0.0101 loss2:0.0362 loss3:0.3828 | AUC:0.7357 Anomaly AUC:0.9316
[2024-03-09 16:41:09,671][main.py][line:124][INFO] [Epoch:20/20, Batch:209/320]: loss1:0.0093 loss2:0.0466 loss3:0.3851 | AUC:0.7364 Anomaly AUC:0.9314
[2024-03-09 16:41:24,147][main.py][line:124][INFO] [Epoch:20/20, Batch:219/320]: loss1:0.0096 loss2:0.0491 loss3:0.3871 | AUC:0.7339 Anomaly AUC:0.9316
[2024-03-09 16:41:38,579][main.py][line:124][INFO] [Epoch:20/20, Batch:229/320]: loss1:0.0171 loss2:0.0523 loss3:0.3856 | AUC:0.7211 Anomaly AUC:0.9299
[2024-03-09 16:41:53,084][main.py][line:124][INFO] [Epoch:20/20, Batch:239/320]: loss1:0.0108 loss2:0.0390 loss3:0.3823 | AUC:0.7344 Anomaly AUC:0.9314
[2024-03-09 16:42:07,578][main.py][line:124][INFO] [Epoch:20/20, Batch:249/320]: loss1:0.0112 loss2:0.0404 loss3:0.3807 | AUC:0.7363 Anomaly AUC:0.9318
[2024-03-09 16:42:22,279][main.py][line:124][INFO] [Epoch:20/20, Batch:259/320]: loss1:0.0093 loss2:0.0616 loss3:0.3854 | AUC:0.7248 Anomaly AUC:0.9294
[2024-03-09 16:42:36,903][main.py][line:124][INFO] [Epoch:20/20, Batch:269/320]: loss1:0.0123 loss2:0.0322 loss3:0.3848 | AUC:0.7346 Anomaly AUC:0.9307
[2024-03-09 16:42:51,512][main.py][line:124][INFO] [Epoch:20/20, Batch:279/320]: loss1:0.0097 loss2:0.0285 loss3:0.3943 | AUC:0.7257 Anomaly AUC:0.9305
[2024-03-09 16:43:06,049][main.py][line:124][INFO] [Epoch:20/20, Batch:289/320]: loss1:0.0098 loss2:0.0560 loss3:0.3944 | AUC:0.7271 Anomaly AUC:0.9309
[2024-03-09 16:43:20,146][main.py][line:153][INFO] [Epoch:20/20]: lr:0.00010 | loss1:0.0103 loss2:0.0455 loss3:0.3871 | AUC:0.7489 Anomaly AUC:0.9329
[2024-03-09 16:43:20,170][main.py][line:171][INFO] Training completes in 139m 16s | best AUCAP:0.8519 Anomaly AUC:0.9483

[2024-03-09 17:33:42,678][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 17:33:42,953][main.py][line:259][INFO] total params:8.3467M
[2024-03-09 17:33:42,953][main.py][line:268][INFO] Test Mode
[2024-03-09 17:33:42,953][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/xd__current.pkl.
[2024-03-09 17:33:57,710][infer.py][line:94][INFO] offline AUC:0.9487 Anomaly-AUC:0.8403 AP:0.8538 FAR:0.0001 | Complete in 0m 15s

[2024-03-09 17:37:27,740][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 17:37:31,073][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 17:37:31,073][main.py][line:262][INFO] Training Mode
[2024-03-09 17:37:31,074][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 17:37:31,074][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 17:37:49,548][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-09 17:38:28,079][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 17:38:31,167][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 17:38:31,168][main.py][line:262][INFO] Training Mode
[2024-03-09 17:38:31,168][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 17:38:31,168][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 17:39:43,711][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 17:39:46,888][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 17:39:46,888][main.py][line:262][INFO] Training Mode
[2024-03-09 17:39:46,888][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 17:39:46,889][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 17:40:04,395][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-09 17:41:14,269][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2491 loss2:1.0619 loss3:0.4416 | AUC:0.8698 Anomaly AUC:0.6818
[2024-03-09 17:43:12,660][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.1690 loss2:1.0750 loss3:0.4390 | AUC:0.8676 Anomaly AUC:0.6852
[2024-03-09 17:45:51,324][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1971 loss2:1.1707 loss3:0.4288 | AUC:0.8725 Anomaly AUC:0.7009
[2024-03-09 17:49:18,156][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.2239 loss2:1.1279 loss3:0.4413 | AUC:0.8649 Anomaly AUC:0.6820
[2024-03-09 17:53:21,791][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1763 loss2:1.1504 loss3:0.4365 | AUC:0.8698 Anomaly AUC:0.6917
[2024-03-09 17:57:58,804][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.2279 loss2:1.0240 loss3:0.4516 | AUC:0.8648 Anomaly AUC:0.6953
[2024-03-09 18:02:23,516][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.1730 loss2:1.1085 loss3:0.4492 | AUC:0.8604 Anomaly AUC:0.6741
[2024-03-09 18:06:43,658][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.2310 loss2:1.2024 loss3:0.4448 | AUC:0.8690 Anomaly AUC:0.6852
[2024-03-09 18:10:19,008][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0900 loss2:1.1192 loss3:0.4338 | AUC:0.8672 Anomaly AUC:0.6764
[2024-03-09 18:10:51,154][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.2087 loss2:1.0709 loss3:0.4254 | AUC:0.8673 Anomaly AUC:0.6875
[2024-03-09 18:11:38,955][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.2059 loss2:1.0225 loss3:0.4124 | AUC:0.8713 Anomaly AUC:0.6994
[2024-03-09 18:12:36,024][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1941 loss2:1.0121 loss3:0.4285 | AUC:0.8760 Anomaly AUC:0.7032
[2024-03-09 18:13:30,704][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1468 loss2:0.9806 loss3:0.4278 | AUC:0.8768 Anomaly AUC:0.6997
[2024-03-09 18:14:14,791][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.1579 loss2:1.0799 loss3:0.4260 | AUC:0.8756 Anomaly AUC:0.7070
[2024-03-09 18:15:04,229][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.1355 loss2:1.0053 loss3:0.4286 | AUC:0.8702 Anomaly AUC:0.7005
[2024-03-09 18:16:56,265][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.1488 loss2:1.0632 loss3:0.4144 | AUC:0.8746 Anomaly AUC:0.6972
[2024-03-09 18:18:48,206][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.1170 loss2:1.0105 loss3:0.4349 | AUC:0.8753 Anomaly AUC:0.6911
[2024-03-09 18:19:39,606][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.1478 loss2:0.9902 loss3:0.4125 | AUC:0.8767 Anomaly AUC:0.6973
[2024-03-09 18:21:10,711][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.1287 loss2:0.9511 loss3:0.4162 | AUC:0.8732 Anomaly AUC:0.6922
[2024-03-09 18:22:11,230][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.2354 loss2:0.9714 loss3:0.4045 | AUC:0.8732 Anomaly AUC:0.6970
[2024-03-09 18:23:21,241][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1403 loss2:1.0679 loss3:0.4189 | AUC:0.8768 Anomaly AUC:0.6957
[2024-03-09 18:24:31,977][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0856 loss2:0.9229 loss3:0.4146 | AUC:0.8762 Anomaly AUC:0.6954
[2024-03-09 18:25:53,129][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.1724 loss2:1.0253 loss3:0.4201 | AUC:0.8765 Anomaly AUC:0.6924
[2024-03-09 18:27:51,455][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.1095 loss2:1.0803 loss3:0.4099 | AUC:0.8754 Anomaly AUC:0.6837
[2024-03-09 18:28:47,501][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0490 loss2:0.9963 loss3:0.4207 | AUC:0.8772 Anomaly AUC:0.7019
[2024-03-09 18:30:36,085][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.1096 loss2:0.9775 loss3:0.4119 | AUC:0.8704 Anomaly AUC:0.6741
[2024-03-09 18:31:26,096][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.1096 loss2:0.9775 loss3:0.4119 | AUC:0.8704 Anomaly AUC:0.6741
[2024-03-09 18:34:12,851][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0671 loss2:0.9368 loss3:0.3989 | AUC:0.8650 Anomaly AUC:0.6808
[2024-03-09 18:35:45,309][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.1036 loss2:0.9615 loss3:0.4077 | AUC:0.8585 Anomaly AUC:0.6849
[2024-03-09 18:37:40,198][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.2141 loss2:1.0503 loss3:0.4083 | AUC:0.8592 Anomaly AUC:0.6761
[2024-03-09 18:38:41,472][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.1167 loss2:0.9171 loss3:0.4050 | AUC:0.8685 Anomaly AUC:0.6730
[2024-03-09 18:39:32,891][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.1131 loss2:1.0370 loss3:0.3867 | AUC:0.8659 Anomaly AUC:0.6798
[2024-03-09 18:41:04,814][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0778 loss2:0.9973 loss3:0.3963 | AUC:0.8684 Anomaly AUC:0.6793
[2024-03-09 18:43:05,193][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.1196 loss2:0.9405 loss3:0.4078 | AUC:0.8654 Anomaly AUC:0.6863
[2024-03-09 18:44:56,801][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0939 loss2:1.0388 loss3:0.3961 | AUC:0.8695 Anomaly AUC:0.6790
[2024-03-09 18:46:37,458][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0813 loss2:0.9364 loss3:0.3963 | AUC:0.8642 Anomaly AUC:0.6752
[2024-03-09 18:48:56,324][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.1334 loss2:0.9735 loss3:0.3855 | AUC:0.8642 Anomaly AUC:0.6692
[2024-03-09 18:51:04,870][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.2461 loss2:0.9162 loss3:0.3969 | AUC:0.8670 Anomaly AUC:0.6741
[2024-03-09 18:53:54,272][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.1461 loss2:0.9692 loss3:0.3844 | AUC:0.8681 Anomaly AUC:0.6806
[2024-03-09 18:57:29,107][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0590 loss2:0.9581 loss3:0.3983 | AUC:0.8699 Anomaly AUC:0.6785
[2024-03-09 19:00:25,006][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.1086 loss2:0.9171 loss3:0.3939 | AUC:0.8704 Anomaly AUC:0.6784
[2024-03-09 19:04:53,863][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.1888 loss2:0.9927 loss3:0.3846 | AUC:0.8689 Anomaly AUC:0.6893
[2024-03-09 19:08:22,038][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0831 loss2:0.9863 loss3:0.3961 | AUC:0.8692 Anomaly AUC:0.6853
[2024-03-09 19:12:17,370][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.1371 loss2:0.9195 loss3:0.3875 | AUC:0.8737 Anomaly AUC:0.6856
[2024-03-09 19:16:01,035][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0778 loss2:0.8899 loss3:0.3794 | AUC:0.8729 Anomaly AUC:0.6916
[2024-03-09 19:20:30,562][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0727 loss2:0.8396 loss3:0.3894 | AUC:0.8629 Anomaly AUC:0.6718
[2024-03-09 19:24:58,797][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1084 loss2:0.9173 loss3:0.3798 | AUC:0.8615 Anomaly AUC:0.6727
[2024-03-09 19:29:37,648][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0498 loss2:0.8344 loss3:0.3790 | AUC:0.8624 Anomaly AUC:0.6767
[2024-03-09 19:34:53,753][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.1827 loss2:0.9056 loss3:0.3846 | AUC:0.8668 Anomaly AUC:0.6709
[2024-03-09 19:39:29,513][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0658 loss2:0.8545 loss3:0.3772 | AUC:0.8709 Anomaly AUC:0.6814
[2024-03-09 19:45:20,733][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0576 loss2:0.9281 loss3:0.3867 | AUC:0.8610 Anomaly AUC:0.6667
[2024-03-09 19:50:59,438][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0806 loss2:0.8680 loss3:0.3831 | AUC:0.8608 Anomaly AUC:0.6696
[2024-03-09 19:53:11,880][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0806 loss2:0.8680 loss3:0.3831 | AUC:0.8608 Anomaly AUC:0.6696
[2024-03-09 19:55:11,103][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0456 loss2:0.9534 loss3:0.3758 | AUC:0.8667 Anomaly AUC:0.6777
[2024-03-09 19:57:14,382][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0384 loss2:0.9295 loss3:0.3928 | AUC:0.8641 Anomaly AUC:0.6744
[2024-03-09 19:59:35,151][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0520 loss2:0.8433 loss3:0.3778 | AUC:0.8650 Anomaly AUC:0.6753
[2024-03-09 20:03:33,823][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0482 loss2:0.6802 loss3:0.3841 | AUC:0.8711 Anomaly AUC:0.6808
[2024-03-09 20:05:23,092][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0659 loss2:0.8637 loss3:0.3820 | AUC:0.8705 Anomaly AUC:0.6752
[2024-03-09 20:09:59,852][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0899 loss2:0.9119 loss3:0.3889 | AUC:0.8706 Anomaly AUC:0.6822
[2024-03-09 20:14:34,407][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0610 loss2:0.7768 loss3:0.3758 | AUC:0.8640 Anomaly AUC:0.6734
[2024-03-09 20:26:45,974][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 20:26:49,118][main.py][line:259][INFO] total params:4.6207M
[2024-03-09 20:26:49,118][main.py][line:262][INFO] Training Mode
[2024-03-09 20:26:49,118][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(600, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(600, 300, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=300, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=300, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=300, out_features=300, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=300, out_features=300, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=300, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (fc2): FC_layer(
      (fc): Sequential(
        (0): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 20:26:49,118][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 20:27:29,989][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 20:27:32,934][main.py][line:259][INFO] total params:4.6207M
[2024-03-09 20:27:32,934][main.py][line:262][INFO] Training Mode
[2024-03-09 20:27:32,935][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(600, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(600, 300, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=300, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=300, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=300, out_features=300, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=300, out_features=300, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=300, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (fc2): FC_layer(
      (fc): Sequential(
        (0): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 20:27:32,935][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 20:27:48,415][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 20:27:51,367][main.py][line:259][INFO] total params:4.6198M
[2024-03-09 20:27:51,367][main.py][line:262][INFO] Training Mode
[2024-03-09 20:27:51,368][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(600, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((600,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(600, 300, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=300, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=300, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=300, out_features=300, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=300, out_features=300, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=300, out_features=300, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=300, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (fc2): FC_layer(
      (fc): Sequential(
        (0): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 20:27:51,368][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 20:28:08,328][main.py][line:76][INFO] Random initialize AUCAUC:0.5520 Anomaly AUC:0.56599
[2024-03-09 20:31:45,493][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 20:31:48,629][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 20:31:48,630][main.py][line:262][INFO] Training Mode
[2024-03-09 20:31:48,630][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.01, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 20:31:48,630][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 20:32:06,275][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-09 20:33:11,638][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2980 loss2:1.0809 loss3:0.4354 | AUC:0.8590 Anomaly AUC:0.6621
[2024-03-09 20:33:32,219][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.2156 loss2:1.1128 loss3:0.4391 | AUC:0.8637 Anomaly AUC:0.6736
[2024-03-09 20:33:51,594][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.2247 loss2:1.1867 loss3:0.4418 | AUC:0.8624 Anomaly AUC:0.6772
[2024-03-09 20:34:10,614][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.2560 loss2:1.1515 loss3:0.4385 | AUC:0.8614 Anomaly AUC:0.6746
[2024-03-09 20:34:29,773][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1936 loss2:1.1699 loss3:0.4497 | AUC:0.8635 Anomaly AUC:0.6785
[2024-03-09 20:34:48,879][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.2164 loss2:1.0473 loss3:0.4425 | AUC:0.8607 Anomaly AUC:0.6808
[2024-03-09 20:35:08,441][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.1884 loss2:1.1219 loss3:0.4418 | AUC:0.8499 Anomaly AUC:0.6631
[2024-03-09 20:35:27,485][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.2663 loss2:1.2089 loss3:0.4385 | AUC:0.8581 Anomaly AUC:0.6771
[2024-03-09 20:35:46,438][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.1126 loss2:1.1179 loss3:0.4309 | AUC:0.8543 Anomaly AUC:0.6674
[2024-03-09 20:36:05,390][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.2172 loss2:1.0668 loss3:0.4243 | AUC:0.8587 Anomaly AUC:0.6810
[2024-03-09 20:36:24,307][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.2118 loss2:1.0253 loss3:0.4108 | AUC:0.8617 Anomaly AUC:0.6900
[2024-03-09 20:36:43,210][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.2107 loss2:1.0094 loss3:0.4232 | AUC:0.8619 Anomaly AUC:0.6865
[2024-03-09 20:37:02,239][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1953 loss2:0.9974 loss3:0.4342 | AUC:0.8669 Anomaly AUC:0.6898
[2024-03-09 20:37:21,273][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.2884 loss2:1.0880 loss3:0.4230 | AUC:0.8701 Anomaly AUC:0.6991
[2024-03-09 20:37:40,394][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.2046 loss2:1.0450 loss3:0.4447 | AUC:0.8637 Anomaly AUC:0.6890
[2024-03-09 20:37:59,406][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.1701 loss2:1.0940 loss3:0.4180 | AUC:0.8658 Anomaly AUC:0.6847
[2024-03-09 20:38:18,494][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.2206 loss2:1.0157 loss3:0.4272 | AUC:0.8724 Anomaly AUC:0.6863
[2024-03-09 20:38:37,540][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.1558 loss2:1.0065 loss3:0.4151 | AUC:0.8744 Anomaly AUC:0.6978
[2024-03-09 20:38:56,597][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0941 loss2:0.9884 loss3:0.4284 | AUC:0.8722 Anomaly AUC:0.6997
[2024-03-09 20:39:15,715][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.2248 loss2:0.9795 loss3:0.4130 | AUC:0.8706 Anomaly AUC:0.7000
[2024-03-09 20:39:34,918][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1514 loss2:1.0505 loss3:0.4102 | AUC:0.8700 Anomaly AUC:0.6869
[2024-03-09 20:39:53,896][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.1048 loss2:0.9388 loss3:0.4249 | AUC:0.8703 Anomaly AUC:0.6970
[2024-03-09 20:40:12,881][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.1643 loss2:1.0331 loss3:0.4154 | AUC:0.8717 Anomaly AUC:0.7018
[2024-03-09 20:40:31,920][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0925 loss2:1.0601 loss3:0.4166 | AUC:0.8725 Anomaly AUC:0.6884
[2024-03-09 20:40:50,862][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0599 loss2:0.9933 loss3:0.4158 | AUC:0.8716 Anomaly AUC:0.6985
[2024-03-09 20:41:09,938][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.1086 loss2:0.9771 loss3:0.4112 | AUC:0.8650 Anomaly AUC:0.6808
[2024-03-09 20:41:27,031][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.1086 loss2:0.9771 loss3:0.4112 | AUC:0.8651 Anomaly AUC:0.6809
[2024-03-09 20:41:46,511][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0833 loss2:0.9489 loss3:0.4011 | AUC:0.8663 Anomaly AUC:0.6923
[2024-03-09 20:42:05,354][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.0886 loss2:0.9540 loss3:0.3994 | AUC:0.8629 Anomaly AUC:0.6923
[2024-03-09 20:42:24,288][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.2756 loss2:1.0899 loss3:0.4326 | AUC:0.8500 Anomaly AUC:0.6738
[2024-03-09 20:42:43,302][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.1969 loss2:0.9174 loss3:0.3973 | AUC:0.8674 Anomaly AUC:0.6740
[2024-03-09 20:43:02,325][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.1675 loss2:1.0562 loss3:0.3890 | AUC:0.8689 Anomaly AUC:0.6853
[2024-03-09 20:43:21,335][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.1069 loss2:1.0290 loss3:0.3997 | AUC:0.8634 Anomaly AUC:0.6868
[2024-03-09 20:43:40,588][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.1270 loss2:0.9393 loss3:0.3988 | AUC:0.8644 Anomaly AUC:0.6931
[2024-03-09 20:43:59,725][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.1246 loss2:1.0387 loss3:0.3992 | AUC:0.8648 Anomaly AUC:0.6812
[2024-03-09 20:44:18,786][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0836 loss2:0.9477 loss3:0.3967 | AUC:0.8639 Anomaly AUC:0.6844
[2024-03-09 20:44:37,826][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0615 loss2:0.9803 loss3:0.3945 | AUC:0.8588 Anomaly AUC:0.6844
[2024-03-09 20:44:56,903][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0784 loss2:0.8970 loss3:0.3945 | AUC:0.8602 Anomaly AUC:0.6794
[2024-03-09 20:45:15,906][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0766 loss2:0.9692 loss3:0.4025 | AUC:0.8604 Anomaly AUC:0.6829
[2024-03-09 20:45:35,037][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0635 loss2:0.9599 loss3:0.3990 | AUC:0.8687 Anomaly AUC:0.6878
[2024-03-09 20:45:54,073][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0928 loss2:0.9224 loss3:0.3912 | AUC:0.8724 Anomaly AUC:0.6838
[2024-03-09 20:46:13,081][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.2053 loss2:0.9580 loss3:0.3835 | AUC:0.8692 Anomaly AUC:0.6863
[2024-03-09 20:46:32,134][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0660 loss2:0.9761 loss3:0.3968 | AUC:0.8691 Anomaly AUC:0.6858
[2024-03-09 20:46:51,113][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0988 loss2:0.9199 loss3:0.3817 | AUC:0.8737 Anomaly AUC:0.6868
[2024-03-09 20:47:10,168][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0889 loss2:0.8884 loss3:0.3884 | AUC:0.8639 Anomaly AUC:0.6805
[2024-03-09 20:47:29,368][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0653 loss2:0.8536 loss3:0.3892 | AUC:0.8565 Anomaly AUC:0.6743
[2024-03-09 20:47:48,523][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1639 loss2:0.9678 loss3:0.3816 | AUC:0.8582 Anomaly AUC:0.6819
[2024-03-09 20:48:07,468][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0969 loss2:0.8855 loss3:0.3847 | AUC:0.8624 Anomaly AUC:0.6879
[2024-03-09 20:48:26,409][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.1083 loss2:0.9124 loss3:0.3940 | AUC:0.8610 Anomaly AUC:0.6787
[2024-03-09 20:48:45,533][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0685 loss2:0.8693 loss3:0.3854 | AUC:0.8674 Anomaly AUC:0.6918
[2024-03-09 20:49:04,727][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.1453 loss2:0.9790 loss3:0.3819 | AUC:0.8619 Anomaly AUC:0.6879
[2024-03-09 20:49:23,590][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0791 loss2:0.9091 loss3:0.3843 | AUC:0.8624 Anomaly AUC:0.6768
[2024-03-09 20:49:40,685][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0791 loss2:0.9091 loss3:0.3843 | AUC:0.8623 Anomaly AUC:0.6767
[2024-03-09 20:50:00,144][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0406 loss2:0.9493 loss3:0.3817 | AUC:0.8645 Anomaly AUC:0.6848
[2024-03-09 20:50:19,306][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0639 loss2:0.9464 loss3:0.3857 | AUC:0.8676 Anomaly AUC:0.6817
[2024-03-09 20:50:38,560][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0531 loss2:0.8553 loss3:0.3811 | AUC:0.8690 Anomaly AUC:0.6801
[2024-03-09 20:50:57,667][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0717 loss2:0.6929 loss3:0.3852 | AUC:0.8694 Anomaly AUC:0.6843
[2024-03-09 20:51:16,883][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0577 loss2:0.8401 loss3:0.3794 | AUC:0.8641 Anomaly AUC:0.6712
[2024-03-09 20:51:36,089][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0599 loss2:0.9066 loss3:0.3883 | AUC:0.8623 Anomaly AUC:0.6744
[2024-03-09 20:51:55,261][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0562 loss2:0.7682 loss3:0.3786 | AUC:0.8683 Anomaly AUC:0.6746
[2024-03-09 20:52:14,317][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0659 loss2:0.8988 loss3:0.3785 | AUC:0.8658 Anomaly AUC:0.6768
[2024-03-09 20:52:33,502][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0439 loss2:0.7699 loss3:0.3770 | AUC:0.8677 Anomaly AUC:0.6755
[2024-03-09 20:52:52,481][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0557 loss2:0.8379 loss3:0.3732 | AUC:0.8697 Anomaly AUC:0.6796
[2024-03-09 20:53:11,438][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.2386 loss2:0.8615 loss3:0.3799 | AUC:0.8653 Anomaly AUC:0.6755
[2024-03-09 20:53:30,420][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0878 loss2:0.8466 loss3:0.3706 | AUC:0.8516 Anomaly AUC:0.6508
[2024-03-09 20:53:49,661][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.2541 loss2:0.8180 loss3:0.3631 | AUC:0.8667 Anomaly AUC:0.6692
[2024-03-09 20:54:08,672][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0538 loss2:0.8795 loss3:0.3749 | AUC:0.8627 Anomaly AUC:0.6798
[2024-03-09 20:54:27,781][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0913 loss2:0.8286 loss3:0.3704 | AUC:0.8666 Anomaly AUC:0.6804
[2024-03-09 20:54:46,724][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0819 loss2:0.8085 loss3:0.3755 | AUC:0.8596 Anomaly AUC:0.6643
[2024-03-09 20:55:05,663][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.1904 loss2:0.9041 loss3:0.3728 | AUC:0.8622 Anomaly AUC:0.6726
[2024-03-09 20:55:24,467][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0666 loss2:0.8123 loss3:0.3801 | AUC:0.8624 Anomaly AUC:0.6708
[2024-03-09 20:55:43,509][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0713 loss2:0.8951 loss3:0.3638 | AUC:0.8642 Anomaly AUC:0.6788
[2024-03-09 20:56:02,461][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0869 loss2:0.8460 loss3:0.3635 | AUC:0.8654 Anomaly AUC:0.6825
[2024-03-09 20:56:21,683][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.1216 loss2:0.8919 loss3:0.3725 | AUC:0.8680 Anomaly AUC:0.6813
[2024-03-09 20:56:40,695][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0536 loss2:0.8283 loss3:0.3749 | AUC:0.8612 Anomaly AUC:0.6700
[2024-03-09 20:56:59,752][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0584 loss2:0.8036 loss3:0.3714 | AUC:0.8611 Anomaly AUC:0.6656
[2024-03-09 20:57:18,690][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.1593 loss2:0.8786 loss3:0.3667 | AUC:0.8623 Anomaly AUC:0.6685
[2024-03-09 20:57:37,580][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0557 loss2:0.8287 loss3:0.3617 | AUC:0.8605 Anomaly AUC:0.6643
[2024-03-09 20:57:54,537][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0557 loss2:0.8287 loss3:0.3617 | AUC:0.8605 Anomaly AUC:0.6642
[2024-03-09 20:58:52,616][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-09 20:58:55,790][main.py][line:259][INFO] total params:8.3485M
[2024-03-09 20:58:55,790][main.py][line:262][INFO] Training Mode
[2024-03-09 20:58:55,791][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.01, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-09 20:58:55,791][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-09 20:59:13,428][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-09 21:00:19,433][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2425 loss2:0.8800 loss3:0.5005 | AUC:0.8602 Anomaly AUC:0.6665
[2024-03-09 21:00:39,133][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.1845 loss2:0.8987 loss3:0.4952 | AUC:0.8596 Anomaly AUC:0.6773
[2024-03-09 21:00:58,275][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1818 loss2:0.9337 loss3:0.4900 | AUC:0.8637 Anomaly AUC:0.6835
[2024-03-09 21:01:17,358][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1557 loss2:0.9091 loss3:0.4744 | AUC:0.8626 Anomaly AUC:0.6746
[2024-03-09 21:01:36,614][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.0967 loss2:0.8938 loss3:0.4836 | AUC:0.8616 Anomaly AUC:0.6737
[2024-03-09 21:01:55,639][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.3143 loss2:0.9226 loss3:0.5065 | AUC:0.8637 Anomaly AUC:0.6780
[2024-03-09 21:02:14,589][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0824 loss2:0.8656 loss3:0.4890 | AUC:0.8618 Anomaly AUC:0.6707
[2024-03-09 21:02:33,741][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1827 loss2:0.9885 loss3:0.4780 | AUC:0.8626 Anomaly AUC:0.6739
[2024-03-09 21:02:52,743][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0803 loss2:0.8920 loss3:0.4626 | AUC:0.8659 Anomaly AUC:0.6746
[2024-03-09 21:03:11,834][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1525 loss2:0.8997 loss3:0.4575 | AUC:0.8584 Anomaly AUC:0.6662
[2024-03-09 21:03:30,955][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.1666 loss2:0.8075 loss3:0.4540 | AUC:0.8685 Anomaly AUC:0.6826
[2024-03-09 21:03:50,106][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1478 loss2:0.7964 loss3:0.4700 | AUC:0.8716 Anomaly AUC:0.6876
[2024-03-09 21:04:09,023][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1053 loss2:0.8002 loss3:0.4693 | AUC:0.8694 Anomaly AUC:0.6845
[2024-03-09 21:04:28,080][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.4028 loss2:0.8929 loss3:0.4363 | AUC:0.8716 Anomaly AUC:0.6859
[2024-03-09 21:04:47,054][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.2921 loss2:0.9074 loss3:0.4708 | AUC:0.8715 Anomaly AUC:0.6856
[2024-03-09 21:05:06,040][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.1190 loss2:0.8959 loss3:0.4517 | AUC:0.8611 Anomaly AUC:0.6670
[2024-03-09 21:05:25,195][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.1865 loss2:0.9095 loss3:0.4355 | AUC:0.8641 Anomaly AUC:0.6705
[2024-03-09 21:05:44,169][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.1506 loss2:0.8585 loss3:0.4486 | AUC:0.8683 Anomaly AUC:0.6794
[2024-03-09 21:06:03,241][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0641 loss2:0.7544 loss3:0.4500 | AUC:0.8699 Anomaly AUC:0.6855
[2024-03-09 21:06:22,311][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.1419 loss2:0.8104 loss3:0.4453 | AUC:0.8766 Anomaly AUC:0.7033
[2024-03-09 21:06:41,479][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1407 loss2:0.8454 loss3:0.4479 | AUC:0.8751 Anomaly AUC:0.7025
[2024-03-09 21:07:00,579][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0567 loss2:0.7594 loss3:0.4547 | AUC:0.8722 Anomaly AUC:0.6972
[2024-03-09 21:07:19,623][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.1936 loss2:0.9361 loss3:0.4257 | AUC:0.8757 Anomaly AUC:0.6949
[2024-03-09 21:07:39,054][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.0933 loss2:0.8700 loss3:0.4387 | AUC:0.8749 Anomaly AUC:0.6944
[2024-03-09 21:07:57,993][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0793 loss2:0.8066 loss3:0.4486 | AUC:0.8715 Anomaly AUC:0.6918
[2024-03-09 21:08:16,967][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.1192 loss2:0.7987 loss3:0.4390 | AUC:0.8736 Anomaly AUC:0.6966
[2024-03-09 21:08:34,127][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.1192 loss2:0.7987 loss3:0.4390 | AUC:0.8737 Anomaly AUC:0.6968
[2024-03-09 21:08:53,279][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0562 loss2:0.7608 loss3:0.4282 | AUC:0.8822 Anomaly AUC:0.7157
[2024-03-09 21:09:12,384][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.1257 loss2:0.7669 loss3:0.4347 | AUC:0.8813 Anomaly AUC:0.7135
[2024-03-09 21:09:31,305][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1224 loss2:0.8379 loss3:0.4565 | AUC:0.8826 Anomaly AUC:0.7162
[2024-03-09 21:09:50,528][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0487 loss2:0.7673 loss3:0.4344 | AUC:0.8788 Anomaly AUC:0.7115
[2024-03-09 21:10:09,607][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.0538 loss2:0.7908 loss3:0.4304 | AUC:0.8743 Anomaly AUC:0.6967
[2024-03-09 21:10:28,743][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0460 loss2:0.7983 loss3:0.4314 | AUC:0.8808 Anomaly AUC:0.7103
[2024-03-09 21:10:47,823][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0564 loss2:0.7622 loss3:0.4239 | AUC:0.8795 Anomaly AUC:0.7090
[2024-03-09 21:11:06,939][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0502 loss2:0.8081 loss3:0.4343 | AUC:0.8766 Anomaly AUC:0.7020
[2024-03-09 21:11:26,030][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0943 loss2:0.7963 loss3:0.4298 | AUC:0.8791 Anomaly AUC:0.7065
[2024-03-09 21:11:44,902][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0272 loss2:0.7663 loss3:0.4179 | AUC:0.8808 Anomaly AUC:0.7117
[2024-03-09 21:12:04,061][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0535 loss2:0.7429 loss3:0.4240 | AUC:0.8847 Anomaly AUC:0.7190
[2024-03-09 21:12:22,894][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0347 loss2:0.7594 loss3:0.4329 | AUC:0.8765 Anomaly AUC:0.7039
[2024-03-09 21:12:41,821][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0330 loss2:0.7710 loss3:0.4157 | AUC:0.8744 Anomaly AUC:0.6970
[2024-03-09 21:13:00,777][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0598 loss2:0.7786 loss3:0.4157 | AUC:0.8775 Anomaly AUC:0.7024
[2024-03-09 21:13:19,916][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.1107 loss2:0.7884 loss3:0.4144 | AUC:0.8782 Anomaly AUC:0.7061
[2024-03-09 21:13:39,116][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.1005 loss2:0.8270 loss3:0.4161 | AUC:0.8785 Anomaly AUC:0.7130
[2024-03-09 21:13:58,161][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0386 loss2:0.7161 loss3:0.4076 | AUC:0.8798 Anomaly AUC:0.7166
[2024-03-09 21:14:17,159][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0561 loss2:0.7171 loss3:0.4178 | AUC:0.8832 Anomaly AUC:0.7204
[2024-03-09 21:14:36,124][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0421 loss2:0.6624 loss3:0.4087 | AUC:0.8792 Anomaly AUC:0.7110
[2024-03-09 21:14:55,204][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1441 loss2:0.7991 loss3:0.4187 | AUC:0.8769 Anomaly AUC:0.7051
[2024-03-09 21:15:14,073][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0356 loss2:0.6581 loss3:0.4051 | AUC:0.8785 Anomaly AUC:0.7083
[2024-03-09 21:15:32,967][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.0830 loss2:0.7581 loss3:0.4048 | AUC:0.8744 Anomaly AUC:0.7035
[2024-03-09 21:15:51,862][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0491 loss2:0.6303 loss3:0.4013 | AUC:0.8769 Anomaly AUC:0.7078
[2024-03-09 21:16:11,015][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0845 loss2:0.7220 loss3:0.4101 | AUC:0.8811 Anomaly AUC:0.7142
[2024-03-09 21:16:30,049][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0313 loss2:0.6601 loss3:0.4032 | AUC:0.8814 Anomaly AUC:0.7112
[2024-03-09 21:16:47,129][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0313 loss2:0.6601 loss3:0.4032 | AUC:0.8814 Anomaly AUC:0.7113
[2024-03-09 21:17:06,518][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0151 loss2:0.7776 loss3:0.4069 | AUC:0.8787 Anomaly AUC:0.7118
[2024-03-09 21:17:25,838][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0211 loss2:0.6977 loss3:0.4076 | AUC:0.8770 Anomaly AUC:0.7106
[2024-03-09 21:17:44,765][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0187 loss2:0.6907 loss3:0.4072 | AUC:0.8827 Anomaly AUC:0.7166
[2024-03-09 21:18:03,737][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0516 loss2:0.5094 loss3:0.4013 | AUC:0.8831 Anomaly AUC:0.7172
[2024-03-09 21:18:22,712][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0352 loss2:0.6163 loss3:0.4002 | AUC:0.8769 Anomaly AUC:0.7016
[2024-03-09 21:18:41,711][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0293 loss2:0.7598 loss3:0.4033 | AUC:0.8782 Anomaly AUC:0.7034
[2024-03-09 21:19:00,771][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0328 loss2:0.6006 loss3:0.3994 | AUC:0.8777 Anomaly AUC:0.7058
[2024-03-09 21:19:19,792][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0247 loss2:0.6695 loss3:0.4029 | AUC:0.8777 Anomaly AUC:0.7067
[2024-03-09 21:19:38,996][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0372 loss2:0.6080 loss3:0.3991 | AUC:0.8769 Anomaly AUC:0.7092
[2024-03-09 21:19:58,099][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0207 loss2:0.6968 loss3:0.4078 | AUC:0.8813 Anomaly AUC:0.7154
[2024-03-09 21:20:17,185][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.1948 loss2:0.6490 loss3:0.4156 | AUC:0.8679 Anomaly AUC:0.6832
[2024-03-09 21:20:36,346][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0228 loss2:0.6924 loss3:0.3992 | AUC:0.8593 Anomaly AUC:0.6614
[2024-03-09 21:20:55,281][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.0864 loss2:0.6300 loss3:0.3928 | AUC:0.8769 Anomaly AUC:0.6988
[2024-03-09 21:21:14,376][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0518 loss2:0.6319 loss3:0.4120 | AUC:0.8840 Anomaly AUC:0.7165
[2024-03-09 21:21:33,407][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0605 loss2:0.6680 loss3:0.4013 | AUC:0.8836 Anomaly AUC:0.7213
[2024-03-09 21:21:52,581][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0394 loss2:0.6461 loss3:0.4082 | AUC:0.8842 Anomaly AUC:0.7152
[2024-03-09 21:22:11,681][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.0556 loss2:0.6299 loss3:0.4019 | AUC:0.8761 Anomaly AUC:0.6966
[2024-03-09 21:22:30,896][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0821 loss2:0.6317 loss3:0.4024 | AUC:0.8745 Anomaly AUC:0.6945
[2024-03-09 21:22:49,771][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0492 loss2:0.6640 loss3:0.3987 | AUC:0.8740 Anomaly AUC:0.6918
[2024-03-09 21:23:08,730][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0206 loss2:0.6549 loss3:0.4025 | AUC:0.8693 Anomaly AUC:0.6834
[2024-03-09 21:23:27,834][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.0350 loss2:0.6063 loss3:0.4005 | AUC:0.8710 Anomaly AUC:0.6884
[2024-03-09 21:23:46,858][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0714 loss2:0.6432 loss3:0.4035 | AUC:0.8738 Anomaly AUC:0.6963
[2024-03-09 21:24:05,831][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0249 loss2:0.6124 loss3:0.3988 | AUC:0.8751 Anomaly AUC:0.7015
[2024-03-09 21:24:24,874][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0197 loss2:0.6711 loss3:0.3985 | AUC:0.8747 Anomaly AUC:0.6998
[2024-03-09 21:24:43,992][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0374 loss2:0.6643 loss3:0.3919 | AUC:0.8806 Anomaly AUC:0.7125
[2024-03-09 21:25:01,224][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0374 loss2:0.6643 loss3:0.3919 | AUC:0.8806 Anomaly AUC:0.7124
[2024-03-09 21:25:20,504][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.0565 loss2:0.6123 loss3:0.4086 | AUC:0.8787 Anomaly AUC:0.7046
[2024-03-09 21:25:39,475][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0309 loss2:0.6124 loss3:0.4035 | AUC:0.8766 Anomaly AUC:0.6978
[2024-03-09 21:25:58,579][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.0124 loss2:0.6267 loss3:0.4047 | AUC:0.8709 Anomaly AUC:0.6831
[2024-03-09 21:26:17,719][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0391 loss2:0.5306 loss3:0.4026 | AUC:0.8766 Anomaly AUC:0.7005
[2024-03-09 21:26:36,661][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.0725 loss2:0.6904 loss3:0.4078 | AUC:0.8722 Anomaly AUC:0.6853
[2024-03-09 21:26:55,915][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.0687 loss2:0.6440 loss3:0.4133 | AUC:0.8671 Anomaly AUC:0.6769
[2024-03-09 21:27:15,014][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.0323 loss2:0.5947 loss3:0.4070 | AUC:0.8710 Anomaly AUC:0.6872
[2024-03-09 21:27:34,194][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0293 loss2:0.6022 loss3:0.4038 | AUC:0.8670 Anomaly AUC:0.6844
[2024-03-09 21:27:53,358][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0182 loss2:0.5772 loss3:0.3964 | AUC:0.8735 Anomaly AUC:0.6961
[2024-03-09 21:28:12,511][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.0409 loss2:0.6296 loss3:0.3920 | AUC:0.8764 Anomaly AUC:0.7010
[2024-03-09 21:28:31,348][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.0401 loss2:0.5620 loss3:0.3967 | AUC:0.8718 Anomaly AUC:0.6930
[2024-03-09 21:28:50,487][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.0156 loss2:0.5406 loss3:0.4022 | AUC:0.8709 Anomaly AUC:0.6948
[2024-03-09 21:29:09,635][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0089 loss2:0.6412 loss3:0.4045 | AUC:0.8585 Anomaly AUC:0.6702
[2024-03-09 21:29:28,829][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0242 loss2:0.5601 loss3:0.3989 | AUC:0.8594 Anomaly AUC:0.6695
[2024-03-09 21:29:47,755][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.0127 loss2:0.5318 loss3:0.4094 | AUC:0.8716 Anomaly AUC:0.6861
[2024-03-09 21:30:06,899][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.1124 loss2:0.5910 loss3:0.4151 | AUC:0.8379 Anomaly AUC:0.6283
[2024-03-09 21:30:25,936][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0421 loss2:0.5979 loss3:0.4033 | AUC:0.8714 Anomaly AUC:0.6874
[2024-03-09 21:30:44,958][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.0484 loss2:0.6440 loss3:0.3946 | AUC:0.8741 Anomaly AUC:0.6964
[2024-03-09 21:31:04,161][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0917 loss2:0.6015 loss3:0.3998 | AUC:0.8674 Anomaly AUC:0.6862
[2024-03-09 21:31:23,473][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0250 loss2:0.5412 loss3:0.3986 | AUC:0.8638 Anomaly AUC:0.6774
[2024-03-09 21:31:42,587][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0123 loss2:0.5058 loss3:0.4019 | AUC:0.8684 Anomaly AUC:0.6927
[2024-03-09 21:32:01,629][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0206 loss2:0.5722 loss3:0.4002 | AUC:0.8739 Anomaly AUC:0.7022
[2024-03-09 21:32:20,783][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0205 loss2:0.5273 loss3:0.3999 | AUC:0.8728 Anomaly AUC:0.6958
[2024-03-09 21:32:39,708][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.0203 loss2:0.4373 loss3:0.4027 | AUC:0.8664 Anomaly AUC:0.6838
[2024-03-09 21:32:58,743][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0136 loss2:0.5305 loss3:0.3999 | AUC:0.8671 Anomaly AUC:0.6824
[2024-03-09 21:33:15,886][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0136 loss2:0.5305 loss3:0.3999 | AUC:0.8671 Anomaly AUC:0.6823
[2024-03-09 21:33:35,180][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0360 loss2:0.4671 loss3:0.4024 | AUC:0.8669 Anomaly AUC:0.6820
[2024-03-09 21:33:54,237][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.0249 loss2:0.4897 loss3:0.4031 | AUC:0.8675 Anomaly AUC:0.6864
[2024-03-09 21:34:13,432][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0193 loss2:0.5328 loss3:0.4036 | AUC:0.8648 Anomaly AUC:0.6762
[2024-03-09 21:34:32,544][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0162 loss2:0.5215 loss3:0.3952 | AUC:0.8676 Anomaly AUC:0.6805
[2024-03-09 21:34:51,754][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0425 loss2:0.4879 loss3:0.4010 | AUC:0.8628 Anomaly AUC:0.6699
[2024-03-09 21:35:10,785][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0339 loss2:0.5563 loss3:0.3985 | AUC:0.8485 Anomaly AUC:0.6412
[2024-03-09 21:35:29,746][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0105 loss2:0.5301 loss3:0.4056 | AUC:0.8747 Anomaly AUC:0.7005
[2024-03-09 21:35:48,990][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0141 loss2:0.5132 loss3:0.3976 | AUC:0.8640 Anomaly AUC:0.6743
[2024-03-09 21:36:07,871][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0337 loss2:0.5727 loss3:0.3935 | AUC:0.8708 Anomaly AUC:0.6896
[2024-03-09 21:36:26,887][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0227 loss2:0.4966 loss3:0.3993 | AUC:0.8679 Anomaly AUC:0.6896
[2024-03-09 21:36:45,751][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0321 loss2:0.5534 loss3:0.3975 | AUC:0.8541 Anomaly AUC:0.6500
[2024-03-09 21:37:04,832][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0168 loss2:0.4632 loss3:0.4006 | AUC:0.8646 Anomaly AUC:0.6720
[2024-03-09 21:37:23,918][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0244 loss2:0.4086 loss3:0.4007 | AUC:0.8663 Anomaly AUC:0.6856
[2024-03-09 21:37:42,816][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0177 loss2:0.5356 loss3:0.3982 | AUC:0.8628 Anomaly AUC:0.6697
[2024-03-09 21:38:02,195][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0135 loss2:0.4370 loss3:0.3940 | AUC:0.8717 Anomaly AUC:0.6847
[2024-03-09 21:38:21,597][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0153 loss2:0.4676 loss3:0.3962 | AUC:0.8718 Anomaly AUC:0.6851
[2024-03-09 21:38:40,742][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0229 loss2:0.3640 loss3:0.3981 | AUC:0.8699 Anomaly AUC:0.6841
[2024-03-09 21:38:59,898][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0311 loss2:0.4691 loss3:0.3959 | AUC:0.8694 Anomaly AUC:0.6824
[2024-03-09 21:39:19,075][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0126 loss2:0.4589 loss3:0.3960 | AUC:0.8697 Anomaly AUC:0.6825
[2024-03-09 21:39:38,087][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.0201 loss2:0.5197 loss3:0.3940 | AUC:0.8700 Anomaly AUC:0.6857
[2024-03-09 21:39:57,065][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0122 loss2:0.4895 loss3:0.3967 | AUC:0.8669 Anomaly AUC:0.6820
[2024-03-09 21:40:16,226][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0163 loss2:0.4862 loss3:0.3983 | AUC:0.8648 Anomaly AUC:0.6750
[2024-03-09 21:40:35,515][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0193 loss2:0.3937 loss3:0.4015 | AUC:0.8696 Anomaly AUC:0.6880
[2024-03-09 21:40:54,293][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0196 loss2:0.4505 loss3:0.3935 | AUC:0.8674 Anomaly AUC:0.6776
[2024-03-09 21:41:13,325][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0163 loss2:0.3860 loss3:0.4007 | AUC:0.8669 Anomaly AUC:0.6793
[2024-03-09 21:41:30,555][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0163 loss2:0.3860 loss3:0.4007 | AUC:0.8670 Anomaly AUC:0.6794
[2024-03-09 21:41:30,579][main.py][line:171][INFO] Training completes in 42m 17s | best AUCAUC:0.8847 Anomaly AUC:0.7190

[2024-03-10 14:29:31,825][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 14:29:34,904][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 14:29:34,904][main.py][line:262][INFO] Training Mode
[2024-03-10 14:29:34,905][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 14:29:34,905][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 14:29:52,500][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-10 14:30:58,329][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2554 loss2:0.7896 loss3:0.4422 | AUC:0.8558 Anomaly AUC:0.6455
[2024-03-10 14:31:17,893][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.2132 loss2:0.8389 loss3:0.4492 | AUC:0.8560 Anomaly AUC:0.6466
[2024-03-10 14:31:36,856][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.1697 loss2:0.9030 loss3:0.4444 | AUC:0.8587 Anomaly AUC:0.6521
[2024-03-10 14:31:55,865][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1714 loss2:0.8307 loss3:0.4463 | AUC:0.8554 Anomaly AUC:0.6405
[2024-03-10 14:32:14,608][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1049 loss2:0.8179 loss3:0.4369 | AUC:0.8540 Anomaly AUC:0.6375
[2024-03-10 14:32:33,667][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.2121 loss2:0.7696 loss3:0.4460 | AUC:0.8592 Anomaly AUC:0.6494
[2024-03-10 14:32:52,542][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.1090 loss2:0.8485 loss3:0.4597 | AUC:0.8409 Anomaly AUC:0.6328
[2024-03-10 14:33:11,614][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.2910 loss2:0.9094 loss3:0.4328 | AUC:0.8592 Anomaly AUC:0.6546
[2024-03-10 14:33:30,436][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0966 loss2:0.8332 loss3:0.4460 | AUC:0.8496 Anomaly AUC:0.6426
[2024-03-10 14:33:49,169][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1605 loss2:0.8396 loss3:0.4146 | AUC:0.8524 Anomaly AUC:0.6355
[2024-03-10 14:34:07,866][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.2134 loss2:0.7743 loss3:0.4148 | AUC:0.8547 Anomaly AUC:0.6497
[2024-03-10 14:34:26,755][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1678 loss2:0.7712 loss3:0.4135 | AUC:0.8531 Anomaly AUC:0.6378
[2024-03-10 14:34:45,824][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1990 loss2:0.7693 loss3:0.4117 | AUC:0.8448 Anomaly AUC:0.6280
[2024-03-10 14:35:04,898][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.1921 loss2:0.7934 loss3:0.4129 | AUC:0.8528 Anomaly AUC:0.6380
[2024-03-10 14:35:23,988][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.1397 loss2:0.7319 loss3:0.4186 | AUC:0.8488 Anomaly AUC:0.6317
[2024-03-10 14:35:43,032][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0995 loss2:0.8359 loss3:0.4088 | AUC:0.8617 Anomaly AUC:0.6568
[2024-03-10 14:36:01,907][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.1604 loss2:0.7912 loss3:0.4292 | AUC:0.8503 Anomaly AUC:0.6250
[2024-03-10 14:36:20,881][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.3532 loss2:0.7957 loss3:0.3941 | AUC:0.8459 Anomaly AUC:0.6183
[2024-03-10 14:36:39,855][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.0792 loss2:0.7504 loss3:0.4338 | AUC:0.8492 Anomaly AUC:0.6263
[2024-03-10 14:36:59,015][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.1524 loss2:0.7576 loss3:0.4098 | AUC:0.8553 Anomaly AUC:0.6385
[2024-03-10 14:37:17,986][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1720 loss2:0.8152 loss3:0.4127 | AUC:0.8542 Anomaly AUC:0.6365
[2024-03-10 14:37:36,939][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.1121 loss2:0.7199 loss3:0.4237 | AUC:0.8596 Anomaly AUC:0.6549
[2024-03-10 14:37:55,891][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.0588 loss2:0.8264 loss3:0.4063 | AUC:0.8620 Anomaly AUC:0.6599
[2024-03-10 14:38:15,076][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.1211 loss2:0.8328 loss3:0.3976 | AUC:0.8608 Anomaly AUC:0.6549
[2024-03-10 14:38:34,065][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0860 loss2:0.7694 loss3:0.4051 | AUC:0.8567 Anomaly AUC:0.6527
[2024-03-10 14:38:52,984][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.0756 loss2:0.7291 loss3:0.4007 | AUC:0.8508 Anomaly AUC:0.6321
[2024-03-10 14:39:10,236][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.0756 loss2:0.7291 loss3:0.4007 | AUC:0.8507 Anomaly AUC:0.6319
[2024-03-10 14:39:29,551][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0573 loss2:0.7188 loss3:0.3955 | AUC:0.8542 Anomaly AUC:0.6404
[2024-03-10 14:39:48,491][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.0701 loss2:0.7288 loss3:0.3933 | AUC:0.8568 Anomaly AUC:0.6443
[2024-03-10 14:40:07,448][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1286 loss2:0.8032 loss3:0.3956 | AUC:0.8456 Anomaly AUC:0.6090
[2024-03-10 14:40:26,262][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0674 loss2:0.7442 loss3:0.4045 | AUC:0.8472 Anomaly AUC:0.6309
[2024-03-10 14:40:45,225][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.1097 loss2:0.7660 loss3:0.3918 | AUC:0.8588 Anomaly AUC:0.6532
[2024-03-10 14:41:04,091][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0662 loss2:0.7457 loss3:0.3865 | AUC:0.8493 Anomaly AUC:0.6304
[2024-03-10 14:41:22,871][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0453 loss2:0.7172 loss3:0.3817 | AUC:0.8581 Anomaly AUC:0.6469
[2024-03-10 14:41:41,830][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0473 loss2:0.7426 loss3:0.3928 | AUC:0.8532 Anomaly AUC:0.6462
[2024-03-10 14:42:00,965][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0632 loss2:0.7208 loss3:0.3993 | AUC:0.8453 Anomaly AUC:0.6355
[2024-03-10 14:42:20,047][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0521 loss2:0.7168 loss3:0.3797 | AUC:0.8523 Anomaly AUC:0.6265
[2024-03-10 14:42:38,833][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0556 loss2:0.7387 loss3:0.3877 | AUC:0.8424 Anomaly AUC:0.6012
[2024-03-10 14:42:57,723][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.1302 loss2:0.7591 loss3:0.3679 | AUC:0.8507 Anomaly AUC:0.6226
[2024-03-10 14:43:16,517][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0934 loss2:0.7776 loss3:0.3737 | AUC:0.8549 Anomaly AUC:0.6390
[2024-03-10 14:43:35,577][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.1106 loss2:0.7584 loss3:0.3831 | AUC:0.8595 Anomaly AUC:0.6570
[2024-03-10 14:43:54,662][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.1552 loss2:0.7691 loss3:0.3689 | AUC:0.8497 Anomaly AUC:0.6450
[2024-03-10 14:44:13,589][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0713 loss2:0.7856 loss3:0.3765 | AUC:0.8598 Anomaly AUC:0.6589
[2024-03-10 14:44:32,538][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0483 loss2:0.7063 loss3:0.3801 | AUC:0.8552 Anomaly AUC:0.6480
[2024-03-10 14:44:51,542][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0928 loss2:0.6879 loss3:0.3661 | AUC:0.8564 Anomaly AUC:0.6403
[2024-03-10 14:45:10,429][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0479 loss2:0.6306 loss3:0.3750 | AUC:0.8477 Anomaly AUC:0.6196
[2024-03-10 14:45:29,340][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.0971 loss2:0.7191 loss3:0.3631 | AUC:0.8503 Anomaly AUC:0.6332
[2024-03-10 14:45:48,247][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0477 loss2:0.6678 loss3:0.3659 | AUC:0.8480 Anomaly AUC:0.6179
[2024-03-10 14:46:07,284][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.0557 loss2:0.7412 loss3:0.3558 | AUC:0.8510 Anomaly AUC:0.6311
[2024-03-10 14:46:26,481][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0463 loss2:0.6420 loss3:0.3532 | AUC:0.8449 Anomaly AUC:0.6312
[2024-03-10 14:46:45,250][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0547 loss2:0.7077 loss3:0.3578 | AUC:0.8459 Anomaly AUC:0.6220
[2024-03-10 14:47:04,118][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0572 loss2:0.6385 loss3:0.3492 | AUC:0.8364 Anomaly AUC:0.6252
[2024-03-10 14:47:21,278][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0572 loss2:0.6385 loss3:0.3492 | AUC:0.8364 Anomaly AUC:0.6251
[2024-03-10 14:47:40,704][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0457 loss2:0.7629 loss3:0.3548 | AUC:0.8281 Anomaly AUC:0.6192
[2024-03-10 14:47:59,574][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0428 loss2:0.7000 loss3:0.3542 | AUC:0.8365 Anomaly AUC:0.6355
[2024-03-10 14:48:18,554][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0505 loss2:0.6769 loss3:0.3496 | AUC:0.8405 Anomaly AUC:0.6067
[2024-03-10 14:48:37,653][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0439 loss2:0.5148 loss3:0.3493 | AUC:0.8501 Anomaly AUC:0.6360
[2024-03-10 14:48:56,475][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0389 loss2:0.6646 loss3:0.3442 | AUC:0.8557 Anomaly AUC:0.6421
[2024-03-10 14:49:15,303][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.1538 loss2:0.7715 loss3:0.3465 | AUC:0.8390 Anomaly AUC:0.6180
[2024-03-10 14:49:34,342][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.1495 loss2:0.6487 loss3:0.3480 | AUC:0.8325 Anomaly AUC:0.6041
[2024-03-10 14:49:53,319][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0386 loss2:0.6708 loss3:0.3472 | AUC:0.8412 Anomaly AUC:0.6110
[2024-03-10 14:50:12,483][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.1122 loss2:0.6547 loss3:0.3401 | AUC:0.8441 Anomaly AUC:0.6156
[2024-03-10 14:50:31,544][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0350 loss2:0.6998 loss3:0.3343 | AUC:0.8458 Anomaly AUC:0.6293
[2024-03-10 14:50:50,482][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.0437 loss2:0.6103 loss3:0.3445 | AUC:0.8429 Anomaly AUC:0.6347
[2024-03-10 14:51:09,636][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.1252 loss2:0.7356 loss3:0.3429 | AUC:0.8443 Anomaly AUC:0.6271
[2024-03-10 14:51:28,467][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.0487 loss2:0.6286 loss3:0.3294 | AUC:0.8544 Anomaly AUC:0.6335
[2024-03-10 14:51:47,463][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0733 loss2:0.6387 loss3:0.3320 | AUC:0.8441 Anomaly AUC:0.6114
[2024-03-10 14:52:06,325][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0483 loss2:0.6249 loss3:0.3413 | AUC:0.8421 Anomaly AUC:0.6097
[2024-03-10 14:52:25,260][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0368 loss2:0.6477 loss3:0.3393 | AUC:0.8301 Anomaly AUC:0.6215
[2024-03-10 14:52:44,442][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.1003 loss2:0.6602 loss3:0.3380 | AUC:0.8387 Anomaly AUC:0.6197
[2024-03-10 14:53:03,426][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0527 loss2:0.6454 loss3:0.3270 | AUC:0.8406 Anomaly AUC:0.6163
[2024-03-10 14:53:22,602][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.0880 loss2:0.7242 loss3:0.3263 | AUC:0.8489 Anomaly AUC:0.6253
[2024-03-10 14:53:41,653][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.1241 loss2:0.7048 loss3:0.3312 | AUC:0.8446 Anomaly AUC:0.6104
[2024-03-10 14:54:00,627][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.0610 loss2:0.6585 loss3:0.3348 | AUC:0.8440 Anomaly AUC:0.6319
[2024-03-10 14:54:19,417][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0758 loss2:0.6578 loss3:0.3323 | AUC:0.8444 Anomaly AUC:0.6320
[2024-03-10 14:54:38,245][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0543 loss2:0.6512 loss3:0.3397 | AUC:0.8308 Anomaly AUC:0.6160
[2024-03-10 14:54:57,159][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0400 loss2:0.7147 loss3:0.3272 | AUC:0.8331 Anomaly AUC:0.6064
[2024-03-10 14:55:16,210][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0313 loss2:0.6809 loss3:0.3258 | AUC:0.8308 Anomaly AUC:0.6084
[2024-03-10 14:55:33,294][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0313 loss2:0.6809 loss3:0.3258 | AUC:0.8311 Anomaly AUC:0.6088
[2024-03-10 14:55:52,392][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.3166 loss2:0.6631 loss3:0.3308 | AUC:0.8345 Anomaly AUC:0.6042
[2024-03-10 14:56:11,353][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0516 loss2:0.6578 loss3:0.3246 | AUC:0.8450 Anomaly AUC:0.6238
[2024-03-10 14:56:30,225][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.0470 loss2:0.6601 loss3:0.3247 | AUC:0.8346 Anomaly AUC:0.5943
[2024-03-10 14:56:49,146][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0420 loss2:0.5514 loss3:0.3246 | AUC:0.8342 Anomaly AUC:0.5978
[2024-03-10 14:57:08,380][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.2086 loss2:0.7330 loss3:0.3266 | AUC:0.8372 Anomaly AUC:0.6142
[2024-03-10 14:57:27,609][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.2239 loss2:0.6566 loss3:0.3490 | AUC:0.8179 Anomaly AUC:0.5949
[2024-03-10 14:57:46,683][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.1268 loss2:0.6530 loss3:0.3204 | AUC:0.8414 Anomaly AUC:0.6131
[2024-03-10 14:58:05,705][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0917 loss2:0.6897 loss3:0.3149 | AUC:0.8396 Anomaly AUC:0.6189
[2024-03-10 14:58:24,656][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0468 loss2:0.6419 loss3:0.3224 | AUC:0.8410 Anomaly AUC:0.6289
[2024-03-10 14:58:43,600][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.1385 loss2:0.7191 loss3:0.3214 | AUC:0.8413 Anomaly AUC:0.6084
[2024-03-10 14:59:02,632][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.1648 loss2:0.6764 loss3:0.3185 | AUC:0.8532 Anomaly AUC:0.6327
[2024-03-10 14:59:21,623][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.0609 loss2:0.6479 loss3:0.3181 | AUC:0.8495 Anomaly AUC:0.6274
[2024-03-10 14:59:40,696][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0510 loss2:0.6934 loss3:0.3227 | AUC:0.8340 Anomaly AUC:0.6248
[2024-03-10 14:59:59,765][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0383 loss2:0.6424 loss3:0.3257 | AUC:0.8406 Anomaly AUC:0.6238
[2024-03-10 15:00:18,662][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.1178 loss2:0.5909 loss3:0.3262 | AUC:0.8465 Anomaly AUC:0.6181
[2024-03-10 15:00:37,528][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.0472 loss2:0.6241 loss3:0.3186 | AUC:0.8508 Anomaly AUC:0.6274
[2024-03-10 15:00:56,626][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0454 loss2:0.6608 loss3:0.3127 | AUC:0.8551 Anomaly AUC:0.6480
[2024-03-10 15:01:15,828][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.0598 loss2:0.7257 loss3:0.3265 | AUC:0.8391 Anomaly AUC:0.5948
[2024-03-10 15:01:34,925][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0423 loss2:0.6380 loss3:0.3299 | AUC:0.8345 Anomaly AUC:0.6186
[2024-03-10 15:01:53,865][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0521 loss2:0.6422 loss3:0.3180 | AUC:0.8558 Anomaly AUC:0.6495
[2024-03-10 15:02:12,715][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0452 loss2:0.5980 loss3:0.3186 | AUC:0.8490 Anomaly AUC:0.6377
[2024-03-10 15:02:31,640][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0728 loss2:0.6638 loss3:0.3272 | AUC:0.8358 Anomaly AUC:0.6228
[2024-03-10 15:02:50,673][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0808 loss2:0.6466 loss3:0.3229 | AUC:0.8483 Anomaly AUC:0.6276
[2024-03-10 15:03:09,581][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.1039 loss2:0.5472 loss3:0.3199 | AUC:0.8400 Anomaly AUC:0.6025
[2024-03-10 15:03:28,572][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0488 loss2:0.6293 loss3:0.3161 | AUC:0.8376 Anomaly AUC:0.6116
[2024-03-10 15:03:45,725][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0488 loss2:0.6293 loss3:0.3161 | AUC:0.8376 Anomaly AUC:0.6115
[2024-03-10 15:04:04,925][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0547 loss2:0.5672 loss3:0.3123 | AUC:0.8382 Anomaly AUC:0.6282
[2024-03-10 15:04:23,790][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.1409 loss2:0.6149 loss3:0.3224 | AUC:0.8420 Anomaly AUC:0.6052
[2024-03-10 15:04:42,726][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0768 loss2:0.6518 loss3:0.3247 | AUC:0.8403 Anomaly AUC:0.6286
[2024-03-10 15:05:01,660][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0415 loss2:0.6505 loss3:0.3182 | AUC:0.8446 Anomaly AUC:0.6207
[2024-03-10 15:05:20,802][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0531 loss2:0.6084 loss3:0.3158 | AUC:0.8464 Anomaly AUC:0.6256
[2024-03-10 15:05:39,766][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0398 loss2:0.6657 loss3:0.3143 | AUC:0.8468 Anomaly AUC:0.6288
[2024-03-10 15:05:58,809][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0399 loss2:0.6059 loss3:0.3167 | AUC:0.8439 Anomaly AUC:0.6274
[2024-03-10 15:06:17,673][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0447 loss2:0.6195 loss3:0.3142 | AUC:0.8442 Anomaly AUC:0.6239
[2024-03-10 15:06:36,709][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0325 loss2:0.6906 loss3:0.3151 | AUC:0.8367 Anomaly AUC:0.6003
[2024-03-10 15:06:55,766][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0486 loss2:0.5700 loss3:0.3130 | AUC:0.8352 Anomaly AUC:0.6159
[2024-03-10 15:07:14,720][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0389 loss2:0.6606 loss3:0.3139 | AUC:0.8399 Anomaly AUC:0.6025
[2024-03-10 15:07:33,668][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0459 loss2:0.5811 loss3:0.3091 | AUC:0.8334 Anomaly AUC:0.6277
[2024-03-10 15:07:52,837][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0392 loss2:0.5433 loss3:0.3166 | AUC:0.8376 Anomaly AUC:0.6276
[2024-03-10 15:08:11,780][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0454 loss2:0.6613 loss3:0.3163 | AUC:0.8401 Anomaly AUC:0.6144
[2024-03-10 15:08:30,646][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0500 loss2:0.5677 loss3:0.3099 | AUC:0.8481 Anomaly AUC:0.6320
[2024-03-10 15:08:49,800][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0441 loss2:0.6243 loss3:0.3090 | AUC:0.8474 Anomaly AUC:0.6331
[2024-03-10 15:09:08,765][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0410 loss2:0.4772 loss3:0.3141 | AUC:0.8443 Anomaly AUC:0.6197
[2024-03-10 15:09:27,805][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0395 loss2:0.6261 loss3:0.3138 | AUC:0.8456 Anomaly AUC:0.6341
[2024-03-10 15:09:47,035][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0381 loss2:0.5960 loss3:0.3103 | AUC:0.8386 Anomaly AUC:0.6055
[2024-03-10 15:10:05,945][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.1025 loss2:0.6482 loss3:0.3108 | AUC:0.8319 Anomaly AUC:0.5833
[2024-03-10 15:10:24,805][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0427 loss2:0.6120 loss3:0.3137 | AUC:0.8273 Anomaly AUC:0.6176
[2024-03-10 15:10:43,652][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0507 loss2:0.6685 loss3:0.3484 | AUC:0.8112 Anomaly AUC:0.6051
[2024-03-10 15:11:02,478][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0610 loss2:0.5828 loss3:0.3090 | AUC:0.8487 Anomaly AUC:0.6465
[2024-03-10 15:11:21,371][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0695 loss2:0.6460 loss3:0.3087 | AUC:0.8527 Anomaly AUC:0.6343
[2024-03-10 15:11:40,192][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0512 loss2:0.5525 loss3:0.3141 | AUC:0.8495 Anomaly AUC:0.6258
[2024-03-10 15:11:57,338][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0512 loss2:0.5525 loss3:0.3141 | AUC:0.8498 Anomaly AUC:0.6263
[2024-03-10 15:11:57,362][main.py][line:171][INFO] Training completes in 42m 5s | best AUCAUC:0.8620 Anomaly AUC:0.6599

[2024-03-10 15:17:19,976][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 15:17:22,969][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 15:17:22,969][main.py][line:262][INFO] Training Mode
[2024-03-10 15:17:22,970][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 15:17:22,970][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 15:17:40,649][main.py][line:76][INFO] Random initialize AUCAUC:0.5411 Anomaly AUC:0.54356
[2024-03-10 15:18:46,585][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.3119 loss2:1.2578 loss3:0.4603 | AUC:0.8314 Anomaly AUC:0.5908
[2024-03-10 15:19:06,221][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.2738 loss2:1.2804 loss3:0.4521 | AUC:0.8378 Anomaly AUC:0.6159
[2024-03-10 15:19:25,392][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.2448 loss2:1.3205 loss3:0.4440 | AUC:0.8337 Anomaly AUC:0.6001
[2024-03-10 15:19:44,359][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.2179 loss2:1.3151 loss3:0.4505 | AUC:0.8321 Anomaly AUC:0.5974
[2024-03-10 15:20:03,462][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1314 loss2:1.3432 loss3:0.4627 | AUC:0.8276 Anomaly AUC:0.5884
[2024-03-10 15:20:22,698][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.1687 loss2:1.2299 loss3:0.4535 | AUC:0.8309 Anomaly AUC:0.5942
[2024-03-10 15:20:41,757][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.1463 loss2:1.3052 loss3:0.4596 | AUC:0.8308 Anomaly AUC:0.5971
[2024-03-10 15:21:00,947][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1864 loss2:1.3712 loss3:0.4655 | AUC:0.8367 Anomaly AUC:0.6039
[2024-03-10 15:21:20,038][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.0847 loss2:1.3236 loss3:0.4533 | AUC:0.8367 Anomaly AUC:0.6035
[2024-03-10 15:21:39,356][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1824 loss2:1.2967 loss3:0.4568 | AUC:0.8365 Anomaly AUC:0.6051
[2024-03-10 15:21:58,495][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.1713 loss2:1.2053 loss3:0.4426 | AUC:0.8360 Anomaly AUC:0.6037
[2024-03-10 15:22:17,497][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1246 loss2:1.2243 loss3:0.4505 | AUC:0.8339 Anomaly AUC:0.6054
[2024-03-10 15:22:36,699][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.1702 loss2:1.2442 loss3:0.4517 | AUC:0.8350 Anomaly AUC:0.6013
[2024-03-10 15:22:55,891][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.1213 loss2:1.2009 loss3:0.4364 | AUC:0.8372 Anomaly AUC:0.6041
[2024-03-10 15:23:14,994][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.1517 loss2:1.2249 loss3:0.4312 | AUC:0.8333 Anomaly AUC:0.5977
[2024-03-10 15:23:33,922][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.0701 loss2:1.2973 loss3:0.4423 | AUC:0.8356 Anomaly AUC:0.6036
[2024-03-10 15:39:26,639][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 15:39:29,648][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 15:39:29,648][main.py][line:262][INFO] Training Mode
[2024-03-10 15:39:29,648][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 15:39:29,649][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 15:52:00,989][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 15:52:04,004][main.py][line:259][INFO] total params:3.5705M
[2024-03-10 15:52:04,004][main.py][line:262][INFO] Training Mode
[2024-03-10 15:52:04,004][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 15:52:04,004][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 15:52:26,669][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 15:52:29,679][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 15:52:29,679][main.py][line:262][INFO] Training Mode
[2024-03-10 15:52:29,680][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 15:52:29,680][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 15:53:03,327][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 15:53:06,349][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 15:53:06,349][main.py][line:262][INFO] Training Mode
[2024-03-10 15:53:06,350][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 15:53:06,350][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 15:53:24,045][main.py][line:76][INFO] Random initialize AUCAUC:0.5368 Anomaly AUC:0.54516
[2024-03-10 15:54:30,041][main.py][line:153][INFO] [Epoch:1/6]: lr:0.00010 | loss1:0.2361 loss2:0.8851 loss3:0.4929 | AUC:0.8598 Anomaly AUC:0.6609
[2024-03-10 15:54:49,978][main.py][line:124][INFO] [Epoch:2/6, Batch:9/250]: loss1:0.2104 loss2:0.9133 loss3:0.4864 | AUC:0.8630 Anomaly AUC:0.6708
[2024-03-10 15:55:09,114][main.py][line:124][INFO] [Epoch:2/6, Batch:19/250]: loss1:0.2281 loss2:0.9490 loss3:0.4785 | AUC:0.8646 Anomaly AUC:0.6766
[2024-03-10 15:55:28,210][main.py][line:124][INFO] [Epoch:2/6, Batch:29/250]: loss1:0.1789 loss2:0.9293 loss3:0.4830 | AUC:0.8610 Anomaly AUC:0.6649
[2024-03-10 15:55:47,536][main.py][line:124][INFO] [Epoch:2/6, Batch:39/250]: loss1:0.1450 loss2:0.9194 loss3:0.4678 | AUC:0.8591 Anomaly AUC:0.6620
[2024-03-10 15:56:06,712][main.py][line:124][INFO] [Epoch:2/6, Batch:49/250]: loss1:0.1998 loss2:0.8636 loss3:0.4846 | AUC:0.8593 Anomaly AUC:0.6654
[2024-03-10 15:56:25,827][main.py][line:124][INFO] [Epoch:2/6, Batch:59/250]: loss1:0.0964 loss2:0.8735 loss3:0.4836 | AUC:0.8623 Anomaly AUC:0.6717
[2024-03-10 15:56:44,854][main.py][line:124][INFO] [Epoch:2/6, Batch:69/250]: loss1:0.1852 loss2:0.9914 loss3:0.4818 | AUC:0.8688 Anomaly AUC:0.6844
[2024-03-10 15:57:03,760][main.py][line:124][INFO] [Epoch:2/6, Batch:79/250]: loss1:0.2373 loss2:0.9643 loss3:0.4746 | AUC:0.8643 Anomaly AUC:0.6692
[2024-03-10 15:57:22,986][main.py][line:124][INFO] [Epoch:2/6, Batch:89/250]: loss1:0.1798 loss2:0.9192 loss3:0.4568 | AUC:0.8631 Anomaly AUC:0.6673
[2024-03-10 15:57:42,300][main.py][line:124][INFO] [Epoch:2/6, Batch:99/250]: loss1:0.2335 loss2:0.8589 loss3:0.4535 | AUC:0.8679 Anomaly AUC:0.6762
[2024-03-10 15:58:01,503][main.py][line:124][INFO] [Epoch:2/6, Batch:109/250]: loss1:0.1813 loss2:0.8122 loss3:0.4550 | AUC:0.8675 Anomaly AUC:0.6760
[2024-03-10 15:58:20,789][main.py][line:124][INFO] [Epoch:2/6, Batch:119/250]: loss1:0.0963 loss2:0.8189 loss3:0.4541 | AUC:0.8675 Anomaly AUC:0.6780
[2024-03-10 15:58:39,921][main.py][line:124][INFO] [Epoch:2/6, Batch:129/250]: loss1:0.2689 loss2:0.8462 loss3:0.4652 | AUC:0.8641 Anomaly AUC:0.6700
[2024-03-10 15:58:58,919][main.py][line:124][INFO] [Epoch:2/6, Batch:139/250]: loss1:0.1452 loss2:0.8201 loss3:0.4451 | AUC:0.8665 Anomaly AUC:0.6767
[2024-03-10 15:59:18,152][main.py][line:124][INFO] [Epoch:2/6, Batch:149/250]: loss1:0.1129 loss2:0.9071 loss3:0.4425 | AUC:0.8651 Anomaly AUC:0.6737
[2024-03-10 15:59:37,442][main.py][line:124][INFO] [Epoch:2/6, Batch:159/250]: loss1:0.1230 loss2:0.9009 loss3:0.4593 | AUC:0.8672 Anomaly AUC:0.6737
[2024-03-10 15:59:56,569][main.py][line:124][INFO] [Epoch:2/6, Batch:169/250]: loss1:0.1154 loss2:0.8428 loss3:0.4504 | AUC:0.8725 Anomaly AUC:0.6827
[2024-03-10 16:00:15,784][main.py][line:124][INFO] [Epoch:2/6, Batch:179/250]: loss1:0.1768 loss2:0.8102 loss3:0.4392 | AUC:0.8673 Anomaly AUC:0.6761
[2024-03-10 16:00:34,986][main.py][line:124][INFO] [Epoch:2/6, Batch:189/250]: loss1:0.1959 loss2:0.8485 loss3:0.4423 | AUC:0.8724 Anomaly AUC:0.6907
[2024-03-10 16:00:54,107][main.py][line:124][INFO] [Epoch:2/6, Batch:199/250]: loss1:0.1899 loss2:0.8947 loss3:0.4372 | AUC:0.8730 Anomaly AUC:0.6939
[2024-03-10 16:01:13,096][main.py][line:124][INFO] [Epoch:2/6, Batch:209/250]: loss1:0.0603 loss2:0.7711 loss3:0.4545 | AUC:0.8715 Anomaly AUC:0.6934
[2024-03-10 16:01:32,208][main.py][line:124][INFO] [Epoch:2/6, Batch:219/250]: loss1:0.0938 loss2:0.8752 loss3:0.4412 | AUC:0.8751 Anomaly AUC:0.6921
[2024-03-10 16:01:51,491][main.py][line:124][INFO] [Epoch:2/6, Batch:229/250]: loss1:0.1168 loss2:0.8860 loss3:0.4363 | AUC:0.8727 Anomaly AUC:0.6867
[2024-03-10 16:02:10,657][main.py][line:124][INFO] [Epoch:2/6, Batch:239/250]: loss1:0.0837 loss2:0.8103 loss3:0.4418 | AUC:0.8718 Anomaly AUC:0.6910
[2024-03-10 16:02:29,663][main.py][line:124][INFO] [Epoch:2/6, Batch:249/250]: loss1:0.1256 loss2:0.8149 loss3:0.4340 | AUC:0.8685 Anomaly AUC:0.6835
[2024-03-10 16:02:46,919][main.py][line:153][INFO] [Epoch:2/6]: lr:0.00010 | loss1:0.1256 loss2:0.8149 loss3:0.4340 | AUC:0.8685 Anomaly AUC:0.6836
[2024-03-10 16:03:06,485][main.py][line:124][INFO] [Epoch:3/6, Batch:9/250]: loss1:0.0684 loss2:0.7789 loss3:0.4298 | AUC:0.8761 Anomaly AUC:0.6981
[2024-03-10 16:03:25,679][main.py][line:124][INFO] [Epoch:3/6, Batch:19/250]: loss1:0.1580 loss2:0.7945 loss3:0.4294 | AUC:0.8763 Anomaly AUC:0.6984
[2024-03-10 16:03:44,991][main.py][line:124][INFO] [Epoch:3/6, Batch:29/250]: loss1:0.1744 loss2:0.8577 loss3:0.4392 | AUC:0.8762 Anomaly AUC:0.6998
[2024-03-10 16:04:04,210][main.py][line:124][INFO] [Epoch:3/6, Batch:39/250]: loss1:0.0635 loss2:0.7766 loss3:0.4288 | AUC:0.8772 Anomaly AUC:0.7054
[2024-03-10 16:04:23,544][main.py][line:124][INFO] [Epoch:3/6, Batch:49/250]: loss1:0.0628 loss2:0.7994 loss3:0.4314 | AUC:0.8780 Anomaly AUC:0.7045
[2024-03-10 16:04:42,798][main.py][line:124][INFO] [Epoch:3/6, Batch:59/250]: loss1:0.0461 loss2:0.8096 loss3:0.4279 | AUC:0.8827 Anomaly AUC:0.7166
[2024-03-10 16:05:01,915][main.py][line:124][INFO] [Epoch:3/6, Batch:69/250]: loss1:0.0796 loss2:0.7673 loss3:0.4173 | AUC:0.8759 Anomaly AUC:0.7029
[2024-03-10 16:05:20,983][main.py][line:124][INFO] [Epoch:3/6, Batch:79/250]: loss1:0.0690 loss2:0.8269 loss3:0.4283 | AUC:0.8771 Anomaly AUC:0.7039
[2024-03-10 16:05:40,192][main.py][line:124][INFO] [Epoch:3/6, Batch:89/250]: loss1:0.0679 loss2:0.7854 loss3:0.4251 | AUC:0.8795 Anomaly AUC:0.7069
[2024-03-10 16:05:59,325][main.py][line:124][INFO] [Epoch:3/6, Batch:99/250]: loss1:0.0254 loss2:0.7771 loss3:0.4134 | AUC:0.8822 Anomaly AUC:0.7100
[2024-03-10 16:06:18,547][main.py][line:124][INFO] [Epoch:3/6, Batch:109/250]: loss1:0.0907 loss2:0.7710 loss3:0.4246 | AUC:0.8845 Anomaly AUC:0.7136
[2024-03-10 16:06:37,721][main.py][line:124][INFO] [Epoch:3/6, Batch:119/250]: loss1:0.0888 loss2:0.7721 loss3:0.4118 | AUC:0.8676 Anomaly AUC:0.6784
[2024-03-10 16:06:56,932][main.py][line:124][INFO] [Epoch:3/6, Batch:129/250]: loss1:0.0548 loss2:0.8019 loss3:0.4180 | AUC:0.8733 Anomaly AUC:0.6921
[2024-03-10 16:07:16,205][main.py][line:124][INFO] [Epoch:3/6, Batch:139/250]: loss1:0.0713 loss2:0.7966 loss3:0.4055 | AUC:0.8825 Anomaly AUC:0.7117
[2024-03-10 16:07:35,154][main.py][line:124][INFO] [Epoch:3/6, Batch:149/250]: loss1:0.1504 loss2:0.8036 loss3:0.4033 | AUC:0.8800 Anomaly AUC:0.7073
[2024-03-10 16:07:54,335][main.py][line:124][INFO] [Epoch:3/6, Batch:159/250]: loss1:0.0614 loss2:0.8075 loss3:0.4072 | AUC:0.8855 Anomaly AUC:0.7235
[2024-03-10 16:08:13,643][main.py][line:124][INFO] [Epoch:3/6, Batch:169/250]: loss1:0.0468 loss2:0.7212 loss3:0.4013 | AUC:0.8873 Anomaly AUC:0.7309
[2024-03-10 16:08:32,646][main.py][line:124][INFO] [Epoch:3/6, Batch:179/250]: loss1:0.0335 loss2:0.7039 loss3:0.4021 | AUC:0.8727 Anomaly AUC:0.6881
[2024-03-10 16:08:51,723][main.py][line:124][INFO] [Epoch:3/6, Batch:189/250]: loss1:0.0457 loss2:0.6744 loss3:0.3978 | AUC:0.8771 Anomaly AUC:0.6985
[2024-03-10 16:09:10,889][main.py][line:124][INFO] [Epoch:3/6, Batch:199/250]: loss1:0.1322 loss2:0.7857 loss3:0.3996 | AUC:0.8741 Anomaly AUC:0.6928
[2024-03-10 16:09:30,027][main.py][line:124][INFO] [Epoch:3/6, Batch:209/250]: loss1:0.0564 loss2:0.6628 loss3:0.3924 | AUC:0.8824 Anomaly AUC:0.7169
[2024-03-10 16:09:48,922][main.py][line:124][INFO] [Epoch:3/6, Batch:219/250]: loss1:0.2192 loss2:0.8388 loss3:0.3952 | AUC:0.8821 Anomaly AUC:0.7120
[2024-03-10 16:10:08,145][main.py][line:124][INFO] [Epoch:3/6, Batch:229/250]: loss1:0.0560 loss2:0.6623 loss3:0.3978 | AUC:0.8777 Anomaly AUC:0.7015
[2024-03-10 16:10:27,186][main.py][line:124][INFO] [Epoch:3/6, Batch:239/250]: loss1:0.0343 loss2:0.7068 loss3:0.4025 | AUC:0.8746 Anomaly AUC:0.6863
[2024-03-10 16:10:46,431][main.py][line:124][INFO] [Epoch:3/6, Batch:249/250]: loss1:0.0589 loss2:0.6944 loss3:0.3903 | AUC:0.8781 Anomaly AUC:0.6969
[2024-03-10 16:11:03,842][main.py][line:153][INFO] [Epoch:3/6]: lr:0.00010 | loss1:0.0589 loss2:0.6944 loss3:0.3903 | AUC:0.8780 Anomaly AUC:0.6969
[2024-03-10 16:11:23,146][main.py][line:124][INFO] [Epoch:4/6, Batch:9/250]: loss1:0.0169 loss2:0.7908 loss3:0.3943 | AUC:0.8780 Anomaly AUC:0.6989
[2024-03-10 16:11:42,341][main.py][line:124][INFO] [Epoch:4/6, Batch:19/250]: loss1:0.0203 loss2:0.7132 loss3:0.3863 | AUC:0.8787 Anomaly AUC:0.7046
[2024-03-10 16:12:01,444][main.py][line:124][INFO] [Epoch:4/6, Batch:29/250]: loss1:0.0290 loss2:0.7135 loss3:0.3884 | AUC:0.8787 Anomaly AUC:0.7046
[2024-03-10 16:12:20,432][main.py][line:124][INFO] [Epoch:4/6, Batch:39/250]: loss1:0.0803 loss2:0.5424 loss3:0.3865 | AUC:0.8845 Anomaly AUC:0.7170
[2024-03-10 16:12:39,507][main.py][line:124][INFO] [Epoch:4/6, Batch:49/250]: loss1:0.0313 loss2:0.6289 loss3:0.3832 | AUC:0.8778 Anomaly AUC:0.7011
[2024-03-10 16:12:58,556][main.py][line:124][INFO] [Epoch:4/6, Batch:59/250]: loss1:0.0223 loss2:0.7750 loss3:0.3890 | AUC:0.8869 Anomaly AUC:0.7248
[2024-03-10 16:13:17,689][main.py][line:124][INFO] [Epoch:4/6, Batch:69/250]: loss1:0.0607 loss2:0.6426 loss3:0.3825 | AUC:0.8845 Anomaly AUC:0.7222
[2024-03-10 16:13:36,932][main.py][line:124][INFO] [Epoch:4/6, Batch:79/250]: loss1:0.0292 loss2:0.6884 loss3:0.3851 | AUC:0.8912 Anomaly AUC:0.7344
[2024-03-10 16:13:56,120][main.py][line:124][INFO] [Epoch:4/6, Batch:89/250]: loss1:0.0218 loss2:0.6262 loss3:0.3811 | AUC:0.8939 Anomaly AUC:0.7403
[2024-03-10 16:14:15,251][main.py][line:124][INFO] [Epoch:4/6, Batch:99/250]: loss1:0.0178 loss2:0.6958 loss3:0.3794 | AUC:0.8794 Anomaly AUC:0.6969
[2024-03-10 16:14:34,234][main.py][line:124][INFO] [Epoch:4/6, Batch:109/250]: loss1:0.0711 loss2:0.6389 loss3:0.3862 | AUC:0.8680 Anomaly AUC:0.6731
[2024-03-10 16:14:53,326][main.py][line:124][INFO] [Epoch:4/6, Batch:119/250]: loss1:0.0227 loss2:0.7131 loss3:0.3782 | AUC:0.8914 Anomaly AUC:0.7367
[2024-03-10 16:15:12,554][main.py][line:124][INFO] [Epoch:4/6, Batch:129/250]: loss1:0.1471 loss2:0.6881 loss3:0.3854 | AUC:0.8827 Anomaly AUC:0.7224
[2024-03-10 16:15:31,776][main.py][line:124][INFO] [Epoch:4/6, Batch:139/250]: loss1:0.0324 loss2:0.6268 loss3:0.3783 | AUC:0.8838 Anomaly AUC:0.7181
[2024-03-10 16:15:51,087][main.py][line:124][INFO] [Epoch:4/6, Batch:149/250]: loss1:0.0269 loss2:0.6329 loss3:0.3726 | AUC:0.8815 Anomaly AUC:0.7056
[2024-03-10 16:16:10,399][main.py][line:124][INFO] [Epoch:4/6, Batch:159/250]: loss1:0.0488 loss2:0.6470 loss3:0.3742 | AUC:0.8834 Anomaly AUC:0.7071
[2024-03-10 16:16:29,601][main.py][line:124][INFO] [Epoch:4/6, Batch:169/250]: loss1:0.0262 loss2:0.6366 loss3:0.3772 | AUC:0.8790 Anomaly AUC:0.6987
[2024-03-10 16:16:48,969][main.py][line:124][INFO] [Epoch:4/6, Batch:179/250]: loss1:0.0731 loss2:0.6434 loss3:0.3806 | AUC:0.8759 Anomaly AUC:0.6973
[2024-03-10 16:17:08,231][main.py][line:124][INFO] [Epoch:4/6, Batch:189/250]: loss1:0.1255 loss2:0.7428 loss3:0.3829 | AUC:0.8784 Anomaly AUC:0.7051
[2024-03-10 16:17:27,211][main.py][line:124][INFO] [Epoch:4/6, Batch:199/250]: loss1:0.0757 loss2:0.7026 loss3:0.3881 | AUC:0.8747 Anomaly AUC:0.7049
[2024-03-10 16:17:46,404][main.py][line:124][INFO] [Epoch:4/6, Batch:209/250]: loss1:0.0421 loss2:0.6624 loss3:0.3784 | AUC:0.8787 Anomaly AUC:0.7013
[2024-03-10 16:18:05,372][main.py][line:124][INFO] [Epoch:4/6, Batch:219/250]: loss1:0.0193 loss2:0.6216 loss3:0.3733 | AUC:0.8865 Anomaly AUC:0.7217
[2024-03-10 16:18:24,412][main.py][line:124][INFO] [Epoch:4/6, Batch:229/250]: loss1:0.0581 loss2:0.6734 loss3:0.3754 | AUC:0.8871 Anomaly AUC:0.7255
[2024-03-10 16:18:43,443][main.py][line:124][INFO] [Epoch:4/6, Batch:239/250]: loss1:0.0314 loss2:0.7151 loss3:0.3748 | AUC:0.8822 Anomaly AUC:0.7123
[2024-03-10 16:19:02,482][main.py][line:124][INFO] [Epoch:4/6, Batch:249/250]: loss1:0.0141 loss2:0.6763 loss3:0.3726 | AUC:0.8766 Anomaly AUC:0.6991
[2024-03-10 16:19:19,706][main.py][line:153][INFO] [Epoch:4/6]: lr:0.00010 | loss1:0.0141 loss2:0.6763 loss3:0.3726 | AUC:0.8767 Anomaly AUC:0.6993
[2024-03-10 16:19:38,860][main.py][line:124][INFO] [Epoch:5/6, Batch:9/250]: loss1:0.0825 loss2:0.6273 loss3:0.3731 | AUC:0.8713 Anomaly AUC:0.6913
[2024-03-10 16:19:58,243][main.py][line:124][INFO] [Epoch:5/6, Batch:19/250]: loss1:0.0541 loss2:0.6424 loss3:0.3738 | AUC:0.8727 Anomaly AUC:0.6913
[2024-03-10 16:20:17,300][main.py][line:124][INFO] [Epoch:5/6, Batch:29/250]: loss1:0.1087 loss2:0.6857 loss3:0.3768 | AUC:0.8654 Anomaly AUC:0.6794
[2024-03-10 16:20:36,499][main.py][line:124][INFO] [Epoch:5/6, Batch:39/250]: loss1:0.0621 loss2:0.5574 loss3:0.3777 | AUC:0.8814 Anomaly AUC:0.7159
[2024-03-10 16:20:55,630][main.py][line:124][INFO] [Epoch:5/6, Batch:49/250]: loss1:0.0158 loss2:0.6888 loss3:0.3733 | AUC:0.8776 Anomaly AUC:0.7042
[2024-03-10 16:21:14,627][main.py][line:124][INFO] [Epoch:5/6, Batch:59/250]: loss1:0.0385 loss2:0.6371 loss3:0.3749 | AUC:0.8801 Anomaly AUC:0.7119
[2024-03-10 16:21:33,872][main.py][line:124][INFO] [Epoch:5/6, Batch:69/250]: loss1:0.0168 loss2:0.6128 loss3:0.3711 | AUC:0.8702 Anomaly AUC:0.6929
[2024-03-10 16:21:53,020][main.py][line:124][INFO] [Epoch:5/6, Batch:79/250]: loss1:0.0710 loss2:0.6444 loss3:0.3736 | AUC:0.8636 Anomaly AUC:0.6740
[2024-03-10 16:22:12,129][main.py][line:124][INFO] [Epoch:5/6, Batch:89/250]: loss1:0.0145 loss2:0.5928 loss3:0.3656 | AUC:0.8729 Anomaly AUC:0.6894
[2024-03-10 16:22:31,442][main.py][line:124][INFO] [Epoch:5/6, Batch:99/250]: loss1:0.0146 loss2:0.6463 loss3:0.3653 | AUC:0.8705 Anomaly AUC:0.6875
[2024-03-10 16:22:50,687][main.py][line:124][INFO] [Epoch:5/6, Batch:109/250]: loss1:0.0397 loss2:0.6039 loss3:0.3670 | AUC:0.8722 Anomaly AUC:0.6907
[2024-03-10 16:23:09,795][main.py][line:124][INFO] [Epoch:5/6, Batch:119/250]: loss1:0.0162 loss2:0.5693 loss3:0.3651 | AUC:0.8688 Anomaly AUC:0.6814
[2024-03-10 16:23:28,733][main.py][line:124][INFO] [Epoch:5/6, Batch:129/250]: loss1:0.0149 loss2:0.6618 loss3:0.3633 | AUC:0.8753 Anomaly AUC:0.6896
[2024-03-10 16:23:47,674][main.py][line:124][INFO] [Epoch:5/6, Batch:139/250]: loss1:0.0996 loss2:0.6342 loss3:0.3616 | AUC:0.8814 Anomaly AUC:0.7035
[2024-03-10 16:24:06,624][main.py][line:124][INFO] [Epoch:5/6, Batch:149/250]: loss1:0.0184 loss2:0.5338 loss3:0.3667 | AUC:0.8770 Anomaly AUC:0.7037
[2024-03-10 16:24:25,774][main.py][line:124][INFO] [Epoch:5/6, Batch:159/250]: loss1:0.0111 loss2:0.5651 loss3:0.3704 | AUC:0.8685 Anomaly AUC:0.6822
[2024-03-10 16:24:44,653][main.py][line:124][INFO] [Epoch:5/6, Batch:169/250]: loss1:0.0154 loss2:0.6156 loss3:0.3604 | AUC:0.8778 Anomaly AUC:0.6968
[2024-03-10 16:25:03,679][main.py][line:124][INFO] [Epoch:5/6, Batch:179/250]: loss1:0.0175 loss2:0.6668 loss3:0.3639 | AUC:0.8634 Anomaly AUC:0.6737
[2024-03-10 16:25:22,634][main.py][line:124][INFO] [Epoch:5/6, Batch:189/250]: loss1:0.0593 loss2:0.6074 loss3:0.3672 | AUC:0.8680 Anomaly AUC:0.6859
[2024-03-10 16:25:41,884][main.py][line:124][INFO] [Epoch:5/6, Batch:199/250]: loss1:0.0576 loss2:0.5990 loss3:0.3689 | AUC:0.8715 Anomaly AUC:0.6893
[2024-03-10 16:26:01,060][main.py][line:124][INFO] [Epoch:5/6, Batch:209/250]: loss1:0.0416 loss2:0.5848 loss3:0.3626 | AUC:0.8782 Anomaly AUC:0.6987
[2024-03-10 16:26:20,335][main.py][line:124][INFO] [Epoch:5/6, Batch:219/250]: loss1:0.0562 loss2:0.6347 loss3:0.3654 | AUC:0.8657 Anomaly AUC:0.6763
[2024-03-10 16:26:39,774][main.py][line:124][INFO] [Epoch:5/6, Batch:229/250]: loss1:0.0261 loss2:0.5840 loss3:0.3637 | AUC:0.8484 Anomaly AUC:0.6467
[2024-03-10 16:26:58,914][main.py][line:124][INFO] [Epoch:5/6, Batch:239/250]: loss1:0.0149 loss2:0.4890 loss3:0.3684 | AUC:0.8558 Anomaly AUC:0.6571
[2024-03-10 16:27:17,941][main.py][line:124][INFO] [Epoch:5/6, Batch:249/250]: loss1:0.0118 loss2:0.5863 loss3:0.3671 | AUC:0.8431 Anomaly AUC:0.6380
[2024-03-10 16:27:35,320][main.py][line:153][INFO] [Epoch:5/6]: lr:0.00010 | loss1:0.0118 loss2:0.5863 loss3:0.3671 | AUC:0.8433 Anomaly AUC:0.6384
[2024-03-10 16:27:54,837][main.py][line:124][INFO] [Epoch:6/6, Batch:9/250]: loss1:0.0330 loss2:0.5157 loss3:0.3708 | AUC:0.8583 Anomaly AUC:0.6653
[2024-03-10 16:28:13,886][main.py][line:124][INFO] [Epoch:6/6, Batch:19/250]: loss1:0.0139 loss2:0.5250 loss3:0.3655 | AUC:0.8678 Anomaly AUC:0.6896
[2024-03-10 16:28:33,053][main.py][line:124][INFO] [Epoch:6/6, Batch:29/250]: loss1:0.0129 loss2:0.5705 loss3:0.3642 | AUC:0.8664 Anomaly AUC:0.6841
[2024-03-10 16:28:52,201][main.py][line:124][INFO] [Epoch:6/6, Batch:39/250]: loss1:0.0246 loss2:0.5852 loss3:0.3630 | AUC:0.8583 Anomaly AUC:0.6567
[2024-03-10 16:29:11,408][main.py][line:124][INFO] [Epoch:6/6, Batch:49/250]: loss1:0.0437 loss2:0.5503 loss3:0.3652 | AUC:0.8631 Anomaly AUC:0.6632
[2024-03-10 16:29:30,480][main.py][line:124][INFO] [Epoch:6/6, Batch:59/250]: loss1:0.0158 loss2:0.6159 loss3:0.3627 | AUC:0.8698 Anomaly AUC:0.6839
[2024-03-10 16:29:49,532][main.py][line:124][INFO] [Epoch:6/6, Batch:69/250]: loss1:0.0258 loss2:0.5645 loss3:0.3624 | AUC:0.8691 Anomaly AUC:0.6893
[2024-03-10 16:30:08,660][main.py][line:124][INFO] [Epoch:6/6, Batch:79/250]: loss1:0.0070 loss2:0.5627 loss3:0.3640 | AUC:0.8684 Anomaly AUC:0.6876
[2024-03-10 16:30:27,870][main.py][line:124][INFO] [Epoch:6/6, Batch:89/250]: loss1:0.0252 loss2:0.6248 loss3:0.3657 | AUC:0.8561 Anomaly AUC:0.6591
[2024-03-10 16:30:46,933][main.py][line:124][INFO] [Epoch:6/6, Batch:99/250]: loss1:0.0206 loss2:0.5600 loss3:0.3671 | AUC:0.8588 Anomaly AUC:0.6650
[2024-03-10 16:31:05,789][main.py][line:124][INFO] [Epoch:6/6, Batch:109/250]: loss1:0.0314 loss2:0.6020 loss3:0.3654 | AUC:0.8531 Anomaly AUC:0.6510
[2024-03-10 16:31:25,089][main.py][line:124][INFO] [Epoch:6/6, Batch:119/250]: loss1:0.0158 loss2:0.5378 loss3:0.3671 | AUC:0.8390 Anomaly AUC:0.6242
[2024-03-10 16:31:44,152][main.py][line:124][INFO] [Epoch:6/6, Batch:129/250]: loss1:0.0303 loss2:0.4661 loss3:0.3663 | AUC:0.8668 Anomaly AUC:0.6717
[2024-03-10 16:32:03,280][main.py][line:124][INFO] [Epoch:6/6, Batch:139/250]: loss1:0.0497 loss2:0.5956 loss3:0.3668 | AUC:0.8311 Anomaly AUC:0.6168
[2024-03-10 16:32:22,489][main.py][line:124][INFO] [Epoch:6/6, Batch:149/250]: loss1:0.0216 loss2:0.5035 loss3:0.3627 | AUC:0.8675 Anomaly AUC:0.6804
[2024-03-10 16:32:41,448][main.py][line:124][INFO] [Epoch:6/6, Batch:159/250]: loss1:0.0121 loss2:0.5315 loss3:0.3617 | AUC:0.8633 Anomaly AUC:0.6660
[2024-03-10 16:33:00,756][main.py][line:124][INFO] [Epoch:6/6, Batch:169/250]: loss1:0.0137 loss2:0.4010 loss3:0.3635 | AUC:0.8390 Anomaly AUC:0.6217
[2024-03-10 16:33:19,975][main.py][line:124][INFO] [Epoch:6/6, Batch:179/250]: loss1:0.0587 loss2:0.5596 loss3:0.3622 | AUC:0.8491 Anomaly AUC:0.6399
[2024-03-10 16:33:39,143][main.py][line:124][INFO] [Epoch:6/6, Batch:189/250]: loss1:0.0205 loss2:0.5401 loss3:0.3623 | AUC:0.8616 Anomaly AUC:0.6721
[2024-03-10 16:33:58,189][main.py][line:124][INFO] [Epoch:6/6, Batch:199/250]: loss1:0.0389 loss2:0.5806 loss3:0.3648 | AUC:0.8707 Anomaly AUC:0.6880
[2024-03-10 16:34:17,184][main.py][line:124][INFO] [Epoch:6/6, Batch:209/250]: loss1:0.0157 loss2:0.5803 loss3:0.3699 | AUC:0.8487 Anomaly AUC:0.6467
[2024-03-10 16:34:36,356][main.py][line:124][INFO] [Epoch:6/6, Batch:219/250]: loss1:0.0095 loss2:0.5688 loss3:0.3648 | AUC:0.8617 Anomaly AUC:0.6692
[2024-03-10 16:34:55,381][main.py][line:124][INFO] [Epoch:6/6, Batch:229/250]: loss1:0.0113 loss2:0.4631 loss3:0.3632 | AUC:0.8733 Anomaly AUC:0.6919
[2024-03-10 16:35:14,806][main.py][line:124][INFO] [Epoch:6/6, Batch:239/250]: loss1:0.0096 loss2:0.5351 loss3:0.3602 | AUC:0.8721 Anomaly AUC:0.6886
[2024-03-10 16:35:33,786][main.py][line:124][INFO] [Epoch:6/6, Batch:249/250]: loss1:0.0090 loss2:0.4525 loss3:0.3620 | AUC:0.8734 Anomaly AUC:0.6880
[2024-03-10 16:35:50,900][main.py][line:153][INFO] [Epoch:6/6]: lr:0.00010 | loss1:0.0090 loss2:0.4525 loss3:0.3620 | AUC:0.8734 Anomaly AUC:0.6880
[2024-03-10 16:35:50,924][main.py][line:171][INFO] Training completes in 42m 27s | best AUCAUC:0.8939 Anomaly AUC:0.7403

[2024-03-10 16:37:30,429][main.py][line:182][INFO] Config:{'dataset': 'ucf-crime', 'model_name': 'ucf_', 'metrics': 'AUC', 'feat_prefix': './data/ucf-i3d', 'train_list': './list/ucf/train.list', 'test_list': './list/ucf/test.list', 'token_feat': './list/ucf/ucf-prompt.npy', 'gt': './list/ucf/ucf-gt.npy', 'win_size': 9, 'gamma': 0.6, 'bias': 0.2, 'norm': True, 't_step': 9, 'temp': 0.09, 'seed': 2023, 'test_bs': 10, 'smooth': 'slide', 'kappa': 8, 'ckpt_path': './ckpt/ucf__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 1.1, 'alpha': 0.5, 'margin': 100, 'max_epoch': 6, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/ucf/features/', 'result_dir': './result/ucf/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 16:37:30,610][main.py][line:259][INFO] total params:8.3485M
[2024-03-10 16:37:30,610][main.py][line:268][INFO] Test Mode
[2024-03-10 16:37:30,610][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/ucf__current.pkl.
[2024-03-10 16:37:48,642][infer.py][line:94][INFO] offline AUC:0.8988 Anomaly-AUC:0.7513 AP:0.4126 FAR:0.0295 | Complete in 0m 18s

[2024-03-10 16:38:00,152][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 16:38:03,278][main.py][line:259][INFO] total params:8.3467M
[2024-03-10 16:38:03,278][main.py][line:262][INFO] Training Mode
[2024-03-10 16:38:03,278][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 16:38:03,278][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 16:38:16,532][main.py][line:76][INFO] Random initialize AUCAP:0.2726 Anomaly AUC:0.58987
[2024-03-10 16:39:27,539][main.py][line:153][INFO] [Epoch:1/20]: lr:0.00010 | loss1:0.2455 loss2:0.5968 loss3:0.4367 | AUC:0.7937 Anomaly AUC:0.9361
[2024-03-10 16:39:42,355][main.py][line:124][INFO] [Epoch:2/20, Batch:9/320]: loss1:0.1921 loss2:0.5691 loss3:0.4414 | AUC:0.7839 Anomaly AUC:0.9351
[2024-03-10 16:39:56,611][main.py][line:124][INFO] [Epoch:2/20, Batch:19/320]: loss1:0.1499 loss2:0.6057 loss3:0.4474 | AUC:0.7692 Anomaly AUC:0.9327
[2024-03-10 16:40:11,135][main.py][line:124][INFO] [Epoch:2/20, Batch:29/320]: loss1:0.2211 loss2:0.5842 loss3:0.4461 | AUC:0.7368 Anomaly AUC:0.9251
[2024-03-10 16:40:25,722][main.py][line:124][INFO] [Epoch:2/20, Batch:39/320]: loss1:0.0974 loss2:0.4805 loss3:0.4536 | AUC:0.7835 Anomaly AUC:0.9354
[2024-03-10 16:40:40,448][main.py][line:124][INFO] [Epoch:2/20, Batch:49/320]: loss1:0.1556 loss2:0.5480 loss3:0.4620 | AUC:0.7495 Anomaly AUC:0.9311
[2024-03-10 16:40:54,954][main.py][line:124][INFO] [Epoch:2/20, Batch:59/320]: loss1:0.1918 loss2:0.6227 loss3:0.4420 | AUC:0.7362 Anomaly AUC:0.9221
[2024-03-10 16:41:09,690][main.py][line:124][INFO] [Epoch:2/20, Batch:69/320]: loss1:0.1509 loss2:0.5034 loss3:0.4636 | AUC:0.7541 Anomaly AUC:0.9306
[2024-03-10 16:41:24,425][main.py][line:124][INFO] [Epoch:2/20, Batch:79/320]: loss1:0.1337 loss2:0.5265 loss3:0.4508 | AUC:0.7755 Anomaly AUC:0.9319
[2024-03-10 16:41:39,021][main.py][line:124][INFO] [Epoch:2/20, Batch:89/320]: loss1:0.1952 loss2:0.5767 loss3:0.4360 | AUC:0.6966 Anomaly AUC:0.9161
[2024-03-10 16:41:53,572][main.py][line:124][INFO] [Epoch:2/20, Batch:99/320]: loss1:0.1696 loss2:0.4974 loss3:0.4324 | AUC:0.7841 Anomaly AUC:0.9337
[2024-03-10 16:42:08,393][main.py][line:124][INFO] [Epoch:2/20, Batch:109/320]: loss1:0.2954 loss2:0.6088 loss3:0.4292 | AUC:0.7408 Anomaly AUC:0.9271
[2024-03-10 16:42:23,020][main.py][line:124][INFO] [Epoch:2/20, Batch:119/320]: loss1:0.1799 loss2:0.4945 loss3:0.4216 | AUC:0.7493 Anomaly AUC:0.9257
[2024-03-10 16:42:37,571][main.py][line:124][INFO] [Epoch:2/20, Batch:129/320]: loss1:0.2475 loss2:0.4827 loss3:0.4410 | AUC:0.7529 Anomaly AUC:0.9299
[2024-03-10 16:42:52,241][main.py][line:124][INFO] [Epoch:2/20, Batch:139/320]: loss1:0.2131 loss2:0.5457 loss3:0.4200 | AUC:0.7606 Anomaly AUC:0.9275
[2024-03-10 16:43:06,888][main.py][line:124][INFO] [Epoch:2/20, Batch:149/320]: loss1:0.1466 loss2:0.4527 loss3:0.4286 | AUC:0.7790 Anomaly AUC:0.9356
[2024-03-10 16:43:21,399][main.py][line:124][INFO] [Epoch:2/20, Batch:159/320]: loss1:0.1892 loss2:0.5206 loss3:0.4322 | AUC:0.7222 Anomaly AUC:0.9180
[2024-03-10 16:43:36,057][main.py][line:124][INFO] [Epoch:2/20, Batch:169/320]: loss1:0.1085 loss2:0.4181 loss3:0.4319 | AUC:0.7513 Anomaly AUC:0.9274
[2024-03-10 16:43:50,896][main.py][line:124][INFO] [Epoch:2/20, Batch:179/320]: loss1:0.1916 loss2:0.5080 loss3:0.4377 | AUC:0.7759 Anomaly AUC:0.9357
[2024-03-10 16:44:05,617][main.py][line:124][INFO] [Epoch:2/20, Batch:189/320]: loss1:0.1142 loss2:0.4076 loss3:0.4252 | AUC:0.7646 Anomaly AUC:0.9326
[2024-03-10 16:44:20,188][main.py][line:124][INFO] [Epoch:2/20, Batch:199/320]: loss1:0.1389 loss2:0.4119 loss3:0.4136 | AUC:0.7474 Anomaly AUC:0.9280
[2024-03-10 16:44:34,889][main.py][line:124][INFO] [Epoch:2/20, Batch:209/320]: loss1:0.1167 loss2:0.4305 loss3:0.4270 | AUC:0.7477 Anomaly AUC:0.9240
[2024-03-10 16:44:49,550][main.py][line:124][INFO] [Epoch:2/20, Batch:219/320]: loss1:0.1181 loss2:0.4507 loss3:0.4296 | AUC:0.8031 Anomaly AUC:0.9413
[2024-03-10 16:45:04,345][main.py][line:124][INFO] [Epoch:2/20, Batch:229/320]: loss1:0.2805 loss2:0.5166 loss3:0.4220 | AUC:0.6876 Anomaly AUC:0.9077
[2024-03-10 16:45:18,985][main.py][line:124][INFO] [Epoch:2/20, Batch:239/320]: loss1:0.2034 loss2:0.4585 loss3:0.4072 | AUC:0.7334 Anomaly AUC:0.9276
[2024-03-10 16:45:34,109][main.py][line:124][INFO] [Epoch:2/20, Batch:249/320]: loss1:0.1178 loss2:0.3599 loss3:0.4138 | AUC:0.7485 Anomaly AUC:0.9224
[2024-03-10 16:45:48,996][main.py][line:124][INFO] [Epoch:2/20, Batch:259/320]: loss1:0.3657 loss2:0.4854 loss3:0.4195 | AUC:0.7413 Anomaly AUC:0.9288
[2024-03-10 16:46:03,659][main.py][line:124][INFO] [Epoch:2/20, Batch:269/320]: loss1:0.1353 loss2:0.4330 loss3:0.3981 | AUC:0.8359 Anomaly AUC:0.9471
[2024-03-10 16:46:18,542][main.py][line:124][INFO] [Epoch:2/20, Batch:279/320]: loss1:0.0967 loss2:0.4034 loss3:0.4104 | AUC:0.7627 Anomaly AUC:0.9341
[2024-03-10 16:46:33,163][main.py][line:124][INFO] [Epoch:2/20, Batch:289/320]: loss1:0.2145 loss2:0.4508 loss3:0.4184 | AUC:0.6767 Anomaly AUC:0.9154
[2024-03-10 16:46:47,130][main.py][line:153][INFO] [Epoch:2/20]: lr:0.00010 | loss1:0.1781 loss2:0.4240 loss3:0.4147 | AUC:0.7315 Anomaly AUC:0.9252
[2024-03-10 16:47:02,304][main.py][line:124][INFO] [Epoch:3/20, Batch:9/320]: loss1:0.1432 loss2:0.4385 loss3:0.4037 | AUC:0.7748 Anomaly AUC:0.9319
[2024-03-10 16:47:16,763][main.py][line:124][INFO] [Epoch:3/20, Batch:19/320]: loss1:0.0866 loss2:0.3686 loss3:0.4135 | AUC:0.7609 Anomaly AUC:0.9298
[2024-03-10 16:47:31,571][main.py][line:124][INFO] [Epoch:3/20, Batch:29/320]: loss1:0.0481 loss2:0.3635 loss3:0.4075 | AUC:0.7708 Anomaly AUC:0.9328
[2024-03-10 16:47:46,072][main.py][line:124][INFO] [Epoch:3/20, Batch:39/320]: loss1:0.1271 loss2:0.4157 loss3:0.4004 | AUC:0.7413 Anomaly AUC:0.9279
[2024-03-10 16:48:00,782][main.py][line:124][INFO] [Epoch:3/20, Batch:49/320]: loss1:0.0862 loss2:0.3325 loss3:0.4033 | AUC:0.7485 Anomaly AUC:0.9272
[2024-03-10 16:48:15,411][main.py][line:124][INFO] [Epoch:3/20, Batch:59/320]: loss1:0.1000 loss2:0.3639 loss3:0.4036 | AUC:0.7915 Anomaly AUC:0.9390
[2024-03-10 16:48:29,946][main.py][line:124][INFO] [Epoch:3/20, Batch:69/320]: loss1:0.0697 loss2:0.3484 loss3:0.4043 | AUC:0.7690 Anomaly AUC:0.9293
[2024-03-10 16:48:44,519][main.py][line:124][INFO] [Epoch:3/20, Batch:79/320]: loss1:0.2749 loss2:0.4908 loss3:0.4014 | AUC:0.7590 Anomaly AUC:0.9305
[2024-03-10 16:48:59,077][main.py][line:124][INFO] [Epoch:3/20, Batch:89/320]: loss1:0.0453 loss2:0.3337 loss3:0.3940 | AUC:0.7893 Anomaly AUC:0.9376
[2024-03-10 16:49:13,992][main.py][line:124][INFO] [Epoch:3/20, Batch:99/320]: loss1:0.0340 loss2:0.2870 loss3:0.4013 | AUC:0.7647 Anomaly AUC:0.9335
[2024-03-10 16:49:28,748][main.py][line:124][INFO] [Epoch:3/20, Batch:109/320]: loss1:0.1353 loss2:0.3950 loss3:0.3944 | AUC:0.7532 Anomaly AUC:0.9315
[2024-03-10 16:49:43,595][main.py][line:124][INFO] [Epoch:3/20, Batch:119/320]: loss1:0.0202 loss2:0.2823 loss3:0.3951 | AUC:0.7426 Anomaly AUC:0.9265
[2024-03-10 16:49:58,046][main.py][line:124][INFO] [Epoch:3/20, Batch:129/320]: loss1:0.0751 loss2:0.3354 loss3:0.4001 | AUC:0.7889 Anomaly AUC:0.9386
[2024-03-10 16:50:12,559][main.py][line:124][INFO] [Epoch:3/20, Batch:139/320]: loss1:0.0565 loss2:0.3095 loss3:0.3894 | AUC:0.7246 Anomaly AUC:0.9198
[2024-03-10 16:50:27,370][main.py][line:124][INFO] [Epoch:3/20, Batch:149/320]: loss1:0.0493 loss2:0.3395 loss3:0.3875 | AUC:0.7743 Anomaly AUC:0.9388
[2024-03-10 16:50:41,878][main.py][line:124][INFO] [Epoch:3/20, Batch:159/320]: loss1:0.1081 loss2:0.3242 loss3:0.3772 | AUC:0.7726 Anomaly AUC:0.9321
[2024-03-10 16:50:56,467][main.py][line:124][INFO] [Epoch:3/20, Batch:169/320]: loss1:0.0761 loss2:0.3260 loss3:0.3826 | AUC:0.7221 Anomaly AUC:0.9279
[2024-03-10 16:51:10,945][main.py][line:124][INFO] [Epoch:3/20, Batch:179/320]: loss1:0.0809 loss2:0.2661 loss3:0.3841 | AUC:0.7912 Anomaly AUC:0.9358
[2024-03-10 16:51:25,528][main.py][line:124][INFO] [Epoch:3/20, Batch:189/320]: loss1:0.0931 loss2:0.3566 loss3:0.3813 | AUC:0.7554 Anomaly AUC:0.9303
[2024-03-10 16:51:40,003][main.py][line:124][INFO] [Epoch:3/20, Batch:199/320]: loss1:0.1335 loss2:0.3604 loss3:0.3803 | AUC:0.7228 Anomaly AUC:0.9254
[2024-03-10 16:51:54,926][main.py][line:124][INFO] [Epoch:3/20, Batch:209/320]: loss1:0.0926 loss2:0.3634 loss3:0.3691 | AUC:0.7800 Anomaly AUC:0.9382
[2024-03-10 16:52:09,562][main.py][line:124][INFO] [Epoch:3/20, Batch:219/320]: loss1:0.0592 loss2:0.3231 loss3:0.3714 | AUC:0.7865 Anomaly AUC:0.9384
[2024-03-10 16:52:23,949][main.py][line:124][INFO] [Epoch:3/20, Batch:229/320]: loss1:0.0736 loss2:0.3190 loss3:0.3787 | AUC:0.7643 Anomaly AUC:0.9346
[2024-03-10 16:52:38,353][main.py][line:124][INFO] [Epoch:3/20, Batch:239/320]: loss1:0.0505 loss2:0.2838 loss3:0.3721 | AUC:0.7390 Anomaly AUC:0.9190
[2024-03-10 16:52:52,761][main.py][line:124][INFO] [Epoch:3/20, Batch:249/320]: loss1:0.1480 loss2:0.3195 loss3:0.3763 | AUC:0.7061 Anomaly AUC:0.9225
[2024-03-10 16:53:07,268][main.py][line:124][INFO] [Epoch:3/20, Batch:259/320]: loss1:0.0270 loss2:0.2621 loss3:0.3738 | AUC:0.7820 Anomaly AUC:0.9351
[2024-03-10 16:53:21,657][main.py][line:124][INFO] [Epoch:3/20, Batch:269/320]: loss1:0.0475 loss2:0.3018 loss3:0.3729 | AUC:0.7891 Anomaly AUC:0.9337
[2024-03-10 16:53:36,505][main.py][line:124][INFO] [Epoch:3/20, Batch:279/320]: loss1:0.1373 loss2:0.3320 loss3:0.3734 | AUC:0.7323 Anomaly AUC:0.9234
[2024-03-10 16:53:51,029][main.py][line:124][INFO] [Epoch:3/20, Batch:289/320]: loss1:0.1988 loss2:0.4375 loss3:0.3660 | AUC:0.7898 Anomaly AUC:0.9322
[2024-03-10 16:54:05,095][main.py][line:153][INFO] [Epoch:3/20]: lr:0.00010 | loss1:0.0884 loss2:0.3416 loss3:0.3618 | AUC:0.8096 Anomaly AUC:0.9386
[2024-03-10 16:54:20,141][main.py][line:124][INFO] [Epoch:4/20, Batch:9/320]: loss1:0.0346 loss2:0.3135 loss3:0.3689 | AUC:0.7413 Anomaly AUC:0.9253
[2024-03-10 16:54:35,048][main.py][line:124][INFO] [Epoch:4/20, Batch:19/320]: loss1:0.0375 loss2:0.2957 loss3:0.3674 | AUC:0.7538 Anomaly AUC:0.9267
[2024-03-10 16:54:49,600][main.py][line:124][INFO] [Epoch:4/20, Batch:29/320]: loss1:0.0525 loss2:0.3013 loss3:0.3661 | AUC:0.7674 Anomaly AUC:0.9247
[2024-03-10 16:55:04,214][main.py][line:124][INFO] [Epoch:4/20, Batch:39/320]: loss1:0.1549 loss2:0.3909 loss3:0.3635 | AUC:0.7673 Anomaly AUC:0.9306
[2024-03-10 16:55:18,771][main.py][line:124][INFO] [Epoch:4/20, Batch:49/320]: loss1:0.0326 loss2:0.2887 loss3:0.3621 | AUC:0.7308 Anomaly AUC:0.9177
[2024-03-10 16:55:33,425][main.py][line:124][INFO] [Epoch:4/20, Batch:59/320]: loss1:0.0471 loss2:0.2829 loss3:0.3566 | AUC:0.7278 Anomaly AUC:0.9178
[2024-03-10 16:55:47,891][main.py][line:124][INFO] [Epoch:4/20, Batch:69/320]: loss1:0.0432 loss2:0.2509 loss3:0.3619 | AUC:0.8013 Anomaly AUC:0.9339
[2024-03-10 16:56:02,428][main.py][line:124][INFO] [Epoch:4/20, Batch:79/320]: loss1:0.0677 loss2:0.3323 loss3:0.3619 | AUC:0.8175 Anomaly AUC:0.9385
[2024-03-10 16:56:16,955][main.py][line:124][INFO] [Epoch:4/20, Batch:89/320]: loss1:0.0588 loss2:0.2939 loss3:0.3592 | AUC:0.7647 Anomaly AUC:0.9245
[2024-03-10 16:56:31,373][main.py][line:124][INFO] [Epoch:4/20, Batch:99/320]: loss1:0.1189 loss2:0.3554 loss3:0.3525 | AUC:0.8300 Anomaly AUC:0.9415
[2024-03-10 16:56:45,865][main.py][line:124][INFO] [Epoch:4/20, Batch:109/320]: loss1:0.0274 loss2:0.2385 loss3:0.3580 | AUC:0.6995 Anomaly AUC:0.9239
[2024-03-10 16:57:00,438][main.py][line:124][INFO] [Epoch:4/20, Batch:119/320]: loss1:0.1802 loss2:0.4189 loss3:0.3647 | AUC:0.7679 Anomaly AUC:0.9311
[2024-03-10 16:57:14,863][main.py][line:124][INFO] [Epoch:4/20, Batch:129/320]: loss1:0.1049 loss2:0.2864 loss3:0.3475 | AUC:0.7262 Anomaly AUC:0.9245
[2024-03-10 16:57:29,262][main.py][line:124][INFO] [Epoch:4/20, Batch:139/320]: loss1:0.0792 loss2:0.2193 loss3:0.3520 | AUC:0.7542 Anomaly AUC:0.9252
[2024-03-10 16:57:43,709][main.py][line:124][INFO] [Epoch:4/20, Batch:149/320]: loss1:0.0217 loss2:0.2498 loss3:0.3531 | AUC:0.7928 Anomaly AUC:0.9334
[2024-03-10 16:57:58,314][main.py][line:124][INFO] [Epoch:4/20, Batch:159/320]: loss1:0.0797 loss2:0.3969 loss3:0.3499 | AUC:0.7125 Anomaly AUC:0.9233
[2024-03-10 16:58:12,663][main.py][line:124][INFO] [Epoch:4/20, Batch:169/320]: loss1:0.0672 loss2:0.2960 loss3:0.3462 | AUC:0.7907 Anomaly AUC:0.9314
[2024-03-10 16:58:27,081][main.py][line:124][INFO] [Epoch:4/20, Batch:179/320]: loss1:0.0246 loss2:0.2588 loss3:0.3477 | AUC:0.8141 Anomaly AUC:0.9369
[2024-03-10 16:58:41,614][main.py][line:124][INFO] [Epoch:4/20, Batch:189/320]: loss1:0.0393 loss2:0.2464 loss3:0.3484 | AUC:0.7489 Anomaly AUC:0.9267
[2024-03-10 16:58:56,006][main.py][line:124][INFO] [Epoch:4/20, Batch:199/320]: loss1:0.0658 loss2:0.2249 loss3:0.3437 | AUC:0.7251 Anomaly AUC:0.9239
[2024-03-10 16:59:10,520][main.py][line:124][INFO] [Epoch:4/20, Batch:209/320]: loss1:0.0485 loss2:0.2750 loss3:0.3449 | AUC:0.7612 Anomaly AUC:0.9261
[2024-03-10 16:59:24,833][main.py][line:124][INFO] [Epoch:4/20, Batch:219/320]: loss1:0.0826 loss2:0.2545 loss3:0.3464 | AUC:0.7735 Anomaly AUC:0.9244
[2024-03-10 16:59:39,449][main.py][line:124][INFO] [Epoch:4/20, Batch:229/320]: loss1:0.0359 loss2:0.2975 loss3:0.3452 | AUC:0.7921 Anomaly AUC:0.9311
[2024-03-10 16:59:53,928][main.py][line:124][INFO] [Epoch:4/20, Batch:239/320]: loss1:0.1351 loss2:0.4388 loss3:0.3483 | AUC:0.7175 Anomaly AUC:0.9162
[2024-03-10 17:00:08,465][main.py][line:124][INFO] [Epoch:4/20, Batch:249/320]: loss1:0.0146 loss2:0.2388 loss3:0.3451 | AUC:0.7135 Anomaly AUC:0.9210
[2024-03-10 17:00:22,937][main.py][line:124][INFO] [Epoch:4/20, Batch:259/320]: loss1:0.0684 loss2:0.2784 loss3:0.3432 | AUC:0.7944 Anomaly AUC:0.9318
[2024-03-10 17:00:37,519][main.py][line:124][INFO] [Epoch:4/20, Batch:269/320]: loss1:0.0114 loss2:0.2354 loss3:0.3415 | AUC:0.7383 Anomaly AUC:0.9229
[2024-03-10 17:00:52,256][main.py][line:124][INFO] [Epoch:4/20, Batch:279/320]: loss1:0.0683 loss2:0.2804 loss3:0.3411 | AUC:0.7844 Anomaly AUC:0.9334
[2024-03-10 17:01:06,877][main.py][line:124][INFO] [Epoch:4/20, Batch:289/320]: loss1:0.1058 loss2:0.2752 loss3:0.3469 | AUC:0.7812 Anomaly AUC:0.9299
[2024-03-10 17:01:20,864][main.py][line:153][INFO] [Epoch:4/20]: lr:0.00010 | loss1:0.0115 loss2:0.2314 loss3:0.3409 | AUC:0.7336 Anomaly AUC:0.9256
[2024-03-10 17:01:35,892][main.py][line:124][INFO] [Epoch:5/20, Batch:9/320]: loss1:0.0435 loss2:0.1791 loss3:0.3421 | AUC:0.7574 Anomaly AUC:0.9269
[2024-03-10 17:01:50,337][main.py][line:124][INFO] [Epoch:5/20, Batch:19/320]: loss1:0.0297 loss2:0.2764 loss3:0.3427 | AUC:0.8082 Anomaly AUC:0.9338
[2024-03-10 17:02:04,784][main.py][line:124][INFO] [Epoch:5/20, Batch:29/320]: loss1:0.0554 loss2:0.2464 loss3:0.3482 | AUC:0.7554 Anomaly AUC:0.9224
[2024-03-10 17:02:19,322][main.py][line:124][INFO] [Epoch:5/20, Batch:39/320]: loss1:0.0509 loss2:0.3229 loss3:0.3415 | AUC:0.7550 Anomaly AUC:0.9196
[2024-03-10 17:02:33,894][main.py][line:124][INFO] [Epoch:5/20, Batch:49/320]: loss1:0.0585 loss2:0.3247 loss3:0.3467 | AUC:0.7977 Anomaly AUC:0.9346
[2024-03-10 17:02:48,283][main.py][line:124][INFO] [Epoch:5/20, Batch:59/320]: loss1:0.0726 loss2:0.2788 loss3:0.3468 | AUC:0.7219 Anomaly AUC:0.9115
[2024-03-10 17:03:02,745][main.py][line:124][INFO] [Epoch:5/20, Batch:69/320]: loss1:0.0330 loss2:0.2303 loss3:0.3489 | AUC:0.6625 Anomaly AUC:0.9030
[2024-03-10 17:03:17,345][main.py][line:124][INFO] [Epoch:5/20, Batch:79/320]: loss1:0.0522 loss2:0.2432 loss3:0.3448 | AUC:0.7385 Anomaly AUC:0.9184
[2024-03-10 17:03:31,815][main.py][line:124][INFO] [Epoch:5/20, Batch:89/320]: loss1:0.0497 loss2:0.2330 loss3:0.3411 | AUC:0.7478 Anomaly AUC:0.9198
[2024-03-10 17:03:46,344][main.py][line:124][INFO] [Epoch:5/20, Batch:99/320]: loss1:0.0222 loss2:0.2484 loss3:0.3450 | AUC:0.7500 Anomaly AUC:0.9265
[2024-03-10 17:04:00,874][main.py][line:124][INFO] [Epoch:5/20, Batch:109/320]: loss1:0.0442 loss2:0.2375 loss3:0.3411 | AUC:0.8161 Anomaly AUC:0.9378
[2024-03-10 17:04:15,263][main.py][line:124][INFO] [Epoch:5/20, Batch:119/320]: loss1:0.0360 loss2:0.3050 loss3:0.3389 | AUC:0.7867 Anomaly AUC:0.9314
[2024-03-10 17:04:29,754][main.py][line:124][INFO] [Epoch:5/20, Batch:129/320]: loss1:0.0397 loss2:0.2266 loss3:0.3417 | AUC:0.7564 Anomaly AUC:0.9270
[2024-03-10 17:04:44,036][main.py][line:124][INFO] [Epoch:5/20, Batch:139/320]: loss1:0.0397 loss2:0.3087 loss3:0.3373 | AUC:0.7631 Anomaly AUC:0.9269
[2024-03-10 17:04:58,682][main.py][line:124][INFO] [Epoch:5/20, Batch:149/320]: loss1:0.0410 loss2:0.2473 loss3:0.3392 | AUC:0.8297 Anomaly AUC:0.9374
[2024-03-10 17:05:13,185][main.py][line:124][INFO] [Epoch:5/20, Batch:159/320]: loss1:0.0298 loss2:0.2924 loss3:0.3462 | AUC:0.7424 Anomaly AUC:0.9211
[2024-03-10 17:05:27,811][main.py][line:124][INFO] [Epoch:5/20, Batch:169/320]: loss1:0.0283 loss2:0.1842 loss3:0.3392 | AUC:0.7845 Anomaly AUC:0.9306
[2024-03-10 17:05:42,419][main.py][line:124][INFO] [Epoch:5/20, Batch:179/320]: loss1:0.0481 loss2:0.2580 loss3:0.3379 | AUC:0.7775 Anomaly AUC:0.9284
[2024-03-10 17:05:57,081][main.py][line:124][INFO] [Epoch:5/20, Batch:189/320]: loss1:0.0157 loss2:0.1565 loss3:0.3353 | AUC:0.7460 Anomaly AUC:0.9238
[2024-03-10 17:06:11,587][main.py][line:124][INFO] [Epoch:5/20, Batch:199/320]: loss1:0.0148 loss2:0.2069 loss3:0.3387 | AUC:0.7240 Anomaly AUC:0.9188
[2024-03-10 17:06:26,087][main.py][line:124][INFO] [Epoch:5/20, Batch:209/320]: loss1:0.0157 loss2:0.1883 loss3:0.3366 | AUC:0.7901 Anomaly AUC:0.9310
[2024-03-10 17:06:40,508][main.py][line:124][INFO] [Epoch:5/20, Batch:219/320]: loss1:0.0235 loss2:0.2284 loss3:0.3371 | AUC:0.8196 Anomaly AUC:0.9374
[2024-03-10 17:06:55,147][main.py][line:124][INFO] [Epoch:5/20, Batch:229/320]: loss1:0.0421 loss2:0.2209 loss3:0.3336 | AUC:0.7807 Anomaly AUC:0.9296
[2024-03-10 17:07:09,797][main.py][line:124][INFO] [Epoch:5/20, Batch:239/320]: loss1:0.0843 loss2:0.3458 loss3:0.3366 | AUC:0.7694 Anomaly AUC:0.9254
[2024-03-10 17:07:24,217][main.py][line:124][INFO] [Epoch:5/20, Batch:249/320]: loss1:0.0392 loss2:0.2300 loss3:0.3427 | AUC:0.7886 Anomaly AUC:0.9391
[2024-03-10 17:07:38,772][main.py][line:124][INFO] [Epoch:5/20, Batch:259/320]: loss1:0.0249 loss2:0.1728 loss3:0.3405 | AUC:0.7712 Anomaly AUC:0.9313
[2024-03-10 17:07:53,370][main.py][line:124][INFO] [Epoch:5/20, Batch:269/320]: loss1:0.0954 loss2:0.2326 loss3:0.3414 | AUC:0.7418 Anomaly AUC:0.9242
[2024-03-10 17:08:07,766][main.py][line:124][INFO] [Epoch:5/20, Batch:279/320]: loss1:0.0213 loss2:0.2616 loss3:0.3390 | AUC:0.8037 Anomaly AUC:0.9335
[2024-03-10 17:08:22,213][main.py][line:124][INFO] [Epoch:5/20, Batch:289/320]: loss1:0.1135 loss2:0.2399 loss3:0.3415 | AUC:0.8088 Anomaly AUC:0.9367
[2024-03-10 17:08:36,091][main.py][line:153][INFO] [Epoch:5/20]: lr:0.00010 | loss1:0.0094 loss2:0.2459 loss3:0.3418 | AUC:0.7920 Anomaly AUC:0.9348
[2024-03-10 17:08:50,906][main.py][line:124][INFO] [Epoch:6/20, Batch:9/320]: loss1:0.0432 loss2:0.2512 loss3:0.3378 | AUC:0.7934 Anomaly AUC:0.9394
[2024-03-10 17:09:05,123][main.py][line:124][INFO] [Epoch:6/20, Batch:19/320]: loss1:0.0320 loss2:0.2640 loss3:0.3376 | AUC:0.7576 Anomaly AUC:0.9272
[2024-03-10 17:09:19,596][main.py][line:124][INFO] [Epoch:6/20, Batch:29/320]: loss1:0.0149 loss2:0.1728 loss3:0.3394 | AUC:0.6983 Anomaly AUC:0.9167
[2024-03-10 17:09:34,227][main.py][line:124][INFO] [Epoch:6/20, Batch:39/320]: loss1:0.0516 loss2:0.2285 loss3:0.3452 | AUC:0.7654 Anomaly AUC:0.9292
[2024-03-10 17:09:48,691][main.py][line:124][INFO] [Epoch:6/20, Batch:49/320]: loss1:0.0385 loss2:0.2209 loss3:0.3403 | AUC:0.7796 Anomaly AUC:0.9307
[2024-03-10 17:10:03,438][main.py][line:124][INFO] [Epoch:6/20, Batch:59/320]: loss1:0.0860 loss2:0.2888 loss3:0.3423 | AUC:0.6538 Anomaly AUC:0.9165
[2024-03-10 17:10:17,938][main.py][line:124][INFO] [Epoch:6/20, Batch:69/320]: loss1:0.0191 loss2:0.2202 loss3:0.3420 | AUC:0.7468 Anomaly AUC:0.9278
[2024-03-10 17:10:32,402][main.py][line:124][INFO] [Epoch:6/20, Batch:79/320]: loss1:0.0258 loss2:0.2143 loss3:0.3397 | AUC:0.7918 Anomaly AUC:0.9362
[2024-03-10 17:10:46,887][main.py][line:124][INFO] [Epoch:6/20, Batch:89/320]: loss1:0.0125 loss2:0.2091 loss3:0.3440 | AUC:0.7612 Anomaly AUC:0.9304
[2024-03-10 17:11:01,380][main.py][line:124][INFO] [Epoch:6/20, Batch:99/320]: loss1:0.0740 loss2:0.2371 loss3:0.3412 | AUC:0.7025 Anomaly AUC:0.9199
[2024-03-10 17:11:15,843][main.py][line:124][INFO] [Epoch:6/20, Batch:109/320]: loss1:0.0349 loss2:0.1979 loss3:0.3362 | AUC:0.7641 Anomaly AUC:0.9298
[2024-03-10 17:11:30,399][main.py][line:124][INFO] [Epoch:6/20, Batch:119/320]: loss1:0.0265 loss2:0.2888 loss3:0.3413 | AUC:0.7321 Anomaly AUC:0.9259
[2024-03-10 17:11:44,961][main.py][line:124][INFO] [Epoch:6/20, Batch:129/320]: loss1:0.0335 loss2:0.2451 loss3:0.3427 | AUC:0.7241 Anomaly AUC:0.9225
[2024-03-10 17:11:59,642][main.py][line:124][INFO] [Epoch:6/20, Batch:139/320]: loss1:0.0678 loss2:0.2512 loss3:0.3382 | AUC:0.7835 Anomaly AUC:0.9337
[2024-03-10 17:12:14,082][main.py][line:124][INFO] [Epoch:6/20, Batch:149/320]: loss1:0.1030 loss2:0.2528 loss3:0.3388 | AUC:0.7971 Anomaly AUC:0.9354
[2024-03-10 17:12:28,404][main.py][line:124][INFO] [Epoch:6/20, Batch:159/320]: loss1:0.0238 loss2:0.1764 loss3:0.3375 | AUC:0.8060 Anomaly AUC:0.9415
[2024-03-10 17:12:42,968][main.py][line:124][INFO] [Epoch:6/20, Batch:169/320]: loss1:0.0507 loss2:0.2219 loss3:0.3431 | AUC:0.7568 Anomaly AUC:0.9314
[2024-03-10 17:12:57,590][main.py][line:124][INFO] [Epoch:6/20, Batch:179/320]: loss1:0.0313 loss2:0.1580 loss3:0.3378 | AUC:0.7358 Anomaly AUC:0.9144
[2024-03-10 17:13:12,066][main.py][line:124][INFO] [Epoch:6/20, Batch:189/320]: loss1:0.0130 loss2:0.1783 loss3:0.3391 | AUC:0.7713 Anomaly AUC:0.9247
[2024-03-10 17:13:27,070][main.py][line:124][INFO] [Epoch:6/20, Batch:199/320]: loss1:0.0183 loss2:0.3263 loss3:0.3363 | AUC:0.7976 Anomaly AUC:0.9345
[2024-03-10 17:13:41,624][main.py][line:124][INFO] [Epoch:6/20, Batch:209/320]: loss1:0.0098 loss2:0.2016 loss3:0.3354 | AUC:0.8146 Anomaly AUC:0.9355
[2024-03-10 17:13:56,146][main.py][line:124][INFO] [Epoch:6/20, Batch:219/320]: loss1:0.0153 loss2:0.1420 loss3:0.3378 | AUC:0.8007 Anomaly AUC:0.9325
[2024-03-10 17:14:10,760][main.py][line:124][INFO] [Epoch:6/20, Batch:229/320]: loss1:0.0049 loss2:0.1846 loss3:0.3363 | AUC:0.7915 Anomaly AUC:0.9337
[2024-03-10 17:14:25,502][main.py][line:124][INFO] [Epoch:6/20, Batch:239/320]: loss1:0.0108 loss2:0.2012 loss3:0.3370 | AUC:0.7846 Anomaly AUC:0.9308
[2024-03-10 17:14:39,983][main.py][line:124][INFO] [Epoch:6/20, Batch:249/320]: loss1:0.0165 loss2:0.2072 loss3:0.3349 | AUC:0.7709 Anomaly AUC:0.9296
[2024-03-10 17:14:54,482][main.py][line:124][INFO] [Epoch:6/20, Batch:259/320]: loss1:0.0168 loss2:0.2339 loss3:0.3439 | AUC:0.7421 Anomaly AUC:0.9222
[2024-03-10 17:15:09,201][main.py][line:124][INFO] [Epoch:6/20, Batch:269/320]: loss1:0.0571 loss2:0.2468 loss3:0.3429 | AUC:0.7481 Anomaly AUC:0.9228
[2024-03-10 17:15:23,795][main.py][line:124][INFO] [Epoch:6/20, Batch:279/320]: loss1:0.0208 loss2:0.1378 loss3:0.3368 | AUC:0.7842 Anomaly AUC:0.9291
[2024-03-10 17:15:38,423][main.py][line:124][INFO] [Epoch:6/20, Batch:289/320]: loss1:0.0131 loss2:0.1612 loss3:0.3340 | AUC:0.7917 Anomaly AUC:0.9313
[2024-03-10 17:15:53,144][main.py][line:153][INFO] [Epoch:6/20]: lr:0.00010 | loss1:0.2658 loss2:0.3396 loss3:0.3370 | AUC:0.8077 Anomaly AUC:0.9383
[2024-03-10 17:16:08,572][main.py][line:124][INFO] [Epoch:7/20, Batch:9/320]: loss1:0.0317 loss2:0.1936 loss3:0.3358 | AUC:0.7984 Anomaly AUC:0.9419
[2024-03-10 17:16:23,062][main.py][line:124][INFO] [Epoch:7/20, Batch:19/320]: loss1:0.0246 loss2:0.1975 loss3:0.3351 | AUC:0.7888 Anomaly AUC:0.9378
[2024-03-10 17:16:37,410][main.py][line:124][INFO] [Epoch:7/20, Batch:29/320]: loss1:0.0407 loss2:0.1477 loss3:0.3394 | AUC:0.7235 Anomaly AUC:0.9166
[2024-03-10 17:16:51,960][main.py][line:124][INFO] [Epoch:7/20, Batch:39/320]: loss1:0.0160 loss2:0.2056 loss3:0.3371 | AUC:0.8042 Anomaly AUC:0.9363
[2024-03-10 17:17:06,640][main.py][line:124][INFO] [Epoch:7/20, Batch:49/320]: loss1:0.0103 loss2:0.1283 loss3:0.3408 | AUC:0.8369 Anomaly AUC:0.9469
[2024-03-10 17:17:21,235][main.py][line:124][INFO] [Epoch:7/20, Batch:59/320]: loss1:0.0560 loss2:0.1599 loss3:0.3331 | AUC:0.8327 Anomaly AUC:0.9436
[2024-03-10 17:17:35,774][main.py][line:124][INFO] [Epoch:7/20, Batch:69/320]: loss1:0.0317 loss2:0.1629 loss3:0.3352 | AUC:0.7289 Anomaly AUC:0.9189
[2024-03-10 17:17:50,130][main.py][line:124][INFO] [Epoch:7/20, Batch:79/320]: loss1:0.0187 loss2:0.2517 loss3:0.3378 | AUC:0.6699 Anomaly AUC:0.9129
[2024-03-10 17:18:04,797][main.py][line:124][INFO] [Epoch:7/20, Batch:89/320]: loss1:0.0564 loss2:0.2195 loss3:0.3393 | AUC:0.7261 Anomaly AUC:0.9254
[2024-03-10 17:18:19,489][main.py][line:124][INFO] [Epoch:7/20, Batch:99/320]: loss1:0.0252 loss2:0.1215 loss3:0.3351 | AUC:0.7956 Anomaly AUC:0.9332
[2024-03-10 17:18:33,946][main.py][line:124][INFO] [Epoch:7/20, Batch:109/320]: loss1:0.0327 loss2:0.2471 loss3:0.3429 | AUC:0.7522 Anomaly AUC:0.9316
[2024-03-10 17:18:48,453][main.py][line:124][INFO] [Epoch:7/20, Batch:119/320]: loss1:0.0215 loss2:0.2417 loss3:0.3384 | AUC:0.7470 Anomaly AUC:0.9251
[2024-03-10 17:19:02,823][main.py][line:124][INFO] [Epoch:7/20, Batch:129/320]: loss1:0.1544 loss2:0.2389 loss3:0.3349 | AUC:0.8078 Anomaly AUC:0.9380
[2024-03-10 17:19:17,286][main.py][line:124][INFO] [Epoch:7/20, Batch:139/320]: loss1:0.0145 loss2:0.1932 loss3:0.3391 | AUC:0.7792 Anomaly AUC:0.9309
[2024-03-10 17:19:31,789][main.py][line:124][INFO] [Epoch:7/20, Batch:149/320]: loss1:0.1098 loss2:0.2305 loss3:0.3357 | AUC:0.7684 Anomaly AUC:0.9310
[2024-03-10 17:19:46,031][main.py][line:124][INFO] [Epoch:7/20, Batch:159/320]: loss1:0.0425 loss2:0.2099 loss3:0.3363 | AUC:0.7828 Anomaly AUC:0.9344
[2024-03-10 17:20:00,498][main.py][line:124][INFO] [Epoch:7/20, Batch:169/320]: loss1:0.0704 loss2:0.2196 loss3:0.3372 | AUC:0.7564 Anomaly AUC:0.9299
[2024-03-10 17:20:14,925][main.py][line:124][INFO] [Epoch:7/20, Batch:179/320]: loss1:0.0126 loss2:0.2064 loss3:0.3385 | AUC:0.7752 Anomaly AUC:0.9319
[2024-03-10 17:20:29,336][main.py][line:124][INFO] [Epoch:7/20, Batch:189/320]: loss1:0.0076 loss2:0.1824 loss3:0.3357 | AUC:0.7466 Anomaly AUC:0.9283
[2024-03-10 17:20:43,808][main.py][line:124][INFO] [Epoch:7/20, Batch:199/320]: loss1:0.0250 loss2:0.2152 loss3:0.3376 | AUC:0.7456 Anomaly AUC:0.9245
[2024-03-10 17:20:58,142][main.py][line:124][INFO] [Epoch:7/20, Batch:209/320]: loss1:0.1143 loss2:0.3151 loss3:0.3431 | AUC:0.6850 Anomaly AUC:0.9050
[2024-03-10 17:21:12,640][main.py][line:124][INFO] [Epoch:7/20, Batch:219/320]: loss1:0.0692 loss2:0.1789 loss3:0.3409 | AUC:0.8112 Anomaly AUC:0.9358
[2024-03-10 17:21:27,072][main.py][line:124][INFO] [Epoch:7/20, Batch:229/320]: loss1:0.0331 loss2:0.1462 loss3:0.3378 | AUC:0.8220 Anomaly AUC:0.9370
[2024-03-10 17:21:41,237][main.py][line:124][INFO] [Epoch:7/20, Batch:239/320]: loss1:0.0176 loss2:0.2089 loss3:0.3348 | AUC:0.8210 Anomaly AUC:0.9394
[2024-03-10 17:21:55,697][main.py][line:124][INFO] [Epoch:7/20, Batch:249/320]: loss1:0.0557 loss2:0.2035 loss3:0.3412 | AUC:0.7854 Anomaly AUC:0.9353
[2024-03-10 17:22:10,103][main.py][line:124][INFO] [Epoch:7/20, Batch:259/320]: loss1:0.0371 loss2:0.2346 loss3:0.3353 | AUC:0.7598 Anomaly AUC:0.9284
[2024-03-10 17:22:24,611][main.py][line:124][INFO] [Epoch:7/20, Batch:269/320]: loss1:0.0283 loss2:0.2827 loss3:0.3410 | AUC:0.6983 Anomaly AUC:0.9133
[2024-03-10 17:22:39,306][main.py][line:124][INFO] [Epoch:7/20, Batch:279/320]: loss1:0.0474 loss2:0.1846 loss3:0.3379 | AUC:0.7829 Anomaly AUC:0.9272
[2024-03-10 17:22:54,088][main.py][line:124][INFO] [Epoch:7/20, Batch:289/320]: loss1:0.0166 loss2:0.1795 loss3:0.3370 | AUC:0.7918 Anomaly AUC:0.9334
[2024-03-10 17:23:08,580][main.py][line:153][INFO] [Epoch:7/20]: lr:0.00010 | loss1:0.0243 loss2:0.1747 loss3:0.3419 | AUC:0.8043 Anomaly AUC:0.9330
[2024-03-10 17:23:23,272][main.py][line:124][INFO] [Epoch:8/20, Batch:9/320]: loss1:0.0080 loss2:0.2193 loss3:0.3371 | AUC:0.7875 Anomaly AUC:0.9343
[2024-03-10 17:23:37,950][main.py][line:124][INFO] [Epoch:8/20, Batch:19/320]: loss1:0.0353 loss2:0.1771 loss3:0.3366 | AUC:0.7547 Anomaly AUC:0.9260
[2024-03-10 17:23:53,295][main.py][line:124][INFO] [Epoch:8/20, Batch:29/320]: loss1:0.0548 loss2:0.1103 loss3:0.3345 | AUC:0.7208 Anomaly AUC:0.9216
[2024-03-10 17:24:08,239][main.py][line:124][INFO] [Epoch:8/20, Batch:39/320]: loss1:0.0173 loss2:0.1997 loss3:0.3422 | AUC:0.7549 Anomaly AUC:0.9281
[2024-03-10 17:24:22,678][main.py][line:124][INFO] [Epoch:8/20, Batch:49/320]: loss1:0.0414 loss2:0.2548 loss3:0.3358 | AUC:0.6707 Anomaly AUC:0.9091
[2024-03-10 17:24:37,032][main.py][line:124][INFO] [Epoch:8/20, Batch:59/320]: loss1:0.0169 loss2:0.1845 loss3:0.3351 | AUC:0.7385 Anomaly AUC:0.9245
[2024-03-10 17:24:51,616][main.py][line:124][INFO] [Epoch:8/20, Batch:69/320]: loss1:0.0498 loss2:0.2059 loss3:0.3380 | AUC:0.7389 Anomaly AUC:0.9214
[2024-03-10 17:25:06,583][main.py][line:124][INFO] [Epoch:8/20, Batch:79/320]: loss1:0.1048 loss2:0.2004 loss3:0.3366 | AUC:0.7487 Anomaly AUC:0.9262
[2024-03-10 17:25:21,193][main.py][line:124][INFO] [Epoch:8/20, Batch:89/320]: loss1:0.0285 loss2:0.2069 loss3:0.3378 | AUC:0.8065 Anomaly AUC:0.9409
[2024-03-10 17:25:35,701][main.py][line:124][INFO] [Epoch:8/20, Batch:99/320]: loss1:0.0171 loss2:0.1914 loss3:0.3354 | AUC:0.7845 Anomaly AUC:0.9364
[2024-03-10 17:25:50,208][main.py][line:124][INFO] [Epoch:8/20, Batch:109/320]: loss1:0.0266 loss2:0.1364 loss3:0.3393 | AUC:0.7504 Anomaly AUC:0.9308
[2024-03-10 17:26:04,629][main.py][line:124][INFO] [Epoch:8/20, Batch:119/320]: loss1:0.0104 loss2:0.0927 loss3:0.3315 | AUC:0.7448 Anomaly AUC:0.9296
[2024-03-10 17:26:19,132][main.py][line:124][INFO] [Epoch:8/20, Batch:129/320]: loss1:0.0077 loss2:0.1402 loss3:0.3375 | AUC:0.7472 Anomaly AUC:0.9303
[2024-03-10 17:26:33,963][main.py][line:124][INFO] [Epoch:8/20, Batch:139/320]: loss1:0.0141 loss2:0.1473 loss3:0.3346 | AUC:0.7194 Anomaly AUC:0.9254
[2024-03-10 17:26:48,432][main.py][line:124][INFO] [Epoch:8/20, Batch:149/320]: loss1:0.0156 loss2:0.1090 loss3:0.3318 | AUC:0.7352 Anomaly AUC:0.9301
[2024-03-10 17:27:02,888][main.py][line:124][INFO] [Epoch:8/20, Batch:159/320]: loss1:0.0073 loss2:0.1682 loss3:0.3355 | AUC:0.7315 Anomaly AUC:0.9286
[2024-03-10 17:27:17,438][main.py][line:124][INFO] [Epoch:8/20, Batch:169/320]: loss1:0.0540 loss2:0.2019 loss3:0.3402 | AUC:0.6993 Anomaly AUC:0.9205
[2024-03-10 17:27:31,703][main.py][line:124][INFO] [Epoch:8/20, Batch:179/320]: loss1:0.0094 loss2:0.1447 loss3:0.3393 | AUC:0.7373 Anomaly AUC:0.9310
[2024-03-10 17:27:46,117][main.py][line:124][INFO] [Epoch:8/20, Batch:189/320]: loss1:0.0357 loss2:0.1833 loss3:0.3365 | AUC:0.6897 Anomaly AUC:0.9217
[2024-03-10 17:28:00,632][main.py][line:124][INFO] [Epoch:8/20, Batch:199/320]: loss1:0.0180 loss2:0.1847 loss3:0.3425 | AUC:0.6220 Anomaly AUC:0.8978
[2024-03-10 17:28:15,066][main.py][line:124][INFO] [Epoch:8/20, Batch:209/320]: loss1:0.0514 loss2:0.1647 loss3:0.3429 | AUC:0.6753 Anomaly AUC:0.9100
[2024-03-10 17:28:29,716][main.py][line:124][INFO] [Epoch:8/20, Batch:219/320]: loss1:0.0419 loss2:0.1463 loss3:0.3348 | AUC:0.7390 Anomaly AUC:0.9291
[2024-03-10 17:28:43,996][main.py][line:124][INFO] [Epoch:8/20, Batch:229/320]: loss1:0.0248 loss2:0.1611 loss3:0.3394 | AUC:0.7311 Anomaly AUC:0.9239
[2024-03-10 17:28:58,558][main.py][line:124][INFO] [Epoch:8/20, Batch:239/320]: loss1:0.0225 loss2:0.1313 loss3:0.3334 | AUC:0.7180 Anomaly AUC:0.9254
[2024-03-10 17:29:13,040][main.py][line:124][INFO] [Epoch:8/20, Batch:249/320]: loss1:0.0115 loss2:0.1042 loss3:0.3335 | AUC:0.7139 Anomaly AUC:0.9256
[2024-03-10 17:29:27,309][main.py][line:124][INFO] [Epoch:8/20, Batch:259/320]: loss1:0.0737 loss2:0.1418 loss3:0.3402 | AUC:0.6889 Anomaly AUC:0.9127
[2024-03-10 17:29:41,892][main.py][line:124][INFO] [Epoch:8/20, Batch:269/320]: loss1:0.0226 loss2:0.1337 loss3:0.3338 | AUC:0.6894 Anomaly AUC:0.9145
[2024-03-10 17:29:56,572][main.py][line:124][INFO] [Epoch:8/20, Batch:279/320]: loss1:0.0091 loss2:0.1491 loss3:0.3348 | AUC:0.7037 Anomaly AUC:0.9218
[2024-03-10 17:30:11,156][main.py][line:124][INFO] [Epoch:8/20, Batch:289/320]: loss1:0.0074 loss2:0.1429 loss3:0.3328 | AUC:0.7502 Anomaly AUC:0.9302
[2024-03-10 17:30:25,714][main.py][line:153][INFO] [Epoch:8/20]: lr:0.00010 | loss1:0.1236 loss2:0.2172 loss3:0.3351 | AUC:0.7479 Anomaly AUC:0.9299
[2024-03-10 17:30:41,464][main.py][line:124][INFO] [Epoch:9/20, Batch:9/320]: loss1:0.0118 loss2:0.1526 loss3:0.3333 | AUC:0.7571 Anomaly AUC:0.9308
[2024-03-10 17:30:56,255][main.py][line:124][INFO] [Epoch:9/20, Batch:19/320]: loss1:0.0128 loss2:0.0985 loss3:0.3364 | AUC:0.7418 Anomaly AUC:0.9284
[2024-03-10 17:31:10,808][main.py][line:124][INFO] [Epoch:9/20, Batch:29/320]: loss1:0.0064 loss2:0.0873 loss3:0.3316 | AUC:0.7454 Anomaly AUC:0.9301
[2024-03-10 17:31:25,299][main.py][line:124][INFO] [Epoch:9/20, Batch:39/320]: loss1:0.1188 loss2:0.1770 loss3:0.3392 | AUC:0.7381 Anomaly AUC:0.9273
[2024-03-10 17:31:39,942][main.py][line:124][INFO] [Epoch:9/20, Batch:49/320]: loss1:0.0077 loss2:0.0992 loss3:0.3326 | AUC:0.7344 Anomaly AUC:0.9272
[2024-03-10 17:31:54,499][main.py][line:124][INFO] [Epoch:9/20, Batch:59/320]: loss1:0.0079 loss2:0.0933 loss3:0.3344 | AUC:0.7360 Anomaly AUC:0.9268
[2024-03-10 17:32:09,062][main.py][line:124][INFO] [Epoch:9/20, Batch:69/320]: loss1:0.0108 loss2:0.1384 loss3:0.3354 | AUC:0.7189 Anomaly AUC:0.9235
[2024-03-10 17:32:23,587][main.py][line:124][INFO] [Epoch:9/20, Batch:79/320]: loss1:0.0114 loss2:0.1578 loss3:0.3321 | AUC:0.7295 Anomaly AUC:0.9257
[2024-03-10 17:32:37,991][main.py][line:124][INFO] [Epoch:9/20, Batch:89/320]: loss1:0.0130 loss2:0.1536 loss3:0.3317 | AUC:0.7491 Anomaly AUC:0.9285
[2024-03-10 17:32:52,615][main.py][line:124][INFO] [Epoch:9/20, Batch:99/320]: loss1:0.0886 loss2:0.1975 loss3:0.3308 | AUC:0.7367 Anomaly AUC:0.9265
[2024-03-10 17:33:07,443][main.py][line:124][INFO] [Epoch:9/20, Batch:109/320]: loss1:0.0055 loss2:0.1428 loss3:0.3415 | AUC:0.7348 Anomaly AUC:0.9237
[2024-03-10 17:33:22,010][main.py][line:124][INFO] [Epoch:9/20, Batch:119/320]: loss1:0.0070 loss2:0.1415 loss3:0.3328 | AUC:0.7398 Anomaly AUC:0.9245
[2024-03-10 17:33:36,591][main.py][line:124][INFO] [Epoch:9/20, Batch:129/320]: loss1:0.0073 loss2:0.0919 loss3:0.3355 | AUC:0.7641 Anomaly AUC:0.9297
[2024-03-10 17:33:51,100][main.py][line:124][INFO] [Epoch:9/20, Batch:139/320]: loss1:0.0069 loss2:0.0916 loss3:0.3322 | AUC:0.7576 Anomaly AUC:0.9311
[2024-03-10 17:34:05,752][main.py][line:124][INFO] [Epoch:9/20, Batch:149/320]: loss1:0.0191 loss2:0.1694 loss3:0.3371 | AUC:0.7399 Anomaly AUC:0.9215
[2024-03-10 17:34:20,246][main.py][line:124][INFO] [Epoch:9/20, Batch:159/320]: loss1:0.0182 loss2:0.0988 loss3:0.3328 | AUC:0.7102 Anomaly AUC:0.9256
[2024-03-10 17:34:34,784][main.py][line:124][INFO] [Epoch:9/20, Batch:169/320]: loss1:0.0271 loss2:0.1743 loss3:0.3353 | AUC:0.6440 Anomaly AUC:0.9095
[2024-03-10 17:34:49,069][main.py][line:124][INFO] [Epoch:9/20, Batch:179/320]: loss1:0.0996 loss2:0.2935 loss3:0.3353 | AUC:0.7285 Anomaly AUC:0.9262
[2024-03-10 17:35:03,668][main.py][line:124][INFO] [Epoch:9/20, Batch:189/320]: loss1:0.0040 loss2:0.2090 loss3:0.3334 | AUC:0.7639 Anomaly AUC:0.9277
[2024-03-10 17:35:18,385][main.py][line:124][INFO] [Epoch:9/20, Batch:199/320]: loss1:0.0261 loss2:0.1185 loss3:0.3336 | AUC:0.7400 Anomaly AUC:0.9194
[2024-03-10 17:35:32,769][main.py][line:124][INFO] [Epoch:9/20, Batch:209/320]: loss1:0.0532 loss2:0.1522 loss3:0.3456 | AUC:0.6587 Anomaly AUC:0.8973
[2024-03-10 17:35:47,257][main.py][line:124][INFO] [Epoch:9/20, Batch:219/320]: loss1:0.0314 loss2:0.1870 loss3:0.3424 | AUC:0.7255 Anomaly AUC:0.9136
[2024-03-10 17:36:01,701][main.py][line:124][INFO] [Epoch:9/20, Batch:229/320]: loss1:0.0470 loss2:0.1995 loss3:0.3448 | AUC:0.7934 Anomaly AUC:0.9240
[2024-03-10 17:36:16,221][main.py][line:124][INFO] [Epoch:9/20, Batch:239/320]: loss1:0.0483 loss2:0.1324 loss3:0.3440 | AUC:0.7426 Anomaly AUC:0.8971
[2024-03-10 17:36:30,730][main.py][line:124][INFO] [Epoch:9/20, Batch:249/320]: loss1:0.0316 loss2:0.1704 loss3:0.3404 | AUC:0.7682 Anomaly AUC:0.9121
[2024-03-10 17:36:45,184][main.py][line:124][INFO] [Epoch:9/20, Batch:259/320]: loss1:0.0604 loss2:0.2197 loss3:0.3470 | AUC:0.7992 Anomaly AUC:0.9310
[2024-03-10 17:36:59,472][main.py][line:124][INFO] [Epoch:9/20, Batch:269/320]: loss1:0.0460 loss2:0.2154 loss3:0.3440 | AUC:0.6995 Anomaly AUC:0.9161
[2024-03-10 17:37:13,712][main.py][line:124][INFO] [Epoch:9/20, Batch:279/320]: loss1:0.0194 loss2:0.2096 loss3:0.3375 | AUC:0.7174 Anomaly AUC:0.9253
[2024-03-10 17:37:28,392][main.py][line:124][INFO] [Epoch:9/20, Batch:289/320]: loss1:0.0195 loss2:0.1663 loss3:0.3364 | AUC:0.6923 Anomaly AUC:0.9122
[2024-03-10 17:37:43,132][main.py][line:153][INFO] [Epoch:9/20]: lr:0.00010 | loss1:0.0513 loss2:0.1913 loss3:0.3348 | AUC:0.7055 Anomaly AUC:0.9179
[2024-03-10 17:37:58,092][main.py][line:124][INFO] [Epoch:10/20, Batch:9/320]: loss1:0.0237 loss2:0.1458 loss3:0.3409 | AUC:0.6956 Anomaly AUC:0.9091
[2024-03-10 17:38:12,505][main.py][line:124][INFO] [Epoch:10/20, Batch:19/320]: loss1:0.0121 loss2:0.1249 loss3:0.3394 | AUC:0.7515 Anomaly AUC:0.9268
[2024-03-10 17:38:27,004][main.py][line:124][INFO] [Epoch:10/20, Batch:29/320]: loss1:0.0319 loss2:0.1473 loss3:0.3395 | AUC:0.7594 Anomaly AUC:0.9301
[2024-03-10 17:38:41,425][main.py][line:124][INFO] [Epoch:10/20, Batch:39/320]: loss1:0.0065 loss2:0.1176 loss3:0.3408 | AUC:0.7573 Anomaly AUC:0.9319
[2024-03-10 17:38:55,889][main.py][line:124][INFO] [Epoch:10/20, Batch:49/320]: loss1:0.0131 loss2:0.1559 loss3:0.3367 | AUC:0.7529 Anomaly AUC:0.9329
[2024-03-10 17:39:10,216][main.py][line:124][INFO] [Epoch:10/20, Batch:59/320]: loss1:0.0300 loss2:0.0993 loss3:0.3469 | AUC:0.7243 Anomaly AUC:0.9170
[2024-03-10 17:39:24,638][main.py][line:124][INFO] [Epoch:10/20, Batch:69/320]: loss1:0.0452 loss2:0.1150 loss3:0.3393 | AUC:0.6811 Anomaly AUC:0.9206
[2024-03-10 17:39:39,346][main.py][line:124][INFO] [Epoch:10/20, Batch:79/320]: loss1:0.0699 loss2:0.0945 loss3:0.3347 | AUC:0.6757 Anomaly AUC:0.9139
[2024-03-10 17:39:53,968][main.py][line:124][INFO] [Epoch:10/20, Batch:89/320]: loss1:0.0061 loss2:0.1103 loss3:0.3327 | AUC:0.7136 Anomaly AUC:0.9248
[2024-03-10 17:40:08,472][main.py][line:124][INFO] [Epoch:10/20, Batch:99/320]: loss1:0.0964 loss2:0.1509 loss3:0.3329 | AUC:0.6944 Anomaly AUC:0.9194
[2024-03-10 17:40:23,001][main.py][line:124][INFO] [Epoch:10/20, Batch:109/320]: loss1:0.0248 loss2:0.1144 loss3:0.3420 | AUC:0.6714 Anomaly AUC:0.9150
[2024-03-10 17:40:37,416][main.py][line:124][INFO] [Epoch:10/20, Batch:119/320]: loss1:0.0186 loss2:0.1266 loss3:0.3344 | AUC:0.7356 Anomaly AUC:0.9275
[2024-03-10 17:40:51,799][main.py][line:124][INFO] [Epoch:10/20, Batch:129/320]: loss1:0.0163 loss2:0.1480 loss3:0.3327 | AUC:0.6933 Anomaly AUC:0.9215
[2024-03-10 17:41:06,097][main.py][line:124][INFO] [Epoch:10/20, Batch:139/320]: loss1:0.0621 loss2:0.1474 loss3:0.3381 | AUC:0.6885 Anomaly AUC:0.9137
[2024-03-10 17:41:20,730][main.py][line:124][INFO] [Epoch:10/20, Batch:149/320]: loss1:0.0283 loss2:0.1162 loss3:0.3334 | AUC:0.7134 Anomaly AUC:0.9206
[2024-03-10 17:41:35,474][main.py][line:124][INFO] [Epoch:10/20, Batch:159/320]: loss1:0.0214 loss2:0.1133 loss3:0.3324 | AUC:0.7344 Anomaly AUC:0.9203
[2024-03-10 17:41:49,868][main.py][line:124][INFO] [Epoch:10/20, Batch:169/320]: loss1:0.0230 loss2:0.1062 loss3:0.3344 | AUC:0.7306 Anomaly AUC:0.9237
[2024-03-10 17:42:04,256][main.py][line:124][INFO] [Epoch:10/20, Batch:179/320]: loss1:0.0129 loss2:0.1821 loss3:0.3319 | AUC:0.7185 Anomaly AUC:0.9208
[2024-03-10 17:42:18,741][main.py][line:124][INFO] [Epoch:10/20, Batch:189/320]: loss1:0.0108 loss2:0.1127 loss3:0.3315 | AUC:0.7222 Anomaly AUC:0.9183
[2024-03-10 17:42:33,208][main.py][line:124][INFO] [Epoch:10/20, Batch:199/320]: loss1:0.0079 loss2:0.0786 loss3:0.3308 | AUC:0.7489 Anomaly AUC:0.9240
[2024-03-10 17:42:47,883][main.py][line:124][INFO] [Epoch:10/20, Batch:209/320]: loss1:0.0089 loss2:0.1059 loss3:0.3319 | AUC:0.7445 Anomaly AUC:0.9237
[2024-03-10 17:43:02,502][main.py][line:124][INFO] [Epoch:10/20, Batch:219/320]: loss1:0.0059 loss2:0.1025 loss3:0.3300 | AUC:0.7651 Anomaly AUC:0.9295
[2024-03-10 17:43:17,094][main.py][line:124][INFO] [Epoch:10/20, Batch:229/320]: loss1:0.0068 loss2:0.1596 loss3:0.3319 | AUC:0.7512 Anomaly AUC:0.9284
[2024-03-10 17:43:31,606][main.py][line:124][INFO] [Epoch:10/20, Batch:239/320]: loss1:0.0104 loss2:0.1076 loss3:0.3347 | AUC:0.6445 Anomaly AUC:0.9063
[2024-03-10 17:43:46,208][main.py][line:124][INFO] [Epoch:10/20, Batch:249/320]: loss1:0.0137 loss2:0.1525 loss3:0.3302 | AUC:0.6606 Anomaly AUC:0.9098
[2024-03-10 17:44:00,850][main.py][line:124][INFO] [Epoch:10/20, Batch:259/320]: loss1:0.0108 loss2:0.0802 loss3:0.3323 | AUC:0.6690 Anomaly AUC:0.9102
[2024-03-10 17:44:15,789][main.py][line:124][INFO] [Epoch:10/20, Batch:269/320]: loss1:0.0079 loss2:0.1614 loss3:0.3325 | AUC:0.7275 Anomaly AUC:0.9259
[2024-03-10 17:44:30,615][main.py][line:124][INFO] [Epoch:10/20, Batch:279/320]: loss1:0.0245 loss2:0.0662 loss3:0.3309 | AUC:0.7653 Anomaly AUC:0.9307
[2024-03-10 17:44:45,119][main.py][line:124][INFO] [Epoch:10/20, Batch:289/320]: loss1:0.0034 loss2:0.1012 loss3:0.3320 | AUC:0.7481 Anomaly AUC:0.9275
[2024-03-10 17:44:59,554][main.py][line:153][INFO] [Epoch:10/20]: lr:0.00010 | loss1:0.1036 loss2:0.1358 loss3:0.3304 | AUC:0.7644 Anomaly AUC:0.9296
[2024-03-10 17:45:14,452][main.py][line:124][INFO] [Epoch:11/20, Batch:9/320]: loss1:0.0118 loss2:0.0887 loss3:0.3341 | AUC:0.7487 Anomaly AUC:0.9280
[2024-03-10 17:45:29,137][main.py][line:124][INFO] [Epoch:11/20, Batch:19/320]: loss1:0.0116 loss2:0.1279 loss3:0.3309 | AUC:0.7286 Anomaly AUC:0.9259
[2024-03-10 17:45:44,169][main.py][line:124][INFO] [Epoch:11/20, Batch:29/320]: loss1:0.0073 loss2:0.1102 loss3:0.3322 | AUC:0.6980 Anomaly AUC:0.9208
[2024-03-10 17:45:58,652][main.py][line:124][INFO] [Epoch:11/20, Batch:39/320]: loss1:0.0101 loss2:0.0949 loss3:0.3321 | AUC:0.7043 Anomaly AUC:0.9208
[2024-03-10 17:46:13,061][main.py][line:124][INFO] [Epoch:11/20, Batch:49/320]: loss1:0.0159 loss2:0.1327 loss3:0.3288 | AUC:0.7329 Anomaly AUC:0.9213
[2024-03-10 17:46:27,635][main.py][line:124][INFO] [Epoch:11/20, Batch:59/320]: loss1:0.0150 loss2:0.1408 loss3:0.3342 | AUC:0.7164 Anomaly AUC:0.9179
[2024-03-10 17:46:42,095][main.py][line:124][INFO] [Epoch:11/20, Batch:69/320]: loss1:0.0115 loss2:0.0604 loss3:0.3283 | AUC:0.7527 Anomaly AUC:0.9261
[2024-03-10 17:46:56,526][main.py][line:124][INFO] [Epoch:11/20, Batch:79/320]: loss1:0.0086 loss2:0.0791 loss3:0.3291 | AUC:0.7436 Anomaly AUC:0.9232
[2024-03-10 17:47:10,950][main.py][line:124][INFO] [Epoch:11/20, Batch:89/320]: loss1:0.0308 loss2:0.1336 loss3:0.3351 | AUC:0.7133 Anomaly AUC:0.9093
[2024-03-10 17:47:25,686][main.py][line:124][INFO] [Epoch:11/20, Batch:99/320]: loss1:0.0152 loss2:0.1597 loss3:0.3327 | AUC:0.7641 Anomaly AUC:0.9213
[2024-03-10 17:47:40,164][main.py][line:124][INFO] [Epoch:11/20, Batch:109/320]: loss1:0.0137 loss2:0.1165 loss3:0.3301 | AUC:0.7688 Anomaly AUC:0.9248
[2024-03-10 17:47:54,580][main.py][line:124][INFO] [Epoch:11/20, Batch:119/320]: loss1:0.0398 loss2:0.1217 loss3:0.3332 | AUC:0.7775 Anomaly AUC:0.9263
[2024-03-10 17:48:09,012][main.py][line:124][INFO] [Epoch:11/20, Batch:129/320]: loss1:0.0142 loss2:0.0880 loss3:0.3318 | AUC:0.7803 Anomaly AUC:0.9270
[2024-03-10 17:48:23,321][main.py][line:124][INFO] [Epoch:11/20, Batch:139/320]: loss1:0.0087 loss2:0.1073 loss3:0.3298 | AUC:0.7649 Anomaly AUC:0.9242
[2024-03-10 17:48:37,713][main.py][line:124][INFO] [Epoch:11/20, Batch:149/320]: loss1:0.0081 loss2:0.1077 loss3:0.3329 | AUC:0.7573 Anomaly AUC:0.9221
[2024-03-10 17:48:52,264][main.py][line:124][INFO] [Epoch:11/20, Batch:159/320]: loss1:0.0051 loss2:0.1373 loss3:0.3335 | AUC:0.6866 Anomaly AUC:0.9002
[2024-03-10 17:49:06,727][main.py][line:124][INFO] [Epoch:11/20, Batch:169/320]: loss1:0.1002 loss2:0.1642 loss3:0.3314 | AUC:0.7615 Anomaly AUC:0.9141
[2024-03-10 17:49:21,477][main.py][line:124][INFO] [Epoch:11/20, Batch:179/320]: loss1:0.0419 loss2:0.1563 loss3:0.3319 | AUC:0.7948 Anomaly AUC:0.9301
[2024-03-10 17:49:36,107][main.py][line:124][INFO] [Epoch:11/20, Batch:189/320]: loss1:0.0667 loss2:0.1668 loss3:0.3386 | AUC:0.8119 Anomaly AUC:0.9396
[2024-03-10 17:49:50,602][main.py][line:124][INFO] [Epoch:11/20, Batch:199/320]: loss1:0.0871 loss2:0.2047 loss3:0.3566 | AUC:0.6597 Anomaly AUC:0.8701
[2024-03-10 17:50:05,144][main.py][line:124][INFO] [Epoch:11/20, Batch:209/320]: loss1:0.0447 loss2:0.1573 loss3:0.3351 | AUC:0.7679 Anomaly AUC:0.9229
[2024-03-10 17:50:19,621][main.py][line:124][INFO] [Epoch:11/20, Batch:219/320]: loss1:0.0414 loss2:0.1546 loss3:0.3405 | AUC:0.7048 Anomaly AUC:0.9145
[2024-03-10 17:50:34,176][main.py][line:124][INFO] [Epoch:11/20, Batch:229/320]: loss1:0.1383 loss2:0.1700 loss3:0.3348 | AUC:0.7328 Anomaly AUC:0.9231
[2024-03-10 17:50:48,607][main.py][line:124][INFO] [Epoch:11/20, Batch:239/320]: loss1:0.0190 loss2:0.1423 loss3:0.3397 | AUC:0.7107 Anomaly AUC:0.9168
[2024-03-10 17:51:02,940][main.py][line:124][INFO] [Epoch:11/20, Batch:249/320]: loss1:0.0139 loss2:0.1210 loss3:0.3321 | AUC:0.7156 Anomaly AUC:0.9238
[2024-03-10 17:51:17,411][main.py][line:124][INFO] [Epoch:11/20, Batch:259/320]: loss1:0.0199 loss2:0.1018 loss3:0.3365 | AUC:0.6657 Anomaly AUC:0.9101
[2024-03-10 17:51:31,769][main.py][line:124][INFO] [Epoch:11/20, Batch:269/320]: loss1:0.0348 loss2:0.1001 loss3:0.3282 | AUC:0.7570 Anomaly AUC:0.9331
[2024-03-10 17:51:46,474][main.py][line:124][INFO] [Epoch:11/20, Batch:279/320]: loss1:0.0165 loss2:0.0743 loss3:0.3309 | AUC:0.7724 Anomaly AUC:0.9350
[2024-03-10 17:52:00,719][main.py][line:124][INFO] [Epoch:11/20, Batch:289/320]: loss1:0.0304 loss2:0.0914 loss3:0.3372 | AUC:0.6972 Anomaly AUC:0.9129
[2024-03-10 17:52:15,236][main.py][line:153][INFO] [Epoch:11/20]: lr:0.00010 | loss1:0.0770 loss2:0.1107 loss3:0.3453 | AUC:0.6720 Anomaly AUC:0.8933
[2024-03-10 17:52:30,708][main.py][line:124][INFO] [Epoch:12/20, Batch:9/320]: loss1:0.0620 loss2:0.0877 loss3:0.3389 | AUC:0.7070 Anomaly AUC:0.9072
[2024-03-10 17:52:45,175][main.py][line:124][INFO] [Epoch:12/20, Batch:19/320]: loss1:0.0317 loss2:0.1068 loss3:0.3402 | AUC:0.8185 Anomaly AUC:0.9354
[2024-03-10 17:52:59,707][main.py][line:124][INFO] [Epoch:12/20, Batch:29/320]: loss1:0.0862 loss2:0.1019 loss3:0.3458 | AUC:0.6626 Anomaly AUC:0.8761
[2024-03-10 17:53:14,257][main.py][line:124][INFO] [Epoch:12/20, Batch:39/320]: loss1:0.0709 loss2:0.1201 loss3:0.3446 | AUC:0.7061 Anomaly AUC:0.8876
[2024-03-10 17:53:28,787][main.py][line:124][INFO] [Epoch:12/20, Batch:49/320]: loss1:0.0373 loss2:0.1190 loss3:0.3455 | AUC:0.7732 Anomaly AUC:0.9171
[2024-03-10 17:53:43,174][main.py][line:124][INFO] [Epoch:12/20, Batch:59/320]: loss1:0.0233 loss2:0.1230 loss3:0.3390 | AUC:0.7554 Anomaly AUC:0.9142
[2024-03-10 17:53:57,516][main.py][line:124][INFO] [Epoch:12/20, Batch:69/320]: loss1:0.0121 loss2:0.1207 loss3:0.3415 | AUC:0.7802 Anomaly AUC:0.9322
[2024-03-10 17:54:11,931][main.py][line:124][INFO] [Epoch:12/20, Batch:79/320]: loss1:0.0433 loss2:0.1076 loss3:0.3365 | AUC:0.7970 Anomaly AUC:0.9359
[2024-03-10 17:54:26,384][main.py][line:124][INFO] [Epoch:12/20, Batch:89/320]: loss1:0.0145 loss2:0.0794 loss3:0.3349 | AUC:0.7091 Anomaly AUC:0.9147
[2024-03-10 17:54:40,701][main.py][line:124][INFO] [Epoch:12/20, Batch:99/320]: loss1:0.0983 loss2:0.2021 loss3:0.3345 | AUC:0.7209 Anomaly AUC:0.9207
[2024-03-10 17:54:55,314][main.py][line:124][INFO] [Epoch:12/20, Batch:109/320]: loss1:0.0261 loss2:0.1315 loss3:0.3357 | AUC:0.7289 Anomaly AUC:0.9215
[2024-03-10 17:55:09,997][main.py][line:124][INFO] [Epoch:12/20, Batch:119/320]: loss1:0.0191 loss2:0.0915 loss3:0.3307 | AUC:0.7211 Anomaly AUC:0.9216
[2024-03-10 17:55:24,470][main.py][line:124][INFO] [Epoch:12/20, Batch:129/320]: loss1:0.0096 loss2:0.1064 loss3:0.3326 | AUC:0.7074 Anomaly AUC:0.9183
[2024-03-10 17:55:39,006][main.py][line:124][INFO] [Epoch:12/20, Batch:139/320]: loss1:0.0710 loss2:0.1336 loss3:0.3468 | AUC:0.6000 Anomaly AUC:0.8438
[2024-03-10 17:55:53,413][main.py][line:124][INFO] [Epoch:12/20, Batch:149/320]: loss1:0.0765 loss2:0.1392 loss3:0.3521 | AUC:0.4362 Anomaly AUC:0.7742
[2024-03-10 17:56:07,766][main.py][line:124][INFO] [Epoch:12/20, Batch:159/320]: loss1:0.0806 loss2:0.1222 loss3:0.3574 | AUC:0.5410 Anomaly AUC:0.8160
[2024-03-10 17:56:22,196][main.py][line:124][INFO] [Epoch:12/20, Batch:169/320]: loss1:0.0478 loss2:0.1700 loss3:0.3499 | AUC:0.5947 Anomaly AUC:0.8576
[2024-03-10 17:56:36,698][main.py][line:124][INFO] [Epoch:12/20, Batch:179/320]: loss1:0.0131 loss2:0.1187 loss3:0.3439 | AUC:0.6775 Anomaly AUC:0.8985
[2024-03-10 17:56:51,087][main.py][line:124][INFO] [Epoch:12/20, Batch:189/320]: loss1:0.0147 loss2:0.1236 loss3:0.3386 | AUC:0.6687 Anomaly AUC:0.9054
[2024-03-10 17:57:05,628][main.py][line:124][INFO] [Epoch:12/20, Batch:199/320]: loss1:0.0186 loss2:0.1242 loss3:0.3346 | AUC:0.6695 Anomaly AUC:0.9052
[2024-03-10 17:57:20,406][main.py][line:124][INFO] [Epoch:12/20, Batch:209/320]: loss1:0.0157 loss2:0.1234 loss3:0.3346 | AUC:0.6798 Anomaly AUC:0.9103
[2024-03-10 17:57:35,008][main.py][line:124][INFO] [Epoch:12/20, Batch:219/320]: loss1:0.0147 loss2:0.1249 loss3:0.3386 | AUC:0.6708 Anomaly AUC:0.9101
[2024-03-10 17:57:50,142][main.py][line:124][INFO] [Epoch:12/20, Batch:229/320]: loss1:0.0190 loss2:0.1076 loss3:0.3340 | AUC:0.6953 Anomaly AUC:0.9222
[2024-03-10 17:58:05,444][main.py][line:124][INFO] [Epoch:12/20, Batch:239/320]: loss1:0.0096 loss2:0.1659 loss3:0.3346 | AUC:0.7023 Anomaly AUC:0.9163
[2024-03-10 17:58:20,348][main.py][line:124][INFO] [Epoch:12/20, Batch:249/320]: loss1:0.0156 loss2:0.1226 loss3:0.3374 | AUC:0.7266 Anomaly AUC:0.9217
[2024-03-10 17:58:35,302][main.py][line:124][INFO] [Epoch:12/20, Batch:259/320]: loss1:0.0309 loss2:0.0683 loss3:0.3331 | AUC:0.7334 Anomaly AUC:0.9280
[2024-03-10 17:58:50,074][main.py][line:124][INFO] [Epoch:12/20, Batch:269/320]: loss1:0.0036 loss2:0.0526 loss3:0.3326 | AUC:0.7195 Anomaly AUC:0.9197
[2024-03-10 17:59:04,644][main.py][line:124][INFO] [Epoch:12/20, Batch:279/320]: loss1:0.0106 loss2:0.0784 loss3:0.3308 | AUC:0.7268 Anomaly AUC:0.9256
[2024-03-10 17:59:19,599][main.py][line:124][INFO] [Epoch:12/20, Batch:289/320]: loss1:0.0078 loss2:0.0981 loss3:0.3302 | AUC:0.7182 Anomaly AUC:0.9251
[2024-03-10 17:59:34,630][main.py][line:153][INFO] [Epoch:12/20]: lr:0.00010 | loss1:0.0049 loss2:0.0557 loss3:0.3316 | AUC:0.7235 Anomaly AUC:0.9264
[2024-03-10 17:59:50,569][main.py][line:124][INFO] [Epoch:13/20, Batch:9/320]: loss1:0.0092 loss2:0.0687 loss3:0.3320 | AUC:0.7449 Anomaly AUC:0.9305
[2024-03-10 18:00:05,578][main.py][line:124][INFO] [Epoch:13/20, Batch:19/320]: loss1:0.0213 loss2:0.0868 loss3:0.3338 | AUC:0.7626 Anomaly AUC:0.9358
[2024-03-10 18:00:20,608][main.py][line:124][INFO] [Epoch:13/20, Batch:29/320]: loss1:0.0053 loss2:0.0606 loss3:0.3285 | AUC:0.7348 Anomaly AUC:0.9320
[2024-03-10 18:00:35,295][main.py][line:124][INFO] [Epoch:13/20, Batch:39/320]: loss1:0.0150 loss2:0.0945 loss3:0.3364 | AUC:0.6268 Anomaly AUC:0.8890
[2024-03-10 18:00:50,370][main.py][line:124][INFO] [Epoch:13/20, Batch:49/320]: loss1:0.0202 loss2:0.0925 loss3:0.3362 | AUC:0.6588 Anomaly AUC:0.9071
[2024-03-10 18:01:04,713][main.py][line:124][INFO] [Epoch:13/20, Batch:59/320]: loss1:0.0257 loss2:0.0830 loss3:0.3294 | AUC:0.7087 Anomaly AUC:0.9264
[2024-03-10 18:01:19,300][main.py][line:124][INFO] [Epoch:13/20, Batch:69/320]: loss1:0.0185 loss2:0.0949 loss3:0.3320 | AUC:0.6421 Anomaly AUC:0.8980
[2024-03-10 18:01:34,156][main.py][line:124][INFO] [Epoch:13/20, Batch:79/320]: loss1:0.0126 loss2:0.0799 loss3:0.3269 | AUC:0.7274 Anomaly AUC:0.9272
[2024-03-10 18:01:48,586][main.py][line:124][INFO] [Epoch:13/20, Batch:89/320]: loss1:0.0070 loss2:0.0893 loss3:0.3280 | AUC:0.7138 Anomaly AUC:0.9193
[2024-03-10 18:02:03,364][main.py][line:124][INFO] [Epoch:13/20, Batch:99/320]: loss1:0.0087 loss2:0.1164 loss3:0.3313 | AUC:0.6982 Anomaly AUC:0.9214
[2024-03-10 18:02:17,990][main.py][line:124][INFO] [Epoch:13/20, Batch:109/320]: loss1:0.0094 loss2:0.0796 loss3:0.3288 | AUC:0.7101 Anomaly AUC:0.9231
[2024-03-10 18:02:32,828][main.py][line:124][INFO] [Epoch:13/20, Batch:119/320]: loss1:0.0085 loss2:0.0562 loss3:0.3290 | AUC:0.7280 Anomaly AUC:0.9271
[2024-03-10 18:02:47,784][main.py][line:124][INFO] [Epoch:13/20, Batch:129/320]: loss1:0.0076 loss2:0.0528 loss3:0.3291 | AUC:0.7379 Anomaly AUC:0.9307
[2024-03-10 18:03:02,279][main.py][line:124][INFO] [Epoch:13/20, Batch:139/320]: loss1:0.0129 loss2:0.0765 loss3:0.3299 | AUC:0.6505 Anomaly AUC:0.9120
[2024-03-10 18:03:16,969][main.py][line:124][INFO] [Epoch:13/20, Batch:149/320]: loss1:0.0164 loss2:0.0949 loss3:0.3310 | AUC:0.6998 Anomaly AUC:0.9166
[2024-03-10 18:03:31,966][main.py][line:124][INFO] [Epoch:13/20, Batch:159/320]: loss1:0.0077 loss2:0.0750 loss3:0.3299 | AUC:0.7383 Anomaly AUC:0.9270
[2024-03-10 18:03:46,376][main.py][line:124][INFO] [Epoch:13/20, Batch:169/320]: loss1:0.0090 loss2:0.0788 loss3:0.3257 | AUC:0.7569 Anomaly AUC:0.9305
[2024-03-10 18:04:00,652][main.py][line:124][INFO] [Epoch:13/20, Batch:179/320]: loss1:0.1414 loss2:0.1745 loss3:0.3312 | AUC:0.7722 Anomaly AUC:0.9311
[2024-03-10 18:04:15,563][main.py][line:124][INFO] [Epoch:13/20, Batch:189/320]: loss1:0.0300 loss2:0.0677 loss3:0.3299 | AUC:0.7684 Anomaly AUC:0.9289
[2024-03-10 18:04:30,495][main.py][line:124][INFO] [Epoch:13/20, Batch:199/320]: loss1:0.0137 loss2:0.0790 loss3:0.3258 | AUC:0.7804 Anomaly AUC:0.9343
[2024-03-10 18:04:45,214][main.py][line:124][INFO] [Epoch:13/20, Batch:209/320]: loss1:0.0128 loss2:0.1016 loss3:0.3297 | AUC:0.7729 Anomaly AUC:0.9339
[2024-03-10 18:05:00,245][main.py][line:124][INFO] [Epoch:13/20, Batch:219/320]: loss1:0.0106 loss2:0.0526 loss3:0.3302 | AUC:0.7944 Anomaly AUC:0.9378
[2024-03-10 18:05:14,967][main.py][line:124][INFO] [Epoch:13/20, Batch:229/320]: loss1:0.0129 loss2:0.1181 loss3:0.3309 | AUC:0.7490 Anomaly AUC:0.9280
[2024-03-10 18:05:29,813][main.py][line:124][INFO] [Epoch:13/20, Batch:239/320]: loss1:0.0165 loss2:0.0693 loss3:0.3317 | AUC:0.7592 Anomaly AUC:0.9272
[2024-03-10 18:05:44,759][main.py][line:124][INFO] [Epoch:13/20, Batch:249/320]: loss1:0.0121 loss2:0.0699 loss3:0.3305 | AUC:0.7863 Anomaly AUC:0.9352
[2024-03-10 18:05:59,550][main.py][line:124][INFO] [Epoch:13/20, Batch:259/320]: loss1:0.0114 loss2:0.0717 loss3:0.3273 | AUC:0.7935 Anomaly AUC:0.9363
[2024-03-10 18:06:14,394][main.py][line:124][INFO] [Epoch:13/20, Batch:269/320]: loss1:0.0279 loss2:0.0965 loss3:0.3345 | AUC:0.6998 Anomaly AUC:0.9061
[2024-03-10 18:06:29,446][main.py][line:124][INFO] [Epoch:13/20, Batch:279/320]: loss1:0.0367 loss2:0.0641 loss3:0.3341 | AUC:0.6966 Anomaly AUC:0.9091
[2024-03-10 18:06:43,933][main.py][line:124][INFO] [Epoch:13/20, Batch:289/320]: loss1:0.0081 loss2:0.0917 loss3:0.3324 | AUC:0.7186 Anomaly AUC:0.9114
[2024-03-10 18:06:59,237][main.py][line:153][INFO] [Epoch:13/20]: lr:0.00010 | loss1:0.0080 loss2:0.1279 loss3:0.3301 | AUC:0.7341 Anomaly AUC:0.9116
[2024-03-10 18:07:14,101][main.py][line:124][INFO] [Epoch:14/20, Batch:9/320]: loss1:0.0675 loss2:0.0890 loss3:0.3428 | AUC:0.6408 Anomaly AUC:0.8840
[2024-03-10 18:07:28,516][main.py][line:124][INFO] [Epoch:14/20, Batch:19/320]: loss1:0.0462 loss2:0.1806 loss3:0.3472 | AUC:0.6760 Anomaly AUC:0.9105
[2024-03-10 18:07:43,728][main.py][line:124][INFO] [Epoch:14/20, Batch:29/320]: loss1:0.0245 loss2:0.0973 loss3:0.3385 | AUC:0.7346 Anomaly AUC:0.9140
[2024-03-10 18:07:59,023][main.py][line:124][INFO] [Epoch:14/20, Batch:39/320]: loss1:0.0070 loss2:0.0920 loss3:0.3347 | AUC:0.7542 Anomaly AUC:0.9241
[2024-03-10 18:08:13,689][main.py][line:124][INFO] [Epoch:14/20, Batch:49/320]: loss1:0.0166 loss2:0.0725 loss3:0.3330 | AUC:0.7489 Anomaly AUC:0.9196
[2024-03-10 18:08:28,968][main.py][line:124][INFO] [Epoch:14/20, Batch:59/320]: loss1:0.0351 loss2:0.1245 loss3:0.3322 | AUC:0.7584 Anomaly AUC:0.9224
[2024-03-10 18:08:44,156][main.py][line:124][INFO] [Epoch:14/20, Batch:69/320]: loss1:0.0133 loss2:0.1728 loss3:0.3331 | AUC:0.7635 Anomaly AUC:0.9234
[2024-03-10 18:08:59,416][main.py][line:124][INFO] [Epoch:14/20, Batch:79/320]: loss1:0.0101 loss2:0.0665 loss3:0.3329 | AUC:0.7680 Anomaly AUC:0.9271
[2024-03-10 18:09:14,634][main.py][line:124][INFO] [Epoch:14/20, Batch:89/320]: loss1:0.0399 loss2:0.1284 loss3:0.3366 | AUC:0.7122 Anomaly AUC:0.9005
[2024-03-10 18:09:29,765][main.py][line:124][INFO] [Epoch:14/20, Batch:99/320]: loss1:0.0296 loss2:0.0620 loss3:0.3397 | AUC:0.7043 Anomaly AUC:0.8963
[2024-03-10 18:09:44,910][main.py][line:124][INFO] [Epoch:14/20, Batch:109/320]: loss1:0.0097 loss2:0.0618 loss3:0.3351 | AUC:0.7570 Anomaly AUC:0.9204
[2024-03-10 18:10:00,028][main.py][line:124][INFO] [Epoch:14/20, Batch:119/320]: loss1:0.0390 loss2:0.1087 loss3:0.3314 | AUC:0.7656 Anomaly AUC:0.9253
[2024-03-10 18:10:15,036][main.py][line:124][INFO] [Epoch:14/20, Batch:129/320]: loss1:0.0047 loss2:0.0944 loss3:0.3322 | AUC:0.7842 Anomaly AUC:0.9311
[2024-03-10 18:10:30,229][main.py][line:124][INFO] [Epoch:14/20, Batch:139/320]: loss1:0.0132 loss2:0.1053 loss3:0.3320 | AUC:0.7743 Anomaly AUC:0.9260
[2024-03-10 18:10:45,433][main.py][line:124][INFO] [Epoch:14/20, Batch:149/320]: loss1:0.0114 loss2:0.0637 loss3:0.3302 | AUC:0.7784 Anomaly AUC:0.9286
[2024-03-10 18:11:00,604][main.py][line:124][INFO] [Epoch:14/20, Batch:159/320]: loss1:0.0051 loss2:0.0470 loss3:0.3280 | AUC:0.7739 Anomaly AUC:0.9275
[2024-03-10 18:11:15,786][main.py][line:124][INFO] [Epoch:14/20, Batch:169/320]: loss1:0.0077 loss2:0.0939 loss3:0.3331 | AUC:0.7664 Anomaly AUC:0.9267
[2024-03-10 18:11:30,988][main.py][line:124][INFO] [Epoch:14/20, Batch:179/320]: loss1:0.0106 loss2:0.0435 loss3:0.3321 | AUC:0.7775 Anomaly AUC:0.9306
[2024-03-10 18:11:46,085][main.py][line:124][INFO] [Epoch:14/20, Batch:189/320]: loss1:0.0145 loss2:0.0599 loss3:0.3308 | AUC:0.7652 Anomaly AUC:0.9301
[2024-03-10 18:12:01,375][main.py][line:124][INFO] [Epoch:14/20, Batch:199/320]: loss1:0.0095 loss2:0.0442 loss3:0.3292 | AUC:0.7808 Anomaly AUC:0.9331
[2024-03-10 18:12:16,661][main.py][line:124][INFO] [Epoch:14/20, Batch:209/320]: loss1:0.0393 loss2:0.0862 loss3:0.3302 | AUC:0.7743 Anomaly AUC:0.9311
[2024-03-10 18:12:31,938][main.py][line:124][INFO] [Epoch:14/20, Batch:219/320]: loss1:0.0086 loss2:0.0711 loss3:0.3298 | AUC:0.7764 Anomaly AUC:0.9315
[2024-03-10 18:12:46,953][main.py][line:124][INFO] [Epoch:14/20, Batch:229/320]: loss1:0.0100 loss2:0.0384 loss3:0.3294 | AUC:0.7757 Anomaly AUC:0.9314
[2024-03-10 18:13:02,087][main.py][line:124][INFO] [Epoch:14/20, Batch:239/320]: loss1:0.0304 loss2:0.0763 loss3:0.3320 | AUC:0.7369 Anomaly AUC:0.9207
[2024-03-10 18:13:17,039][main.py][line:124][INFO] [Epoch:14/20, Batch:249/320]: loss1:0.0692 loss2:0.0971 loss3:0.3523 | AUC:0.6130 Anomaly AUC:0.8717
[2024-03-10 18:13:32,868][main.py][line:124][INFO] [Epoch:14/20, Batch:259/320]: loss1:0.0192 loss2:0.0982 loss3:0.3386 | AUC:0.6944 Anomaly AUC:0.9046
[2024-03-10 18:13:48,097][main.py][line:124][INFO] [Epoch:14/20, Batch:269/320]: loss1:0.0166 loss2:0.0708 loss3:0.3384 | AUC:0.7379 Anomaly AUC:0.9218
[2024-03-10 18:14:03,187][main.py][line:124][INFO] [Epoch:14/20, Batch:279/320]: loss1:0.0163 loss2:0.1145 loss3:0.3384 | AUC:0.7190 Anomaly AUC:0.9193
[2024-03-10 18:14:17,931][main.py][line:124][INFO] [Epoch:14/20, Batch:289/320]: loss1:0.0371 loss2:0.0879 loss3:0.3423 | AUC:0.6975 Anomaly AUC:0.9106
[2024-03-10 18:14:32,798][main.py][line:153][INFO] [Epoch:14/20]: lr:0.00010 | loss1:0.0194 loss2:0.0845 loss3:0.3389 | AUC:0.7329 Anomaly AUC:0.9201
[2024-03-10 18:14:48,229][main.py][line:124][INFO] [Epoch:15/20, Batch:9/320]: loss1:0.0216 loss2:0.0629 loss3:0.3350 | AUC:0.7469 Anomaly AUC:0.9244
[2024-03-10 18:15:02,899][main.py][line:124][INFO] [Epoch:15/20, Batch:19/320]: loss1:0.0054 loss2:0.0513 loss3:0.3347 | AUC:0.7753 Anomaly AUC:0.9307
[2024-03-10 18:15:18,036][main.py][line:124][INFO] [Epoch:15/20, Batch:29/320]: loss1:0.0066 loss2:0.0584 loss3:0.3336 | AUC:0.7727 Anomaly AUC:0.9311
[2024-03-10 18:15:32,934][main.py][line:124][INFO] [Epoch:15/20, Batch:39/320]: loss1:0.0053 loss2:0.0574 loss3:0.3344 | AUC:0.7816 Anomaly AUC:0.9334
[2024-03-10 18:15:48,100][main.py][line:124][INFO] [Epoch:15/20, Batch:49/320]: loss1:0.0059 loss2:0.0456 loss3:0.3301 | AUC:0.7771 Anomaly AUC:0.9314
[2024-03-10 18:16:03,144][main.py][line:124][INFO] [Epoch:15/20, Batch:59/320]: loss1:0.0103 loss2:0.0495 loss3:0.3308 | AUC:0.7675 Anomaly AUC:0.9303
[2024-03-10 18:16:18,071][main.py][line:124][INFO] [Epoch:15/20, Batch:69/320]: loss1:0.0056 loss2:0.0987 loss3:0.3310 | AUC:0.7467 Anomaly AUC:0.9267
[2024-03-10 18:16:32,945][main.py][line:124][INFO] [Epoch:15/20, Batch:79/320]: loss1:0.0080 loss2:0.0442 loss3:0.3301 | AUC:0.7587 Anomaly AUC:0.9268
[2024-03-10 18:16:48,066][main.py][line:124][INFO] [Epoch:15/20, Batch:89/320]: loss1:0.0614 loss2:0.0953 loss3:0.3302 | AUC:0.7726 Anomaly AUC:0.9318
[2024-03-10 18:17:03,170][main.py][line:124][INFO] [Epoch:15/20, Batch:99/320]: loss1:0.0122 loss2:0.0733 loss3:0.3343 | AUC:0.7652 Anomaly AUC:0.9320
[2024-03-10 18:17:17,740][main.py][line:124][INFO] [Epoch:15/20, Batch:109/320]: loss1:0.0134 loss2:0.0914 loss3:0.3285 | AUC:0.7786 Anomaly AUC:0.9307
[2024-03-10 18:17:32,992][main.py][line:124][INFO] [Epoch:15/20, Batch:119/320]: loss1:0.0066 loss2:0.0500 loss3:0.3281 | AUC:0.7853 Anomaly AUC:0.9343
[2024-03-10 18:17:48,134][main.py][line:124][INFO] [Epoch:15/20, Batch:129/320]: loss1:0.0064 loss2:0.0549 loss3:0.3286 | AUC:0.7577 Anomaly AUC:0.9303
[2024-03-10 18:18:03,166][main.py][line:124][INFO] [Epoch:15/20, Batch:139/320]: loss1:0.0050 loss2:0.0745 loss3:0.3285 | AUC:0.7736 Anomaly AUC:0.9348
[2024-03-10 18:18:18,249][main.py][line:124][INFO] [Epoch:15/20, Batch:149/320]: loss1:0.0063 loss2:0.0418 loss3:0.3265 | AUC:0.7715 Anomaly AUC:0.9337
[2024-03-10 18:18:33,111][main.py][line:124][INFO] [Epoch:15/20, Batch:159/320]: loss1:0.0062 loss2:0.0454 loss3:0.3306 | AUC:0.7464 Anomaly AUC:0.9285
[2024-03-10 18:18:48,550][main.py][line:124][INFO] [Epoch:15/20, Batch:169/320]: loss1:0.0075 loss2:0.0753 loss3:0.3321 | AUC:0.7598 Anomaly AUC:0.9306
[2024-03-10 18:19:03,603][main.py][line:124][INFO] [Epoch:15/20, Batch:179/320]: loss1:0.0051 loss2:0.0615 loss3:0.3265 | AUC:0.7532 Anomaly AUC:0.9268
[2024-03-10 18:19:18,481][main.py][line:124][INFO] [Epoch:15/20, Batch:189/320]: loss1:0.1020 loss2:0.1358 loss3:0.3318 | AUC:0.7533 Anomaly AUC:0.9206
[2024-03-10 18:19:33,582][main.py][line:124][INFO] [Epoch:15/20, Batch:199/320]: loss1:0.0586 loss2:0.0844 loss3:0.3391 | AUC:0.7148 Anomaly AUC:0.8994
[2024-03-10 18:19:48,854][main.py][line:124][INFO] [Epoch:15/20, Batch:209/320]: loss1:0.0227 loss2:0.1294 loss3:0.3309 | AUC:0.7634 Anomaly AUC:0.9286
[2024-03-10 18:20:03,873][main.py][line:124][INFO] [Epoch:15/20, Batch:219/320]: loss1:0.0067 loss2:0.1352 loss3:0.3322 | AUC:0.7807 Anomaly AUC:0.9318
[2024-03-10 18:20:18,847][main.py][line:124][INFO] [Epoch:15/20, Batch:229/320]: loss1:0.0284 loss2:0.0761 loss3:0.3324 | AUC:0.7791 Anomaly AUC:0.9323
[2024-03-10 18:20:33,791][main.py][line:124][INFO] [Epoch:15/20, Batch:239/320]: loss1:0.0439 loss2:0.0899 loss3:0.3353 | AUC:0.7035 Anomaly AUC:0.9035
[2024-03-10 18:20:48,703][main.py][line:124][INFO] [Epoch:15/20, Batch:249/320]: loss1:0.0398 loss2:0.0474 loss3:0.3336 | AUC:0.7016 Anomaly AUC:0.9112
[2024-03-10 18:21:03,756][main.py][line:124][INFO] [Epoch:15/20, Batch:259/320]: loss1:0.0933 loss2:0.2220 loss3:0.3523 | AUC:0.6474 Anomaly AUC:0.8719
[2024-03-10 18:21:18,461][main.py][line:124][INFO] [Epoch:15/20, Batch:269/320]: loss1:0.0525 loss2:0.0941 loss3:0.3413 | AUC:0.5667 Anomaly AUC:0.8750
[2024-03-10 18:21:33,487][main.py][line:124][INFO] [Epoch:15/20, Batch:279/320]: loss1:0.0121 loss2:0.0976 loss3:0.3361 | AUC:0.6575 Anomaly AUC:0.9085
[2024-03-10 18:21:48,255][main.py][line:124][INFO] [Epoch:15/20, Batch:289/320]: loss1:0.0382 loss2:0.1711 loss3:0.3393 | AUC:0.5616 Anomaly AUC:0.8673
[2024-03-10 18:22:03,346][main.py][line:153][INFO] [Epoch:15/20]: lr:0.00010 | loss1:0.0244 loss2:0.0900 loss3:0.3365 | AUC:0.6388 Anomaly AUC:0.9055
[2024-03-10 18:22:18,340][main.py][line:124][INFO] [Epoch:16/20, Batch:9/320]: loss1:0.0276 loss2:0.0598 loss3:0.3404 | AUC:0.6571 Anomaly AUC:0.8959
[2024-03-10 18:22:33,002][main.py][line:124][INFO] [Epoch:16/20, Batch:19/320]: loss1:0.0146 loss2:0.0903 loss3:0.3342 | AUC:0.7380 Anomaly AUC:0.9290
[2024-03-10 18:22:47,513][main.py][line:124][INFO] [Epoch:16/20, Batch:29/320]: loss1:0.0102 loss2:0.0849 loss3:0.3364 | AUC:0.6758 Anomaly AUC:0.9164
[2024-03-10 18:23:02,217][main.py][line:124][INFO] [Epoch:16/20, Batch:39/320]: loss1:0.0064 loss2:0.0982 loss3:0.3336 | AUC:0.6842 Anomaly AUC:0.9197
[2024-03-10 18:23:16,737][main.py][line:124][INFO] [Epoch:16/20, Batch:49/320]: loss1:0.0068 loss2:0.0463 loss3:0.3350 | AUC:0.7204 Anomaly AUC:0.9301
[2024-03-10 18:23:31,366][main.py][line:124][INFO] [Epoch:16/20, Batch:59/320]: loss1:0.0095 loss2:0.0616 loss3:0.3324 | AUC:0.6719 Anomaly AUC:0.9123
[2024-03-10 18:23:46,129][main.py][line:124][INFO] [Epoch:16/20, Batch:69/320]: loss1:0.0204 loss2:0.1532 loss3:0.3383 | AUC:0.6882 Anomaly AUC:0.9198
[2024-03-10 18:24:00,774][main.py][line:124][INFO] [Epoch:16/20, Batch:79/320]: loss1:0.0096 loss2:0.0732 loss3:0.3318 | AUC:0.7403 Anomaly AUC:0.9334
[2024-03-10 18:24:15,382][main.py][line:124][INFO] [Epoch:16/20, Batch:89/320]: loss1:0.0087 loss2:0.0450 loss3:0.3365 | AUC:0.7010 Anomaly AUC:0.9211
[2024-03-10 18:24:30,306][main.py][line:124][INFO] [Epoch:16/20, Batch:99/320]: loss1:0.0085 loss2:0.0545 loss3:0.3296 | AUC:0.7324 Anomaly AUC:0.9296
[2024-03-10 18:24:45,175][main.py][line:124][INFO] [Epoch:16/20, Batch:109/320]: loss1:0.0045 loss2:0.0847 loss3:0.3307 | AUC:0.7560 Anomaly AUC:0.9339
[2024-03-10 18:25:00,120][main.py][line:124][INFO] [Epoch:16/20, Batch:119/320]: loss1:0.0110 loss2:0.0533 loss3:0.3316 | AUC:0.7436 Anomaly AUC:0.9320
[2024-03-10 18:25:14,643][main.py][line:124][INFO] [Epoch:16/20, Batch:129/320]: loss1:0.0096 loss2:0.0445 loss3:0.3329 | AUC:0.7372 Anomaly AUC:0.9270
[2024-03-10 18:25:29,204][main.py][line:124][INFO] [Epoch:16/20, Batch:139/320]: loss1:0.0097 loss2:0.0558 loss3:0.3323 | AUC:0.7304 Anomaly AUC:0.9293
[2024-03-10 18:25:43,633][main.py][line:124][INFO] [Epoch:16/20, Batch:149/320]: loss1:0.0151 loss2:0.0590 loss3:0.3314 | AUC:0.6942 Anomaly AUC:0.9184
[2024-03-10 18:25:58,391][main.py][line:124][INFO] [Epoch:16/20, Batch:159/320]: loss1:0.0250 loss2:0.0524 loss3:0.3322 | AUC:0.7276 Anomaly AUC:0.9280
[2024-03-10 18:26:12,936][main.py][line:124][INFO] [Epoch:16/20, Batch:169/320]: loss1:0.0125 loss2:0.0341 loss3:0.3319 | AUC:0.7306 Anomaly AUC:0.9285
[2024-03-10 18:26:27,517][main.py][line:124][INFO] [Epoch:16/20, Batch:179/320]: loss1:0.0180 loss2:0.0422 loss3:0.3319 | AUC:0.7677 Anomaly AUC:0.9348
[2024-03-10 18:26:42,467][main.py][line:124][INFO] [Epoch:16/20, Batch:189/320]: loss1:0.0069 loss2:0.0477 loss3:0.3309 | AUC:0.7222 Anomaly AUC:0.9259
[2024-03-10 18:26:57,539][main.py][line:124][INFO] [Epoch:16/20, Batch:199/320]: loss1:0.0182 loss2:0.0517 loss3:0.3304 | AUC:0.7287 Anomaly AUC:0.9262
[2024-03-10 18:27:12,133][main.py][line:124][INFO] [Epoch:16/20, Batch:209/320]: loss1:0.0240 loss2:0.0454 loss3:0.3300 | AUC:0.7648 Anomaly AUC:0.9316
[2024-03-10 18:27:26,925][main.py][line:124][INFO] [Epoch:16/20, Batch:219/320]: loss1:0.0191 loss2:0.0357 loss3:0.3316 | AUC:0.7638 Anomaly AUC:0.9298
[2024-03-10 18:27:42,080][main.py][line:124][INFO] [Epoch:16/20, Batch:229/320]: loss1:0.0129 loss2:0.0458 loss3:0.3299 | AUC:0.7785 Anomaly AUC:0.9343
[2024-03-10 18:27:56,640][main.py][line:124][INFO] [Epoch:16/20, Batch:239/320]: loss1:0.0033 loss2:0.0376 loss3:0.3286 | AUC:0.7918 Anomaly AUC:0.9376
[2024-03-10 18:28:11,257][main.py][line:124][INFO] [Epoch:16/20, Batch:249/320]: loss1:0.0210 loss2:0.0474 loss3:0.3356 | AUC:0.7687 Anomaly AUC:0.9349
[2024-03-10 18:28:25,828][main.py][line:124][INFO] [Epoch:16/20, Batch:259/320]: loss1:0.0095 loss2:0.0397 loss3:0.3291 | AUC:0.7741 Anomaly AUC:0.9390
[2024-03-10 18:28:40,964][main.py][line:124][INFO] [Epoch:16/20, Batch:269/320]: loss1:0.0144 loss2:0.0419 loss3:0.3304 | AUC:0.7587 Anomaly AUC:0.9379
[2024-03-10 18:28:55,708][main.py][line:124][INFO] [Epoch:16/20, Batch:279/320]: loss1:0.0041 loss2:0.0332 loss3:0.3289 | AUC:0.7437 Anomaly AUC:0.9345
[2024-03-10 18:29:10,220][main.py][line:124][INFO] [Epoch:16/20, Batch:289/320]: loss1:0.0167 loss2:0.0856 loss3:0.3316 | AUC:0.7621 Anomaly AUC:0.9363
[2024-03-10 18:29:25,659][main.py][line:153][INFO] [Epoch:16/20]: lr:0.00010 | loss1:0.0187 loss2:0.0526 loss3:0.3288 | AUC:0.7456 Anomaly AUC:0.9324
[2024-03-10 18:29:40,461][main.py][line:124][INFO] [Epoch:17/20, Batch:9/320]: loss1:0.0186 loss2:0.0924 loss3:0.3331 | AUC:0.7535 Anomaly AUC:0.9335
[2024-03-10 18:29:55,001][main.py][line:124][INFO] [Epoch:17/20, Batch:19/320]: loss1:0.0917 loss2:0.0751 loss3:0.3410 | AUC:0.6379 Anomaly AUC:0.8961
[2024-03-10 18:30:09,373][main.py][line:124][INFO] [Epoch:17/20, Batch:29/320]: loss1:0.0372 loss2:0.0652 loss3:0.3368 | AUC:0.5828 Anomaly AUC:0.8852
[2024-03-10 18:30:24,144][main.py][line:124][INFO] [Epoch:17/20, Batch:39/320]: loss1:0.0216 loss2:0.1035 loss3:0.3341 | AUC:0.7078 Anomaly AUC:0.9216
[2024-03-10 18:30:38,742][main.py][line:124][INFO] [Epoch:17/20, Batch:49/320]: loss1:0.0303 loss2:0.0707 loss3:0.3425 | AUC:0.6821 Anomaly AUC:0.9058
[2024-03-10 18:30:53,540][main.py][line:124][INFO] [Epoch:17/20, Batch:59/320]: loss1:0.0246 loss2:0.0591 loss3:0.3324 | AUC:0.7161 Anomaly AUC:0.9160
[2024-03-10 18:31:08,123][main.py][line:124][INFO] [Epoch:17/20, Batch:69/320]: loss1:0.0853 loss2:0.1201 loss3:0.3327 | AUC:0.7442 Anomaly AUC:0.9216
[2024-03-10 18:31:22,854][main.py][line:124][INFO] [Epoch:17/20, Batch:79/320]: loss1:0.0180 loss2:0.0552 loss3:0.3354 | AUC:0.7256 Anomaly AUC:0.9178
[2024-03-10 18:31:37,729][main.py][line:124][INFO] [Epoch:17/20, Batch:89/320]: loss1:0.0198 loss2:0.0834 loss3:0.3327 | AUC:0.7340 Anomaly AUC:0.9216
[2024-03-10 18:31:52,443][main.py][line:124][INFO] [Epoch:17/20, Batch:99/320]: loss1:0.0115 loss2:0.0551 loss3:0.3326 | AUC:0.7165 Anomaly AUC:0.9163
[2024-03-10 18:32:07,451][main.py][line:124][INFO] [Epoch:17/20, Batch:109/320]: loss1:0.0062 loss2:0.0373 loss3:0.3330 | AUC:0.7379 Anomaly AUC:0.9187
[2024-03-10 18:32:22,027][main.py][line:124][INFO] [Epoch:17/20, Batch:119/320]: loss1:0.0113 loss2:0.0455 loss3:0.3322 | AUC:0.7364 Anomaly AUC:0.9211
[2024-03-10 18:32:36,577][main.py][line:124][INFO] [Epoch:17/20, Batch:129/320]: loss1:0.0294 loss2:0.0602 loss3:0.3336 | AUC:0.7533 Anomaly AUC:0.9255
[2024-03-10 18:32:51,453][main.py][line:124][INFO] [Epoch:17/20, Batch:139/320]: loss1:0.0274 loss2:0.0545 loss3:0.3381 | AUC:0.7249 Anomaly AUC:0.9103
[2024-03-10 18:33:06,294][main.py][line:124][INFO] [Epoch:17/20, Batch:149/320]: loss1:0.0297 loss2:0.1088 loss3:0.3349 | AUC:0.7531 Anomaly AUC:0.9235
[2024-03-10 18:33:20,945][main.py][line:124][INFO] [Epoch:17/20, Batch:159/320]: loss1:0.0097 loss2:0.0443 loss3:0.3328 | AUC:0.7760 Anomaly AUC:0.9296
[2024-03-10 18:33:36,084][main.py][line:124][INFO] [Epoch:17/20, Batch:169/320]: loss1:0.0105 loss2:0.0486 loss3:0.3332 | AUC:0.7732 Anomaly AUC:0.9280
[2024-03-10 18:33:50,732][main.py][line:124][INFO] [Epoch:17/20, Batch:179/320]: loss1:0.0210 loss2:0.0523 loss3:0.3305 | AUC:0.7780 Anomaly AUC:0.9312
[2024-03-10 18:34:05,181][main.py][line:124][INFO] [Epoch:17/20, Batch:189/320]: loss1:0.0147 loss2:0.0711 loss3:0.3367 | AUC:0.7445 Anomaly AUC:0.9166
[2024-03-10 18:34:19,482][main.py][line:124][INFO] [Epoch:17/20, Batch:199/320]: loss1:0.0594 loss2:0.0650 loss3:0.3435 | AUC:0.6664 Anomaly AUC:0.8974
[2024-03-10 18:34:33,964][main.py][line:124][INFO] [Epoch:17/20, Batch:209/320]: loss1:0.0242 loss2:0.0966 loss3:0.3419 | AUC:0.6956 Anomaly AUC:0.8973
[2024-03-10 18:34:48,619][main.py][line:124][INFO] [Epoch:17/20, Batch:219/320]: loss1:0.0322 loss2:0.1077 loss3:0.3429 | AUC:0.7275 Anomaly AUC:0.9060
[2024-03-10 18:35:03,044][main.py][line:124][INFO] [Epoch:17/20, Batch:229/320]: loss1:0.0173 loss2:0.0472 loss3:0.3412 | AUC:0.7232 Anomaly AUC:0.9111
[2024-03-10 18:35:17,421][main.py][line:124][INFO] [Epoch:17/20, Batch:239/320]: loss1:0.0145 loss2:0.0465 loss3:0.3381 | AUC:0.7318 Anomaly AUC:0.9212
[2024-03-10 18:35:31,838][main.py][line:124][INFO] [Epoch:17/20, Batch:249/320]: loss1:0.0036 loss2:0.0934 loss3:0.3323 | AUC:0.7653 Anomaly AUC:0.9280
[2024-03-10 18:35:46,037][main.py][line:124][INFO] [Epoch:17/20, Batch:259/320]: loss1:0.0084 loss2:0.0620 loss3:0.3372 | AUC:0.7881 Anomaly AUC:0.9331
[2024-03-10 18:36:00,731][main.py][line:124][INFO] [Epoch:17/20, Batch:269/320]: loss1:0.0199 loss2:0.0704 loss3:0.3378 | AUC:0.7513 Anomaly AUC:0.9249
[2024-03-10 18:36:15,417][main.py][line:124][INFO] [Epoch:17/20, Batch:279/320]: loss1:0.0198 loss2:0.0282 loss3:0.3345 | AUC:0.8084 Anomaly AUC:0.9396
[2024-03-10 18:36:29,841][main.py][line:124][INFO] [Epoch:17/20, Batch:289/320]: loss1:0.0261 loss2:0.0396 loss3:0.3367 | AUC:0.7175 Anomaly AUC:0.9017
[2024-03-10 18:36:44,576][main.py][line:153][INFO] [Epoch:17/20]: lr:0.00010 | loss1:0.0264 loss2:0.0444 loss3:0.3404 | AUC:0.7288 Anomaly AUC:0.9055
[2024-03-10 18:36:59,244][main.py][line:124][INFO] [Epoch:18/20, Batch:9/320]: loss1:0.0097 loss2:0.0456 loss3:0.3339 | AUC:0.7526 Anomaly AUC:0.9179
[2024-03-10 18:37:13,678][main.py][line:124][INFO] [Epoch:18/20, Batch:19/320]: loss1:0.0061 loss2:0.0300 loss3:0.3385 | AUC:0.7487 Anomaly AUC:0.9186
[2024-03-10 18:37:28,068][main.py][line:124][INFO] [Epoch:18/20, Batch:29/320]: loss1:0.0160 loss2:0.0252 loss3:0.3361 | AUC:0.7708 Anomaly AUC:0.9309
[2024-03-10 18:37:42,573][main.py][line:124][INFO] [Epoch:18/20, Batch:39/320]: loss1:0.0060 loss2:0.0366 loss3:0.3367 | AUC:0.7162 Anomaly AUC:0.9164
[2024-03-10 18:37:57,003][main.py][line:124][INFO] [Epoch:18/20, Batch:49/320]: loss1:0.0221 loss2:0.0760 loss3:0.3405 | AUC:0.6755 Anomaly AUC:0.8937
[2024-03-10 18:38:12,123][main.py][line:124][INFO] [Epoch:18/20, Batch:59/320]: loss1:0.0435 loss2:0.0652 loss3:0.3389 | AUC:0.7399 Anomaly AUC:0.9172
[2024-03-10 18:38:26,813][main.py][line:124][INFO] [Epoch:18/20, Batch:69/320]: loss1:0.0223 loss2:0.0344 loss3:0.3459 | AUC:0.6408 Anomaly AUC:0.8817
[2024-03-10 18:38:41,247][main.py][line:124][INFO] [Epoch:18/20, Batch:79/320]: loss1:0.0220 loss2:0.0920 loss3:0.3387 | AUC:0.6863 Anomaly AUC:0.9062
[2024-03-10 18:38:56,137][main.py][line:124][INFO] [Epoch:18/20, Batch:89/320]: loss1:0.0121 loss2:0.0690 loss3:0.3376 | AUC:0.7173 Anomaly AUC:0.9089
[2024-03-10 18:39:10,987][main.py][line:124][INFO] [Epoch:18/20, Batch:99/320]: loss1:0.0065 loss2:0.0293 loss3:0.3342 | AUC:0.7320 Anomaly AUC:0.9165
[2024-03-10 18:39:25,551][main.py][line:124][INFO] [Epoch:18/20, Batch:109/320]: loss1:0.0084 loss2:0.0521 loss3:0.3330 | AUC:0.7111 Anomaly AUC:0.9137
[2024-03-10 18:39:40,318][main.py][line:124][INFO] [Epoch:18/20, Batch:119/320]: loss1:0.0053 loss2:0.0452 loss3:0.3345 | AUC:0.7207 Anomaly AUC:0.9166
[2024-03-10 18:39:55,083][main.py][line:124][INFO] [Epoch:18/20, Batch:129/320]: loss1:0.0135 loss2:0.0527 loss3:0.3315 | AUC:0.7478 Anomaly AUC:0.9223
[2024-03-10 18:40:10,062][main.py][line:124][INFO] [Epoch:18/20, Batch:139/320]: loss1:0.0081 loss2:0.0533 loss3:0.3326 | AUC:0.7822 Anomaly AUC:0.9331
[2024-03-10 18:40:24,637][main.py][line:124][INFO] [Epoch:18/20, Batch:149/320]: loss1:0.0139 loss2:0.0636 loss3:0.3371 | AUC:0.7595 Anomaly AUC:0.9269
[2024-03-10 18:40:39,326][main.py][line:124][INFO] [Epoch:18/20, Batch:159/320]: loss1:0.0107 loss2:0.0462 loss3:0.3329 | AUC:0.7630 Anomaly AUC:0.9283
[2024-03-10 18:40:53,847][main.py][line:124][INFO] [Epoch:18/20, Batch:169/320]: loss1:0.0093 loss2:0.0540 loss3:0.3311 | AUC:0.7765 Anomaly AUC:0.9304
[2024-03-10 18:41:08,342][main.py][line:124][INFO] [Epoch:18/20, Batch:179/320]: loss1:0.0138 loss2:0.0401 loss3:0.3359 | AUC:0.7282 Anomaly AUC:0.9181
[2024-03-10 18:41:22,828][main.py][line:124][INFO] [Epoch:18/20, Batch:189/320]: loss1:0.0156 loss2:0.0500 loss3:0.3362 | AUC:0.7659 Anomaly AUC:0.9285
[2024-03-10 18:41:37,220][main.py][line:124][INFO] [Epoch:18/20, Batch:199/320]: loss1:0.0079 loss2:0.0311 loss3:0.3335 | AUC:0.7661 Anomaly AUC:0.9301
[2024-03-10 18:41:51,686][main.py][line:124][INFO] [Epoch:18/20, Batch:209/320]: loss1:0.0138 loss2:0.0524 loss3:0.3357 | AUC:0.7615 Anomaly AUC:0.9300
[2024-03-10 18:42:06,300][main.py][line:124][INFO] [Epoch:18/20, Batch:219/320]: loss1:0.0076 loss2:0.0377 loss3:0.3337 | AUC:0.7540 Anomaly AUC:0.9279
[2024-03-10 18:42:20,909][main.py][line:124][INFO] [Epoch:18/20, Batch:229/320]: loss1:0.0093 loss2:0.0282 loss3:0.3311 | AUC:0.7536 Anomaly AUC:0.9289
[2024-03-10 18:42:35,525][main.py][line:124][INFO] [Epoch:18/20, Batch:239/320]: loss1:0.0253 loss2:0.0297 loss3:0.3324 | AUC:0.7670 Anomaly AUC:0.9322
[2024-03-10 18:42:50,771][main.py][line:124][INFO] [Epoch:18/20, Batch:249/320]: loss1:0.0151 loss2:0.0319 loss3:0.3361 | AUC:0.7164 Anomaly AUC:0.9223
[2024-03-10 18:43:05,286][main.py][line:124][INFO] [Epoch:18/20, Batch:259/320]: loss1:0.0131 loss2:0.0339 loss3:0.3316 | AUC:0.7410 Anomaly AUC:0.9316
[2024-03-10 18:43:19,970][main.py][line:124][INFO] [Epoch:18/20, Batch:269/320]: loss1:0.0159 loss2:0.0354 loss3:0.3342 | AUC:0.7204 Anomaly AUC:0.9260
[2024-03-10 18:43:34,619][main.py][line:124][INFO] [Epoch:18/20, Batch:279/320]: loss1:0.0077 loss2:0.0670 loss3:0.3343 | AUC:0.7527 Anomaly AUC:0.9331
[2024-03-10 18:43:49,238][main.py][line:124][INFO] [Epoch:18/20, Batch:289/320]: loss1:0.1057 loss2:0.0391 loss3:0.3310 | AUC:0.7544 Anomaly AUC:0.9326
[2024-03-10 18:44:03,326][main.py][line:153][INFO] [Epoch:18/20]: lr:0.00010 | loss1:0.0074 loss2:0.0384 loss3:0.3375 | AUC:0.7015 Anomaly AUC:0.9184
[2024-03-10 18:44:18,121][main.py][line:124][INFO] [Epoch:19/20, Batch:9/320]: loss1:0.0116 loss2:0.0416 loss3:0.3328 | AUC:0.7389 Anomaly AUC:0.9260
[2024-03-10 18:44:32,589][main.py][line:124][INFO] [Epoch:19/20, Batch:19/320]: loss1:0.0090 loss2:0.0408 loss3:0.3304 | AUC:0.7534 Anomaly AUC:0.9291
[2024-03-10 18:44:47,638][main.py][line:124][INFO] [Epoch:19/20, Batch:29/320]: loss1:0.0496 loss2:0.0433 loss3:0.3336 | AUC:0.7216 Anomaly AUC:0.9075
[2024-03-10 18:45:02,224][main.py][line:124][INFO] [Epoch:19/20, Batch:39/320]: loss1:0.0143 loss2:0.0227 loss3:0.3310 | AUC:0.7821 Anomaly AUC:0.9280
[2024-03-10 18:45:17,173][main.py][line:124][INFO] [Epoch:19/20, Batch:49/320]: loss1:0.0128 loss2:0.0645 loss3:0.3343 | AUC:0.7767 Anomaly AUC:0.9305
[2024-03-10 18:45:32,090][main.py][line:124][INFO] [Epoch:19/20, Batch:59/320]: loss1:0.0088 loss2:0.0593 loss3:0.3339 | AUC:0.7779 Anomaly AUC:0.9350
[2024-03-10 18:45:46,547][main.py][line:124][INFO] [Epoch:19/20, Batch:69/320]: loss1:0.0100 loss2:0.0641 loss3:0.3334 | AUC:0.7848 Anomaly AUC:0.9350
[2024-03-10 18:46:00,996][main.py][line:124][INFO] [Epoch:19/20, Batch:79/320]: loss1:0.0069 loss2:0.0496 loss3:0.3324 | AUC:0.7591 Anomaly AUC:0.9288
[2024-03-10 18:46:15,538][main.py][line:124][INFO] [Epoch:19/20, Batch:89/320]: loss1:0.0110 loss2:0.0739 loss3:0.3326 | AUC:0.7639 Anomaly AUC:0.9320
[2024-03-10 18:46:30,019][main.py][line:124][INFO] [Epoch:19/20, Batch:99/320]: loss1:0.0099 loss2:0.0955 loss3:0.3339 | AUC:0.7424 Anomaly AUC:0.9259
[2024-03-10 18:46:44,619][main.py][line:124][INFO] [Epoch:19/20, Batch:109/320]: loss1:0.0048 loss2:0.0392 loss3:0.3310 | AUC:0.7661 Anomaly AUC:0.9307
[2024-03-10 18:46:59,366][main.py][line:124][INFO] [Epoch:19/20, Batch:119/320]: loss1:0.0200 loss2:0.0365 loss3:0.3348 | AUC:0.7581 Anomaly AUC:0.9261
[2024-03-10 18:47:13,986][main.py][line:124][INFO] [Epoch:19/20, Batch:129/320]: loss1:0.0099 loss2:0.0426 loss3:0.3335 | AUC:0.7714 Anomaly AUC:0.9317
[2024-03-10 18:47:28,894][main.py][line:124][INFO] [Epoch:19/20, Batch:139/320]: loss1:0.0261 loss2:0.0281 loss3:0.3327 | AUC:0.7821 Anomaly AUC:0.9346
[2024-03-10 18:47:43,891][main.py][line:124][INFO] [Epoch:19/20, Batch:149/320]: loss1:0.0052 loss2:0.0485 loss3:0.3303 | AUC:0.7670 Anomaly AUC:0.9307
[2024-03-10 18:47:58,697][main.py][line:124][INFO] [Epoch:19/20, Batch:159/320]: loss1:0.0080 loss2:0.0442 loss3:0.3316 | AUC:0.7200 Anomaly AUC:0.9207
[2024-03-10 18:48:13,459][main.py][line:124][INFO] [Epoch:19/20, Batch:169/320]: loss1:0.0124 loss2:0.0458 loss3:0.3339 | AUC:0.7327 Anomaly AUC:0.9235
[2024-03-10 18:48:27,848][main.py][line:124][INFO] [Epoch:19/20, Batch:179/320]: loss1:0.0377 loss2:0.0604 loss3:0.3351 | AUC:0.7215 Anomaly AUC:0.9112
[2024-03-10 18:48:42,406][main.py][line:124][INFO] [Epoch:19/20, Batch:189/320]: loss1:0.0321 loss2:0.0360 loss3:0.3315 | AUC:0.7593 Anomaly AUC:0.9265
[2024-03-10 18:48:57,534][main.py][line:124][INFO] [Epoch:19/20, Batch:199/320]: loss1:0.0399 loss2:0.0546 loss3:0.3392 | AUC:0.7490 Anomaly AUC:0.9198
[2024-03-10 18:49:12,501][main.py][line:124][INFO] [Epoch:19/20, Batch:209/320]: loss1:0.0223 loss2:0.0410 loss3:0.3356 | AUC:0.7234 Anomaly AUC:0.9161
[2024-03-10 18:49:27,686][main.py][line:124][INFO] [Epoch:19/20, Batch:219/320]: loss1:0.1072 loss2:0.0456 loss3:0.3313 | AUC:0.7091 Anomaly AUC:0.9153
[2024-03-10 18:49:42,706][main.py][line:124][INFO] [Epoch:19/20, Batch:229/320]: loss1:0.0220 loss2:0.1079 loss3:0.3397 | AUC:0.7172 Anomaly AUC:0.9105
[2024-03-10 18:49:57,287][main.py][line:124][INFO] [Epoch:19/20, Batch:239/320]: loss1:0.0299 loss2:0.0732 loss3:0.3322 | AUC:0.7407 Anomaly AUC:0.9278
[2024-03-10 18:50:12,391][main.py][line:124][INFO] [Epoch:19/20, Batch:249/320]: loss1:0.0322 loss2:0.0448 loss3:0.3361 | AUC:0.7452 Anomaly AUC:0.9220
[2024-03-10 18:50:27,121][main.py][line:124][INFO] [Epoch:19/20, Batch:259/320]: loss1:0.0217 loss2:0.0272 loss3:0.3401 | AUC:0.7307 Anomaly AUC:0.9188
[2024-03-10 18:50:41,641][main.py][line:124][INFO] [Epoch:19/20, Batch:269/320]: loss1:0.0134 loss2:0.0422 loss3:0.3359 | AUC:0.7295 Anomaly AUC:0.9230
[2024-03-10 18:50:56,363][main.py][line:124][INFO] [Epoch:19/20, Batch:279/320]: loss1:0.0044 loss2:0.1244 loss3:0.3374 | AUC:0.7583 Anomaly AUC:0.9291
[2024-03-10 18:51:10,914][main.py][line:124][INFO] [Epoch:19/20, Batch:289/320]: loss1:0.0118 loss2:0.0564 loss3:0.3375 | AUC:0.7365 Anomaly AUC:0.9246
[2024-03-10 18:51:25,503][main.py][line:153][INFO] [Epoch:19/20]: lr:0.00010 | loss1:0.0931 loss2:0.1074 loss3:0.3378 | AUC:0.7152 Anomaly AUC:0.9155
[2024-03-10 18:51:40,324][main.py][line:124][INFO] [Epoch:20/20, Batch:9/320]: loss1:0.0534 loss2:0.0655 loss3:0.3421 | AUC:0.6586 Anomaly AUC:0.8859
[2024-03-10 18:51:54,892][main.py][line:124][INFO] [Epoch:20/20, Batch:19/320]: loss1:0.0562 loss2:0.0361 loss3:0.3467 | AUC:0.5150 Anomaly AUC:0.8328
[2024-03-10 18:52:09,378][main.py][line:124][INFO] [Epoch:20/20, Batch:29/320]: loss1:0.0229 loss2:0.1214 loss3:0.3456 | AUC:0.7052 Anomaly AUC:0.8960
[2024-03-10 18:52:23,828][main.py][line:124][INFO] [Epoch:20/20, Batch:39/320]: loss1:0.0248 loss2:0.0747 loss3:0.3484 | AUC:0.6778 Anomaly AUC:0.8917
[2024-03-10 18:52:38,302][main.py][line:124][INFO] [Epoch:20/20, Batch:49/320]: loss1:0.0239 loss2:0.0503 loss3:0.3381 | AUC:0.7153 Anomaly AUC:0.9031
[2024-03-10 18:52:52,851][main.py][line:124][INFO] [Epoch:20/20, Batch:59/320]: loss1:0.0150 loss2:0.0416 loss3:0.3412 | AUC:0.7253 Anomaly AUC:0.9045
[2024-03-10 18:53:08,120][main.py][line:124][INFO] [Epoch:20/20, Batch:69/320]: loss1:0.0103 loss2:0.0318 loss3:0.3399 | AUC:0.7184 Anomaly AUC:0.9010
[2024-03-10 18:53:22,693][main.py][line:124][INFO] [Epoch:20/20, Batch:79/320]: loss1:0.0108 loss2:0.0559 loss3:0.3453 | AUC:0.7604 Anomaly AUC:0.9170
[2024-03-10 18:53:37,383][main.py][line:124][INFO] [Epoch:20/20, Batch:89/320]: loss1:0.0087 loss2:0.0578 loss3:0.3453 | AUC:0.7732 Anomaly AUC:0.9194
[2024-03-10 18:53:52,762][main.py][line:124][INFO] [Epoch:20/20, Batch:99/320]: loss1:0.0247 loss2:0.0804 loss3:0.3424 | AUC:0.7197 Anomaly AUC:0.9032
[2024-03-10 18:54:07,339][main.py][line:124][INFO] [Epoch:20/20, Batch:109/320]: loss1:0.0171 loss2:0.0507 loss3:0.3416 | AUC:0.7260 Anomaly AUC:0.9052
[2024-03-10 18:54:22,334][main.py][line:124][INFO] [Epoch:20/20, Batch:119/320]: loss1:0.0158 loss2:0.0393 loss3:0.3413 | AUC:0.7397 Anomaly AUC:0.9022
[2024-03-10 18:54:36,876][main.py][line:124][INFO] [Epoch:20/20, Batch:129/320]: loss1:0.0162 loss2:0.0705 loss3:0.3406 | AUC:0.7292 Anomaly AUC:0.9030
[2024-03-10 18:54:51,391][main.py][line:124][INFO] [Epoch:20/20, Batch:139/320]: loss1:0.0487 loss2:0.0478 loss3:0.3522 | AUC:0.6056 Anomaly AUC:0.8613
[2024-03-10 18:55:05,897][main.py][line:124][INFO] [Epoch:20/20, Batch:149/320]: loss1:0.0179 loss2:0.0904 loss3:0.3485 | AUC:0.5999 Anomaly AUC:0.8585
[2024-03-10 18:55:20,240][main.py][line:124][INFO] [Epoch:20/20, Batch:159/320]: loss1:0.0166 loss2:0.0391 loss3:0.3456 | AUC:0.7335 Anomaly AUC:0.8975
[2024-03-10 18:55:34,707][main.py][line:124][INFO] [Epoch:20/20, Batch:169/320]: loss1:0.0175 loss2:0.0356 loss3:0.3416 | AUC:0.7644 Anomaly AUC:0.9126
[2024-03-10 18:55:49,102][main.py][line:124][INFO] [Epoch:20/20, Batch:179/320]: loss1:0.0118 loss2:0.0380 loss3:0.3418 | AUC:0.7434 Anomaly AUC:0.9046
[2024-03-10 18:56:03,438][main.py][line:124][INFO] [Epoch:20/20, Batch:189/320]: loss1:0.0229 loss2:0.0382 loss3:0.3428 | AUC:0.7460 Anomaly AUC:0.9053
[2024-03-10 18:56:18,005][main.py][line:124][INFO] [Epoch:20/20, Batch:199/320]: loss1:0.0319 loss2:0.0806 loss3:0.3506 | AUC:0.7013 Anomaly AUC:0.8823
[2024-03-10 18:56:32,484][main.py][line:124][INFO] [Epoch:20/20, Batch:209/320]: loss1:0.0175 loss2:0.0404 loss3:0.3437 | AUC:0.7148 Anomaly AUC:0.8900
[2024-03-10 18:56:47,066][main.py][line:124][INFO] [Epoch:20/20, Batch:219/320]: loss1:0.0137 loss2:0.0681 loss3:0.3438 | AUC:0.7132 Anomaly AUC:0.8912
[2024-03-10 18:57:01,805][main.py][line:124][INFO] [Epoch:20/20, Batch:229/320]: loss1:0.0171 loss2:0.0420 loss3:0.3436 | AUC:0.7464 Anomaly AUC:0.9058
[2024-03-10 18:57:16,233][main.py][line:124][INFO] [Epoch:20/20, Batch:239/320]: loss1:0.0208 loss2:0.0390 loss3:0.3389 | AUC:0.7637 Anomaly AUC:0.9147
[2024-03-10 18:57:30,699][main.py][line:124][INFO] [Epoch:20/20, Batch:249/320]: loss1:0.0342 loss2:0.1044 loss3:0.3599 | AUC:0.6603 Anomaly AUC:0.8704
[2024-03-10 18:57:45,116][main.py][line:124][INFO] [Epoch:20/20, Batch:259/320]: loss1:0.0376 loss2:0.0791 loss3:0.3516 | AUC:0.7093 Anomaly AUC:0.8924
[2024-03-10 18:57:59,785][main.py][line:124][INFO] [Epoch:20/20, Batch:269/320]: loss1:0.0105 loss2:0.0570 loss3:0.3457 | AUC:0.7199 Anomaly AUC:0.8982
[2024-03-10 18:58:14,293][main.py][line:124][INFO] [Epoch:20/20, Batch:279/320]: loss1:0.0109 loss2:0.0411 loss3:0.3483 | AUC:0.7022 Anomaly AUC:0.8919
[2024-03-10 18:58:28,685][main.py][line:124][INFO] [Epoch:20/20, Batch:289/320]: loss1:0.0058 loss2:0.0620 loss3:0.3495 | AUC:0.7240 Anomaly AUC:0.9076
[2024-03-10 18:58:43,306][main.py][line:153][INFO] [Epoch:20/20]: lr:0.00010 | loss1:0.0097 loss2:0.0557 loss3:0.3391 | AUC:0.7380 Anomaly AUC:0.9148
[2024-03-10 18:58:43,331][main.py][line:171][INFO] Training completes in 140m 27s | best AUCAP:0.8369 Anomaly AUC:0.9469

[2024-03-10 19:05:48,254][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 19:05:48,544][main.py][line:259][INFO] total params:8.3467M
[2024-03-10 19:05:48,544][main.py][line:268][INFO] Test Mode
[2024-03-10 19:05:48,544][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/xd__current.pkl.
[2024-03-10 19:06:03,509][infer.py][line:94][INFO] offline AUC:0.9473 Anomaly-AUC:0.8442 AP:0.8387 FAR:0.0032 | Complete in 0m 15s

[2024-03-10 19:09:50,897][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 19:09:51,132][main.py][line:259][INFO] total params:3.5687M
[2024-03-10 19:09:51,132][main.py][line:268][INFO] Test Mode
[2024-03-10 19:09:51,132][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/xd__current.pkl.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.Amemory.memory_block not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.Nmemory.memory_block not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.0.norm.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.0.norm.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.0.fn.to_qkv.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.0.fn.to_out.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.0.fn.to_out.0.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.norm.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.norm.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.fn.net.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.fn.net.0.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.fn.net.3.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.0.1.fn.net.3.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.0.norm.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.0.norm.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.0.fn.to_qkv.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.0.fn.to_out.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.0.fn.to_out.0.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.norm.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.norm.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.fn.net.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.fn.net.0.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.fn.net.3.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.selfatt.layers.1.1.fn.net.3.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.encoder_mu.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.encoder_mu.0.bias not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.encoder_var.0.weight not found in model dict.
[2024-03-10 19:09:51,144][main.py][line:53][INFO] self_attention.UR_DMU.encoder_var.0.bias not found in model dict.
[2024-03-10 19:09:54,569][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 19:09:57,918][main.py][line:259][INFO] total params:3.5687M
[2024-03-10 19:09:57,918][main.py][line:262][INFO] Training Mode
[2024-03-10 19:09:57,919][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 19:09:57,919][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 19:10:15,249][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 19:10:18,291][main.py][line:259][INFO] total params:7.7702M
[2024-03-10 19:10:18,291][main.py][line:262][INFO] Training Mode
[2024-03-10 19:10:18,292][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-10 19:10:18,292][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-10 19:10:31,327][main.py][line:76][INFO] Random initialize AUCAP:0.2093 Anomaly AUC:0.44796
[2024-03-10 19:11:38,820][main.py][line:153][INFO] [Epoch:1/20]: lr:0.00010 | loss1:0.2797 loss2:0.5563 loss3:0.0000 | AUC:0.7258 Anomaly AUC:0.9210
[2024-03-10 19:11:53,415][main.py][line:124][INFO] [Epoch:2/20, Batch:9/320]: loss1:0.1690 loss2:0.4960 loss3:0.0000 | AUC:0.7503 Anomaly AUC:0.9268
[2024-03-10 19:12:07,348][main.py][line:124][INFO] [Epoch:2/20, Batch:19/320]: loss1:0.1808 loss2:0.4901 loss3:0.0000 | AUC:0.7385 Anomaly AUC:0.9166
[2024-03-10 19:12:21,597][main.py][line:124][INFO] [Epoch:2/20, Batch:29/320]: loss1:0.1533 loss2:0.4632 loss3:0.0000 | AUC:0.7698 Anomaly AUC:0.9324
[2024-03-10 19:12:35,818][main.py][line:124][INFO] [Epoch:2/20, Batch:39/320]: loss1:0.1506 loss2:0.4637 loss3:0.0000 | AUC:0.8016 Anomaly AUC:0.9401
[2024-03-10 19:12:50,073][main.py][line:124][INFO] [Epoch:2/20, Batch:49/320]: loss1:0.2061 loss2:0.4715 loss3:0.0000 | AUC:0.7641 Anomaly AUC:0.9333
[2024-03-10 19:13:04,375][main.py][line:124][INFO] [Epoch:2/20, Batch:59/320]: loss1:0.0851 loss2:0.3632 loss3:0.0000 | AUC:0.7612 Anomaly AUC:0.9238
[2024-03-10 19:13:18,523][main.py][line:124][INFO] [Epoch:2/20, Batch:69/320]: loss1:0.1444 loss2:0.4491 loss3:0.0000 | AUC:0.7709 Anomaly AUC:0.9289
[2024-03-10 19:13:32,855][main.py][line:124][INFO] [Epoch:2/20, Batch:79/320]: loss1:0.1576 loss2:0.5215 loss3:0.0000 | AUC:0.7749 Anomaly AUC:0.9292
[2024-03-10 19:13:47,018][main.py][line:124][INFO] [Epoch:2/20, Batch:89/320]: loss1:0.1506 loss2:0.4802 loss3:0.0000 | AUC:0.7676 Anomaly AUC:0.9265
[2024-03-10 19:14:01,219][main.py][line:124][INFO] [Epoch:2/20, Batch:99/320]: loss1:0.1308 loss2:0.4354 loss3:0.0000 | AUC:0.7945 Anomaly AUC:0.9397
[2024-03-10 19:14:15,420][main.py][line:124][INFO] [Epoch:2/20, Batch:109/320]: loss1:0.2368 loss2:0.5120 loss3:0.0000 | AUC:0.7587 Anomaly AUC:0.9289
[2024-03-10 19:14:29,678][main.py][line:124][INFO] [Epoch:2/20, Batch:119/320]: loss1:0.1549 loss2:0.3514 loss3:0.0000 | AUC:0.7480 Anomaly AUC:0.9164
[2024-03-10 19:14:43,820][main.py][line:124][INFO] [Epoch:2/20, Batch:129/320]: loss1:0.1138 loss2:0.4197 loss3:0.0000 | AUC:0.7563 Anomaly AUC:0.9285
[2024-03-10 19:14:58,144][main.py][line:124][INFO] [Epoch:2/20, Batch:139/320]: loss1:0.1781 loss2:0.4989 loss3:0.0000 | AUC:0.8152 Anomaly AUC:0.9399
[2024-03-10 19:15:12,315][main.py][line:124][INFO] [Epoch:2/20, Batch:149/320]: loss1:0.0882 loss2:0.3389 loss3:0.0000 | AUC:0.7292 Anomaly AUC:0.9119
[2024-03-10 19:15:26,568][main.py][line:124][INFO] [Epoch:2/20, Batch:159/320]: loss1:0.1033 loss2:0.4504 loss3:0.0000 | AUC:0.7939 Anomaly AUC:0.9362
[2024-03-10 19:15:40,799][main.py][line:124][INFO] [Epoch:2/20, Batch:169/320]: loss1:0.0771 loss2:0.4150 loss3:0.0000 | AUC:0.7628 Anomaly AUC:0.9277
[2024-03-10 19:15:55,199][main.py][line:124][INFO] [Epoch:2/20, Batch:179/320]: loss1:0.1247 loss2:0.4502 loss3:0.0000 | AUC:0.7549 Anomaly AUC:0.9199
[2024-03-10 19:16:09,418][main.py][line:124][INFO] [Epoch:2/20, Batch:189/320]: loss1:0.2979 loss2:0.5123 loss3:0.0000 | AUC:0.7558 Anomaly AUC:0.9226
[2024-03-10 19:16:23,826][main.py][line:124][INFO] [Epoch:2/20, Batch:199/320]: loss1:0.2609 loss2:0.4909 loss3:0.0000 | AUC:0.7865 Anomaly AUC:0.9253
[2024-03-10 19:16:38,103][main.py][line:124][INFO] [Epoch:2/20, Batch:209/320]: loss1:0.2792 loss2:0.5164 loss3:0.0000 | AUC:0.7898 Anomaly AUC:0.9314
[2024-03-10 19:16:52,255][main.py][line:124][INFO] [Epoch:2/20, Batch:219/320]: loss1:0.1693 loss2:0.4391 loss3:0.0000 | AUC:0.8022 Anomaly AUC:0.9372
[2024-03-10 19:17:06,580][main.py][line:124][INFO] [Epoch:2/20, Batch:229/320]: loss1:0.3507 loss2:0.6310 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9262
[2024-03-10 19:17:20,879][main.py][line:124][INFO] [Epoch:2/20, Batch:239/320]: loss1:0.1430 loss2:0.4949 loss3:0.0000 | AUC:0.7961 Anomaly AUC:0.9343
[2024-03-10 19:17:35,040][main.py][line:124][INFO] [Epoch:2/20, Batch:249/320]: loss1:0.0547 loss2:0.3609 loss3:0.0000 | AUC:0.8243 Anomaly AUC:0.9425
[2024-03-10 19:17:49,490][main.py][line:124][INFO] [Epoch:2/20, Batch:259/320]: loss1:0.1353 loss2:0.3919 loss3:0.0000 | AUC:0.7802 Anomaly AUC:0.9267
[2024-03-10 19:18:03,761][main.py][line:124][INFO] [Epoch:2/20, Batch:269/320]: loss1:0.0922 loss2:0.3897 loss3:0.0000 | AUC:0.7728 Anomaly AUC:0.9274
[2024-03-10 19:18:17,979][main.py][line:124][INFO] [Epoch:2/20, Batch:279/320]: loss1:0.0519 loss2:0.3030 loss3:0.0000 | AUC:0.7809 Anomaly AUC:0.9320
[2024-03-10 19:18:32,075][main.py][line:124][INFO] [Epoch:2/20, Batch:289/320]: loss1:0.1412 loss2:0.3970 loss3:0.0000 | AUC:0.7511 Anomaly AUC:0.9230
[2024-03-10 19:18:45,757][main.py][line:153][INFO] [Epoch:2/20]: lr:0.00010 | loss1:0.1679 loss2:0.3609 loss3:0.0000 | AUC:0.7677 Anomaly AUC:0.9269
[2024-03-10 19:19:01,158][main.py][line:124][INFO] [Epoch:3/20, Batch:9/320]: loss1:0.2655 loss2:0.5087 loss3:0.0000 | AUC:0.7650 Anomaly AUC:0.9331
[2024-03-10 19:19:16,173][main.py][line:124][INFO] [Epoch:3/20, Batch:19/320]: loss1:0.3475 loss2:0.6519 loss3:0.0000 | AUC:0.7496 Anomaly AUC:0.9170
[2024-03-10 19:19:30,439][main.py][line:124][INFO] [Epoch:3/20, Batch:29/320]: loss1:0.1512 loss2:0.4940 loss3:0.0000 | AUC:0.7741 Anomaly AUC:0.9278
[2024-03-10 19:19:45,054][main.py][line:124][INFO] [Epoch:3/20, Batch:39/320]: loss1:0.2964 loss2:0.4768 loss3:0.0000 | AUC:0.7804 Anomaly AUC:0.9291
[2024-03-10 19:19:59,415][main.py][line:124][INFO] [Epoch:3/20, Batch:49/320]: loss1:0.1625 loss2:0.4698 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9302
[2024-03-10 19:20:13,874][main.py][line:124][INFO] [Epoch:3/20, Batch:59/320]: loss1:0.1989 loss2:0.4851 loss3:0.0000 | AUC:0.7576 Anomaly AUC:0.9224
[2024-03-10 19:20:28,416][main.py][line:124][INFO] [Epoch:3/20, Batch:69/320]: loss1:0.0967 loss2:0.3718 loss3:0.0000 | AUC:0.7624 Anomaly AUC:0.9252
[2024-03-10 19:20:42,627][main.py][line:124][INFO] [Epoch:3/20, Batch:79/320]: loss1:0.0663 loss2:0.3108 loss3:0.0000 | AUC:0.7755 Anomaly AUC:0.9314
[2024-03-10 19:20:56,860][main.py][line:124][INFO] [Epoch:3/20, Batch:89/320]: loss1:0.0819 loss2:0.2821 loss3:0.0000 | AUC:0.7514 Anomaly AUC:0.9247
[2024-03-10 19:21:11,167][main.py][line:124][INFO] [Epoch:3/20, Batch:99/320]: loss1:0.1037 loss2:0.3426 loss3:0.0000 | AUC:0.7737 Anomaly AUC:0.9310
[2024-03-10 19:21:25,598][main.py][line:124][INFO] [Epoch:3/20, Batch:109/320]: loss1:0.0870 loss2:0.3566 loss3:0.0000 | AUC:0.7690 Anomaly AUC:0.9286
[2024-03-10 19:21:39,866][main.py][line:124][INFO] [Epoch:3/20, Batch:119/320]: loss1:0.1436 loss2:0.4542 loss3:0.0000 | AUC:0.7589 Anomaly AUC:0.9272
[2024-03-10 19:21:54,191][main.py][line:124][INFO] [Epoch:3/20, Batch:129/320]: loss1:0.0641 loss2:0.2834 loss3:0.0000 | AUC:0.7485 Anomaly AUC:0.9227
[2024-03-10 19:22:08,911][main.py][line:124][INFO] [Epoch:3/20, Batch:139/320]: loss1:0.0670 loss2:0.3499 loss3:0.0000 | AUC:0.7452 Anomaly AUC:0.9244
[2024-03-10 19:22:23,179][main.py][line:124][INFO] [Epoch:3/20, Batch:149/320]: loss1:0.1330 loss2:0.3522 loss3:0.0000 | AUC:0.7301 Anomaly AUC:0.9218
[2024-03-10 19:22:38,059][main.py][line:124][INFO] [Epoch:3/20, Batch:159/320]: loss1:0.1085 loss2:0.3243 loss3:0.0000 | AUC:0.7585 Anomaly AUC:0.9304
[2024-03-10 19:22:52,380][main.py][line:124][INFO] [Epoch:3/20, Batch:169/320]: loss1:0.1749 loss2:0.4088 loss3:0.0000 | AUC:0.7579 Anomaly AUC:0.9275
[2024-03-10 19:23:06,857][main.py][line:124][INFO] [Epoch:3/20, Batch:179/320]: loss1:0.0560 loss2:0.3634 loss3:0.0000 | AUC:0.7612 Anomaly AUC:0.9279
[2024-03-10 19:23:21,068][main.py][line:124][INFO] [Epoch:3/20, Batch:189/320]: loss1:0.0779 loss2:0.3480 loss3:0.0000 | AUC:0.7862 Anomaly AUC:0.9273
[2024-03-10 19:23:35,303][main.py][line:124][INFO] [Epoch:3/20, Batch:199/320]: loss1:0.0704 loss2:0.3317 loss3:0.0000 | AUC:0.7887 Anomaly AUC:0.9338
[2024-03-10 19:23:49,678][main.py][line:124][INFO] [Epoch:3/20, Batch:209/320]: loss1:0.0514 loss2:0.3201 loss3:0.0000 | AUC:0.7088 Anomaly AUC:0.9137
[2024-03-10 19:24:03,766][main.py][line:124][INFO] [Epoch:3/20, Batch:219/320]: loss1:0.0311 loss2:0.3308 loss3:0.0000 | AUC:0.7922 Anomaly AUC:0.9377
[2024-03-10 19:24:18,091][main.py][line:124][INFO] [Epoch:3/20, Batch:229/320]: loss1:0.0768 loss2:0.3353 loss3:0.0000 | AUC:0.7799 Anomaly AUC:0.9257
[2024-03-10 19:24:32,609][main.py][line:124][INFO] [Epoch:3/20, Batch:239/320]: loss1:0.0806 loss2:0.3536 loss3:0.0000 | AUC:0.7697 Anomaly AUC:0.9305
[2024-03-10 19:24:47,281][main.py][line:124][INFO] [Epoch:3/20, Batch:249/320]: loss1:0.0518 loss2:0.3313 loss3:0.0000 | AUC:0.7960 Anomaly AUC:0.9372
[2024-03-10 19:25:01,510][main.py][line:124][INFO] [Epoch:3/20, Batch:259/320]: loss1:0.0573 loss2:0.2755 loss3:0.0000 | AUC:0.7480 Anomaly AUC:0.9156
[2024-03-10 19:25:15,547][main.py][line:124][INFO] [Epoch:3/20, Batch:269/320]: loss1:0.1213 loss2:0.4799 loss3:0.0000 | AUC:0.7959 Anomaly AUC:0.9328
[2024-03-10 19:25:29,751][main.py][line:124][INFO] [Epoch:3/20, Batch:279/320]: loss1:0.0591 loss2:0.3672 loss3:0.0000 | AUC:0.7964 Anomaly AUC:0.9329
[2024-03-10 19:25:43,894][main.py][line:124][INFO] [Epoch:3/20, Batch:289/320]: loss1:0.1725 loss2:0.4877 loss3:0.0000 | AUC:0.7601 Anomaly AUC:0.9252
[2024-03-10 19:25:57,509][main.py][line:153][INFO] [Epoch:3/20]: lr:0.00010 | loss1:0.0818 loss2:0.3799 loss3:0.0000 | AUC:0.8009 Anomaly AUC:0.9367
[2024-03-10 19:26:12,966][main.py][line:124][INFO] [Epoch:4/20, Batch:9/320]: loss1:0.0784 loss2:0.2730 loss3:0.0000 | AUC:0.7944 Anomaly AUC:0.9355
[2024-03-10 19:26:27,153][main.py][line:124][INFO] [Epoch:4/20, Batch:19/320]: loss1:0.0776 loss2:0.2338 loss3:0.0000 | AUC:0.7443 Anomaly AUC:0.9219
[2024-03-10 19:26:41,590][main.py][line:124][INFO] [Epoch:4/20, Batch:29/320]: loss1:0.0673 loss2:0.3209 loss3:0.0000 | AUC:0.7790 Anomaly AUC:0.9307
[2024-03-10 19:26:55,737][main.py][line:124][INFO] [Epoch:4/20, Batch:39/320]: loss1:0.0212 loss2:0.3394 loss3:0.0000 | AUC:0.7960 Anomaly AUC:0.9347
[2024-03-10 19:27:10,394][main.py][line:124][INFO] [Epoch:4/20, Batch:49/320]: loss1:0.1131 loss2:0.3874 loss3:0.0000 | AUC:0.7805 Anomaly AUC:0.9319
[2024-03-10 19:27:24,511][main.py][line:124][INFO] [Epoch:4/20, Batch:59/320]: loss1:0.0779 loss2:0.3532 loss3:0.0000 | AUC:0.7741 Anomaly AUC:0.9293
[2024-03-10 19:27:39,143][main.py][line:124][INFO] [Epoch:4/20, Batch:69/320]: loss1:0.0907 loss2:0.3582 loss3:0.0000 | AUC:0.7834 Anomaly AUC:0.9337
[2024-03-10 19:27:53,371][main.py][line:124][INFO] [Epoch:4/20, Batch:79/320]: loss1:0.0377 loss2:0.2625 loss3:0.0000 | AUC:0.7711 Anomaly AUC:0.9257
[2024-03-10 19:28:07,640][main.py][line:124][INFO] [Epoch:4/20, Batch:89/320]: loss1:0.1164 loss2:0.3230 loss3:0.0000 | AUC:0.7780 Anomaly AUC:0.9282
[2024-03-10 19:28:21,906][main.py][line:124][INFO] [Epoch:4/20, Batch:99/320]: loss1:0.0836 loss2:0.2437 loss3:0.0000 | AUC:0.7344 Anomaly AUC:0.9137
[2024-03-10 19:28:36,256][main.py][line:124][INFO] [Epoch:4/20, Batch:109/320]: loss1:0.1104 loss2:0.3482 loss3:0.0000 | AUC:0.8004 Anomaly AUC:0.9364
[2024-03-10 19:28:51,009][main.py][line:124][INFO] [Epoch:4/20, Batch:119/320]: loss1:0.0309 loss2:0.2080 loss3:0.0000 | AUC:0.7832 Anomaly AUC:0.9314
[2024-03-10 19:29:05,246][main.py][line:124][INFO] [Epoch:4/20, Batch:129/320]: loss1:0.0617 loss2:0.2635 loss3:0.0000 | AUC:0.8012 Anomaly AUC:0.9360
[2024-03-10 19:29:19,513][main.py][line:124][INFO] [Epoch:4/20, Batch:139/320]: loss1:0.0187 loss2:0.3263 loss3:0.0000 | AUC:0.7880 Anomaly AUC:0.9280
[2024-03-10 19:29:34,088][main.py][line:124][INFO] [Epoch:4/20, Batch:149/320]: loss1:0.0932 loss2:0.3766 loss3:0.0000 | AUC:0.7800 Anomaly AUC:0.9279
[2024-03-10 19:29:48,393][main.py][line:124][INFO] [Epoch:4/20, Batch:159/320]: loss1:0.0218 loss2:0.2563 loss3:0.0000 | AUC:0.7923 Anomaly AUC:0.9333
[2024-03-10 19:30:02,646][main.py][line:124][INFO] [Epoch:4/20, Batch:169/320]: loss1:0.0348 loss2:0.2757 loss3:0.0000 | AUC:0.7846 Anomaly AUC:0.9321
[2024-03-10 19:30:16,954][main.py][line:124][INFO] [Epoch:4/20, Batch:179/320]: loss1:0.1562 loss2:0.3264 loss3:0.0000 | AUC:0.7697 Anomaly AUC:0.9242
[2024-03-10 19:30:31,149][main.py][line:124][INFO] [Epoch:4/20, Batch:189/320]: loss1:0.0457 loss2:0.3040 loss3:0.0000 | AUC:0.7807 Anomaly AUC:0.9301
[2024-03-10 19:30:45,463][main.py][line:124][INFO] [Epoch:4/20, Batch:199/320]: loss1:0.0960 loss2:0.3207 loss3:0.0000 | AUC:0.7644 Anomaly AUC:0.9272
[2024-03-10 19:30:59,896][main.py][line:124][INFO] [Epoch:4/20, Batch:209/320]: loss1:0.0739 loss2:0.3120 loss3:0.0000 | AUC:0.7620 Anomaly AUC:0.9285
[2024-03-10 19:31:14,558][main.py][line:124][INFO] [Epoch:4/20, Batch:219/320]: loss1:0.0475 loss2:0.3533 loss3:0.0000 | AUC:0.7780 Anomaly AUC:0.9273
[2024-03-10 19:31:28,792][main.py][line:124][INFO] [Epoch:4/20, Batch:229/320]: loss1:0.0621 loss2:0.2778 loss3:0.0000 | AUC:0.7835 Anomaly AUC:0.9310
[2024-03-10 19:31:42,996][main.py][line:124][INFO] [Epoch:4/20, Batch:239/320]: loss1:0.0375 loss2:0.2913 loss3:0.0000 | AUC:0.7696 Anomaly AUC:0.9265
[2024-03-10 19:31:57,501][main.py][line:124][INFO] [Epoch:4/20, Batch:249/320]: loss1:0.0550 loss2:0.3245 loss3:0.0000 | AUC:0.7736 Anomaly AUC:0.9260
[2024-03-10 19:32:11,513][main.py][line:124][INFO] [Epoch:4/20, Batch:259/320]: loss1:0.0179 loss2:0.3027 loss3:0.0000 | AUC:0.7934 Anomaly AUC:0.9354
[2024-03-10 19:32:25,781][main.py][line:124][INFO] [Epoch:4/20, Batch:269/320]: loss1:0.0567 loss2:0.2728 loss3:0.0000 | AUC:0.8008 Anomaly AUC:0.9371
[2024-03-10 19:32:40,124][main.py][line:124][INFO] [Epoch:4/20, Batch:279/320]: loss1:0.0198 loss2:0.2483 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9354
[2024-03-10 19:32:54,486][main.py][line:124][INFO] [Epoch:4/20, Batch:289/320]: loss1:0.0870 loss2:0.2891 loss3:0.0000 | AUC:0.7822 Anomaly AUC:0.9309
[2024-03-10 19:33:09,673][main.py][line:153][INFO] [Epoch:4/20]: lr:0.00010 | loss1:0.0976 loss2:0.2782 loss3:0.0000 | AUC:0.7449 Anomaly AUC:0.9247
[2024-03-10 19:33:24,979][main.py][line:124][INFO] [Epoch:5/20, Batch:9/320]: loss1:0.0229 loss2:0.3073 loss3:0.0000 | AUC:0.7765 Anomaly AUC:0.9258
[2024-03-10 19:33:39,550][main.py][line:124][INFO] [Epoch:5/20, Batch:19/320]: loss1:0.0423 loss2:0.2307 loss3:0.0000 | AUC:0.7742 Anomaly AUC:0.9311
[2024-03-10 19:33:54,254][main.py][line:124][INFO] [Epoch:5/20, Batch:29/320]: loss1:0.0225 loss2:0.2902 loss3:0.0000 | AUC:0.7875 Anomaly AUC:0.9343
[2024-03-10 19:34:09,089][main.py][line:124][INFO] [Epoch:5/20, Batch:39/320]: loss1:0.0100 loss2:0.2750 loss3:0.0000 | AUC:0.7796 Anomaly AUC:0.9288
[2024-03-10 19:34:23,978][main.py][line:124][INFO] [Epoch:5/20, Batch:49/320]: loss1:0.0112 loss2:0.1825 loss3:0.0000 | AUC:0.7848 Anomaly AUC:0.9330
[2024-03-10 19:34:38,470][main.py][line:124][INFO] [Epoch:5/20, Batch:59/320]: loss1:0.0597 loss2:0.2263 loss3:0.0000 | AUC:0.7888 Anomaly AUC:0.9328
[2024-03-10 19:34:52,796][main.py][line:124][INFO] [Epoch:5/20, Batch:69/320]: loss1:0.0163 loss2:0.2809 loss3:0.0000 | AUC:0.7834 Anomaly AUC:0.9310
[2024-03-10 19:35:07,088][main.py][line:124][INFO] [Epoch:5/20, Batch:79/320]: loss1:0.0216 loss2:0.2911 loss3:0.0000 | AUC:0.8110 Anomaly AUC:0.9373
[2024-03-10 19:35:21,667][main.py][line:124][INFO] [Epoch:5/20, Batch:89/320]: loss1:0.0676 loss2:0.2754 loss3:0.0000 | AUC:0.7677 Anomaly AUC:0.9303
[2024-03-10 19:35:36,271][main.py][line:124][INFO] [Epoch:5/20, Batch:99/320]: loss1:0.0184 loss2:0.2876 loss3:0.0000 | AUC:0.7716 Anomaly AUC:0.9267
[2024-03-10 19:35:50,971][main.py][line:124][INFO] [Epoch:5/20, Batch:109/320]: loss1:0.0224 loss2:0.2300 loss3:0.0000 | AUC:0.7605 Anomaly AUC:0.9246
[2024-03-10 19:36:05,516][main.py][line:124][INFO] [Epoch:5/20, Batch:119/320]: loss1:0.0853 loss2:0.2897 loss3:0.0000 | AUC:0.7595 Anomaly AUC:0.9271
[2024-03-10 19:36:20,146][main.py][line:124][INFO] [Epoch:5/20, Batch:129/320]: loss1:0.0967 loss2:0.3824 loss3:0.0000 | AUC:0.7754 Anomaly AUC:0.9255
[2024-03-10 19:36:35,127][main.py][line:124][INFO] [Epoch:5/20, Batch:139/320]: loss1:0.0579 loss2:0.2422 loss3:0.0000 | AUC:0.7464 Anomaly AUC:0.9177
[2024-03-10 19:36:50,071][main.py][line:124][INFO] [Epoch:5/20, Batch:149/320]: loss1:0.0446 loss2:0.2089 loss3:0.0000 | AUC:0.7664 Anomaly AUC:0.9264
[2024-03-10 19:37:05,048][main.py][line:124][INFO] [Epoch:5/20, Batch:159/320]: loss1:0.0161 loss2:0.2750 loss3:0.0000 | AUC:0.7854 Anomaly AUC:0.9291
[2024-03-10 19:37:19,645][main.py][line:124][INFO] [Epoch:5/20, Batch:169/320]: loss1:0.0120 loss2:0.2003 loss3:0.0000 | AUC:0.7686 Anomaly AUC:0.9235
[2024-03-10 19:37:33,839][main.py][line:124][INFO] [Epoch:5/20, Batch:179/320]: loss1:0.0108 loss2:0.2674 loss3:0.0000 | AUC:0.7810 Anomaly AUC:0.9285
[2024-03-10 19:37:48,093][main.py][line:124][INFO] [Epoch:5/20, Batch:189/320]: loss1:0.0839 loss2:0.2172 loss3:0.0000 | AUC:0.7671 Anomaly AUC:0.9314
[2024-03-10 19:38:02,516][main.py][line:124][INFO] [Epoch:5/20, Batch:199/320]: loss1:0.0191 loss2:0.2396 loss3:0.0000 | AUC:0.7790 Anomaly AUC:0.9285
[2024-03-10 19:38:16,700][main.py][line:124][INFO] [Epoch:5/20, Batch:209/320]: loss1:0.0804 loss2:0.2405 loss3:0.0000 | AUC:0.7933 Anomaly AUC:0.9315
[2024-03-10 19:38:30,736][main.py][line:124][INFO] [Epoch:5/20, Batch:219/320]: loss1:0.0418 loss2:0.2972 loss3:0.0000 | AUC:0.7601 Anomaly AUC:0.9230
[2024-03-10 19:38:45,533][main.py][line:124][INFO] [Epoch:5/20, Batch:229/320]: loss1:0.0177 loss2:0.3088 loss3:0.0000 | AUC:0.7928 Anomaly AUC:0.9322
[2024-03-10 19:38:59,734][main.py][line:124][INFO] [Epoch:5/20, Batch:239/320]: loss1:0.0870 loss2:0.3115 loss3:0.0000 | AUC:0.7809 Anomaly AUC:0.9306
[2024-03-10 19:39:13,938][main.py][line:124][INFO] [Epoch:5/20, Batch:249/320]: loss1:0.0390 loss2:0.2217 loss3:0.0000 | AUC:0.6980 Anomaly AUC:0.9022
[2024-03-10 19:39:28,401][main.py][line:124][INFO] [Epoch:5/20, Batch:259/320]: loss1:0.0209 loss2:0.2959 loss3:0.0000 | AUC:0.7774 Anomaly AUC:0.9243
[2024-03-10 19:39:43,136][main.py][line:124][INFO] [Epoch:5/20, Batch:269/320]: loss1:0.0225 loss2:0.1814 loss3:0.0000 | AUC:0.8114 Anomaly AUC:0.9373
[2024-03-10 19:39:57,364][main.py][line:124][INFO] [Epoch:5/20, Batch:279/320]: loss1:0.0307 loss2:0.2648 loss3:0.0000 | AUC:0.8069 Anomaly AUC:0.9367
[2024-03-10 19:40:11,555][main.py][line:124][INFO] [Epoch:5/20, Batch:289/320]: loss1:0.0126 loss2:0.2168 loss3:0.0000 | AUC:0.7842 Anomaly AUC:0.9304
[2024-03-10 19:40:27,182][main.py][line:153][INFO] [Epoch:5/20]: lr:0.00010 | loss1:0.0786 loss2:0.3147 loss3:0.0000 | AUC:0.7577 Anomaly AUC:0.9259
[2024-03-10 19:40:42,800][main.py][line:124][INFO] [Epoch:6/20, Batch:9/320]: loss1:0.0153 loss2:0.2304 loss3:0.0000 | AUC:0.7689 Anomaly AUC:0.9238
[2024-03-10 19:40:57,810][main.py][line:124][INFO] [Epoch:6/20, Batch:19/320]: loss1:0.0160 loss2:0.1815 loss3:0.0000 | AUC:0.7600 Anomaly AUC:0.9268
[2024-03-10 19:41:13,000][main.py][line:124][INFO] [Epoch:6/20, Batch:29/320]: loss1:0.0513 loss2:0.2066 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9249
[2024-03-10 19:41:27,464][main.py][line:124][INFO] [Epoch:6/20, Batch:39/320]: loss1:0.0101 loss2:0.2072 loss3:0.0000 | AUC:0.7813 Anomaly AUC:0.9301
[2024-03-10 19:41:42,422][main.py][line:124][INFO] [Epoch:6/20, Batch:49/320]: loss1:0.0070 loss2:0.2227 loss3:0.0000 | AUC:0.7596 Anomaly AUC:0.9236
[2024-03-10 19:41:57,880][main.py][line:124][INFO] [Epoch:6/20, Batch:59/320]: loss1:0.0283 loss2:0.2624 loss3:0.0000 | AUC:0.7932 Anomaly AUC:0.9360
[2024-03-10 19:42:12,951][main.py][line:124][INFO] [Epoch:6/20, Batch:69/320]: loss1:0.0120 loss2:0.2249 loss3:0.0000 | AUC:0.7941 Anomaly AUC:0.9358
[2024-03-10 19:42:27,314][main.py][line:124][INFO] [Epoch:6/20, Batch:79/320]: loss1:0.1195 loss2:0.2546 loss3:0.0000 | AUC:0.7893 Anomaly AUC:0.9304
[2024-03-10 19:42:42,460][main.py][line:124][INFO] [Epoch:6/20, Batch:89/320]: loss1:0.0127 loss2:0.2040 loss3:0.0000 | AUC:0.7897 Anomaly AUC:0.9332
[2024-03-10 19:42:57,794][main.py][line:124][INFO] [Epoch:6/20, Batch:99/320]: loss1:0.0205 loss2:0.2702 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9280
[2024-03-10 19:43:13,532][main.py][line:124][INFO] [Epoch:6/20, Batch:109/320]: loss1:0.0236 loss2:0.2248 loss3:0.0000 | AUC:0.7760 Anomaly AUC:0.9305
[2024-03-10 19:43:28,311][main.py][line:124][INFO] [Epoch:6/20, Batch:119/320]: loss1:0.0130 loss2:0.1792 loss3:0.0000 | AUC:0.7750 Anomaly AUC:0.9303
[2024-03-10 19:43:43,231][main.py][line:124][INFO] [Epoch:6/20, Batch:129/320]: loss1:0.0338 loss2:0.1460 loss3:0.0000 | AUC:0.7895 Anomaly AUC:0.9340
[2024-03-10 19:43:58,421][main.py][line:124][INFO] [Epoch:6/20, Batch:139/320]: loss1:0.0111 loss2:0.2352 loss3:0.0000 | AUC:0.7334 Anomaly AUC:0.9157
[2024-03-10 19:44:12,652][main.py][line:124][INFO] [Epoch:6/20, Batch:149/320]: loss1:0.0424 loss2:0.2650 loss3:0.0000 | AUC:0.7443 Anomaly AUC:0.9151
[2024-03-10 19:44:26,797][main.py][line:124][INFO] [Epoch:6/20, Batch:159/320]: loss1:0.0073 loss2:0.2055 loss3:0.0000 | AUC:0.7971 Anomaly AUC:0.9339
[2024-03-10 19:44:41,599][main.py][line:124][INFO] [Epoch:6/20, Batch:169/320]: loss1:0.0594 loss2:0.2056 loss3:0.0000 | AUC:0.7974 Anomaly AUC:0.9362
[2024-03-10 19:44:56,635][main.py][line:124][INFO] [Epoch:6/20, Batch:179/320]: loss1:0.0440 loss2:0.2598 loss3:0.0000 | AUC:0.7705 Anomaly AUC:0.9237
[2024-03-10 19:45:12,056][main.py][line:124][INFO] [Epoch:6/20, Batch:189/320]: loss1:0.0171 loss2:0.2736 loss3:0.0000 | AUC:0.7793 Anomaly AUC:0.9247
[2024-03-10 19:45:26,474][main.py][line:124][INFO] [Epoch:6/20, Batch:199/320]: loss1:0.0781 loss2:0.2094 loss3:0.0000 | AUC:0.7965 Anomaly AUC:0.9383
[2024-03-10 19:45:40,624][main.py][line:124][INFO] [Epoch:6/20, Batch:209/320]: loss1:0.1137 loss2:0.2974 loss3:0.0000 | AUC:0.7761 Anomaly AUC:0.9288
[2024-03-10 19:45:55,279][main.py][line:124][INFO] [Epoch:6/20, Batch:219/320]: loss1:0.0197 loss2:0.2416 loss3:0.0000 | AUC:0.7645 Anomaly AUC:0.9277
[2024-03-10 19:46:10,478][main.py][line:124][INFO] [Epoch:6/20, Batch:229/320]: loss1:0.0125 loss2:0.2577 loss3:0.0000 | AUC:0.7547 Anomaly AUC:0.9259
[2024-03-10 19:46:26,124][main.py][line:124][INFO] [Epoch:6/20, Batch:239/320]: loss1:0.0096 loss2:0.2311 loss3:0.0000 | AUC:0.7878 Anomaly AUC:0.9337
[2024-03-10 19:46:41,874][main.py][line:124][INFO] [Epoch:6/20, Batch:249/320]: loss1:0.0068 loss2:0.2015 loss3:0.0000 | AUC:0.7910 Anomaly AUC:0.9355
[2024-03-10 19:46:57,073][main.py][line:124][INFO] [Epoch:6/20, Batch:259/320]: loss1:0.0112 loss2:0.2197 loss3:0.0000 | AUC:0.7769 Anomaly AUC:0.9309
[2024-03-10 19:47:12,216][main.py][line:124][INFO] [Epoch:6/20, Batch:269/320]: loss1:0.0085 loss2:0.1671 loss3:0.0000 | AUC:0.7543 Anomaly AUC:0.9251
[2024-03-10 19:47:27,060][main.py][line:124][INFO] [Epoch:6/20, Batch:279/320]: loss1:0.0061 loss2:0.2338 loss3:0.0000 | AUC:0.7665 Anomaly AUC:0.9283
[2024-03-10 19:47:41,298][main.py][line:124][INFO] [Epoch:6/20, Batch:289/320]: loss1:0.0145 loss2:0.1926 loss3:0.0000 | AUC:0.7801 Anomaly AUC:0.9329
[2024-03-10 19:47:55,140][main.py][line:153][INFO] [Epoch:6/20]: lr:0.00010 | loss1:0.0059 loss2:0.1841 loss3:0.0000 | AUC:0.7822 Anomaly AUC:0.9330
[2024-03-10 19:48:10,773][main.py][line:124][INFO] [Epoch:7/20, Batch:9/320]: loss1:0.0209 loss2:0.2107 loss3:0.0000 | AUC:0.7821 Anomaly AUC:0.9334
[2024-03-10 19:48:24,800][main.py][line:124][INFO] [Epoch:7/20, Batch:19/320]: loss1:0.0043 loss2:0.1914 loss3:0.0000 | AUC:0.7764 Anomaly AUC:0.9305
[2024-03-10 19:48:39,608][main.py][line:124][INFO] [Epoch:7/20, Batch:29/320]: loss1:0.0048 loss2:0.1984 loss3:0.0000 | AUC:0.7757 Anomaly AUC:0.9306
[2024-03-10 19:48:54,175][main.py][line:124][INFO] [Epoch:7/20, Batch:39/320]: loss1:0.0096 loss2:0.2279 loss3:0.0000 | AUC:0.7725 Anomaly AUC:0.9292
[2024-03-10 19:49:08,460][main.py][line:124][INFO] [Epoch:7/20, Batch:49/320]: loss1:0.0083 loss2:0.1835 loss3:0.0000 | AUC:0.7551 Anomaly AUC:0.9246
[2024-03-10 19:49:23,121][main.py][line:124][INFO] [Epoch:7/20, Batch:59/320]: loss1:0.0384 loss2:0.1824 loss3:0.0000 | AUC:0.7730 Anomaly AUC:0.9309
[2024-03-10 19:49:37,425][main.py][line:124][INFO] [Epoch:7/20, Batch:69/320]: loss1:0.0047 loss2:0.1472 loss3:0.0000 | AUC:0.7847 Anomaly AUC:0.9333
[2024-03-10 19:49:52,001][main.py][line:124][INFO] [Epoch:7/20, Batch:79/320]: loss1:0.0127 loss2:0.1775 loss3:0.0000 | AUC:0.7330 Anomaly AUC:0.9205
[2024-03-10 19:50:08,113][main.py][line:124][INFO] [Epoch:7/20, Batch:89/320]: loss1:0.0095 loss2:0.1846 loss3:0.0000 | AUC:0.7471 Anomaly AUC:0.9221
[2024-03-10 19:50:22,368][main.py][line:124][INFO] [Epoch:7/20, Batch:99/320]: loss1:0.0070 loss2:0.2377 loss3:0.0000 | AUC:0.7914 Anomaly AUC:0.9333
[2024-03-10 19:50:36,451][main.py][line:124][INFO] [Epoch:7/20, Batch:109/320]: loss1:0.0131 loss2:0.1554 loss3:0.0000 | AUC:0.7975 Anomaly AUC:0.9345
[2024-03-10 19:50:51,791][main.py][line:124][INFO] [Epoch:7/20, Batch:119/320]: loss1:0.0074 loss2:0.2021 loss3:0.0000 | AUC:0.8010 Anomaly AUC:0.9353
[2024-03-10 19:51:06,029][main.py][line:124][INFO] [Epoch:7/20, Batch:129/320]: loss1:0.0381 loss2:0.1458 loss3:0.0000 | AUC:0.7700 Anomaly AUC:0.9251
[2024-03-10 19:51:20,640][main.py][line:124][INFO] [Epoch:7/20, Batch:139/320]: loss1:0.0063 loss2:0.1817 loss3:0.0000 | AUC:0.7675 Anomaly AUC:0.9264
[2024-03-10 19:51:34,944][main.py][line:124][INFO] [Epoch:7/20, Batch:149/320]: loss1:0.0261 loss2:0.2620 loss3:0.0000 | AUC:0.7798 Anomaly AUC:0.9271
[2024-03-10 19:51:49,165][main.py][line:124][INFO] [Epoch:7/20, Batch:159/320]: loss1:0.0089 loss2:0.1325 loss3:0.0000 | AUC:0.7968 Anomaly AUC:0.9373
[2024-03-10 19:52:05,009][main.py][line:124][INFO] [Epoch:7/20, Batch:169/320]: loss1:0.0056 loss2:0.1539 loss3:0.0000 | AUC:0.7638 Anomaly AUC:0.9233
[2024-03-10 19:52:19,482][main.py][line:124][INFO] [Epoch:7/20, Batch:179/320]: loss1:0.0071 loss2:0.1676 loss3:0.0000 | AUC:0.7921 Anomaly AUC:0.9358
[2024-03-10 19:52:34,008][main.py][line:124][INFO] [Epoch:7/20, Batch:189/320]: loss1:0.0075 loss2:0.1747 loss3:0.0000 | AUC:0.7998 Anomaly AUC:0.9377
[2024-03-10 19:52:48,305][main.py][line:124][INFO] [Epoch:7/20, Batch:199/320]: loss1:0.0092 loss2:0.1217 loss3:0.0000 | AUC:0.7827 Anomaly AUC:0.9323
[2024-03-10 19:53:02,476][main.py][line:124][INFO] [Epoch:7/20, Batch:209/320]: loss1:0.0049 loss2:0.1743 loss3:0.0000 | AUC:0.7534 Anomaly AUC:0.9245
[2024-03-10 19:53:17,149][main.py][line:124][INFO] [Epoch:7/20, Batch:219/320]: loss1:0.0043 loss2:0.2293 loss3:0.0000 | AUC:0.7708 Anomaly AUC:0.9293
[2024-03-10 19:53:31,252][main.py][line:124][INFO] [Epoch:7/20, Batch:229/320]: loss1:0.0092 loss2:0.1294 loss3:0.0000 | AUC:0.7804 Anomaly AUC:0.9329
[2024-03-10 19:53:45,874][main.py][line:124][INFO] [Epoch:7/20, Batch:239/320]: loss1:0.0530 loss2:0.1568 loss3:0.0000 | AUC:0.8180 Anomaly AUC:0.9413
[2024-03-10 19:54:00,079][main.py][line:124][INFO] [Epoch:7/20, Batch:249/320]: loss1:0.0715 loss2:0.2375 loss3:0.0000 | AUC:0.7432 Anomaly AUC:0.9242
[2024-03-10 19:54:14,395][main.py][line:124][INFO] [Epoch:7/20, Batch:259/320]: loss1:0.0349 loss2:0.1552 loss3:0.0000 | AUC:0.8260 Anomaly AUC:0.9441
[2024-03-10 19:54:29,619][main.py][line:124][INFO] [Epoch:7/20, Batch:269/320]: loss1:0.0308 loss2:0.2621 loss3:0.0000 | AUC:0.7967 Anomaly AUC:0.9302
[2024-03-10 19:54:43,779][main.py][line:124][INFO] [Epoch:7/20, Batch:279/320]: loss1:0.0228 loss2:0.1637 loss3:0.0000 | AUC:0.7492 Anomaly AUC:0.9276
[2024-03-10 19:54:58,468][main.py][line:124][INFO] [Epoch:7/20, Batch:289/320]: loss1:0.0135 loss2:0.2063 loss3:0.0000 | AUC:0.7590 Anomaly AUC:0.9329
[2024-03-10 19:55:12,292][main.py][line:153][INFO] [Epoch:7/20]: lr:0.00010 | loss1:0.0744 loss2:0.1766 loss3:0.0000 | AUC:0.7378 Anomaly AUC:0.9214
[2024-03-10 19:55:26,814][main.py][line:124][INFO] [Epoch:8/20, Batch:9/320]: loss1:0.0268 loss2:0.1393 loss3:0.0000 | AUC:0.7410 Anomaly AUC:0.9259
[2024-03-10 19:55:42,534][main.py][line:124][INFO] [Epoch:8/20, Batch:19/320]: loss1:0.0179 loss2:0.1946 loss3:0.0000 | AUC:0.7641 Anomaly AUC:0.9275
[2024-03-10 19:55:56,988][main.py][line:124][INFO] [Epoch:8/20, Batch:29/320]: loss1:0.0080 loss2:0.1032 loss3:0.0000 | AUC:0.7493 Anomaly AUC:0.9241
[2024-03-10 19:56:11,441][main.py][line:124][INFO] [Epoch:8/20, Batch:39/320]: loss1:0.0627 loss2:0.1952 loss3:0.0000 | AUC:0.7644 Anomaly AUC:0.9265
[2024-03-10 19:56:25,803][main.py][line:124][INFO] [Epoch:8/20, Batch:49/320]: loss1:0.0308 loss2:0.2202 loss3:0.0000 | AUC:0.7750 Anomaly AUC:0.9286
[2024-03-10 19:56:40,241][main.py][line:124][INFO] [Epoch:8/20, Batch:59/320]: loss1:0.0171 loss2:0.1989 loss3:0.0000 | AUC:0.7866 Anomaly AUC:0.9338
[2024-03-10 19:56:55,483][main.py][line:124][INFO] [Epoch:8/20, Batch:69/320]: loss1:0.0120 loss2:0.1544 loss3:0.0000 | AUC:0.7929 Anomaly AUC:0.9338
[2024-03-10 19:57:09,643][main.py][line:124][INFO] [Epoch:8/20, Batch:79/320]: loss1:0.0181 loss2:0.1363 loss3:0.0000 | AUC:0.7756 Anomaly AUC:0.9333
[2024-03-10 19:57:24,446][main.py][line:124][INFO] [Epoch:8/20, Batch:89/320]: loss1:0.0119 loss2:0.1540 loss3:0.0000 | AUC:0.7472 Anomaly AUC:0.9246
[2024-03-10 19:57:38,762][main.py][line:124][INFO] [Epoch:8/20, Batch:99/320]: loss1:0.0166 loss2:0.2218 loss3:0.0000 | AUC:0.7111 Anomaly AUC:0.9095
[2024-03-10 19:57:53,115][main.py][line:124][INFO] [Epoch:8/20, Batch:109/320]: loss1:0.0084 loss2:0.1977 loss3:0.0000 | AUC:0.7950 Anomaly AUC:0.9335
[2024-03-10 19:58:07,979][main.py][line:124][INFO] [Epoch:8/20, Batch:119/320]: loss1:0.0790 loss2:0.3143 loss3:0.0000 | AUC:0.7963 Anomaly AUC:0.9364
[2024-03-10 19:58:22,322][main.py][line:124][INFO] [Epoch:8/20, Batch:129/320]: loss1:0.0114 loss2:0.2302 loss3:0.0000 | AUC:0.7793 Anomaly AUC:0.9318
[2024-03-10 19:58:36,948][main.py][line:124][INFO] [Epoch:8/20, Batch:139/320]: loss1:0.0096 loss2:0.1239 loss3:0.0000 | AUC:0.7604 Anomaly AUC:0.9312
[2024-03-10 19:58:51,043][main.py][line:124][INFO] [Epoch:8/20, Batch:149/320]: loss1:0.0331 loss2:0.1687 loss3:0.0000 | AUC:0.7692 Anomaly AUC:0.9322
[2024-03-10 19:59:05,289][main.py][line:124][INFO] [Epoch:8/20, Batch:159/320]: loss1:0.0226 loss2:0.1784 loss3:0.0000 | AUC:0.7628 Anomaly AUC:0.9293
[2024-03-10 19:59:20,981][main.py][line:124][INFO] [Epoch:8/20, Batch:169/320]: loss1:0.0231 loss2:0.1229 loss3:0.0000 | AUC:0.7879 Anomaly AUC:0.9326
[2024-03-10 19:59:35,230][main.py][line:124][INFO] [Epoch:8/20, Batch:179/320]: loss1:0.0104 loss2:0.1826 loss3:0.0000 | AUC:0.7702 Anomaly AUC:0.9274
[2024-03-10 19:59:49,933][main.py][line:124][INFO] [Epoch:8/20, Batch:189/320]: loss1:0.0262 loss2:0.1601 loss3:0.0000 | AUC:0.7633 Anomaly AUC:0.9269
[2024-03-10 20:00:04,343][main.py][line:124][INFO] [Epoch:8/20, Batch:199/320]: loss1:0.0156 loss2:0.1196 loss3:0.0000 | AUC:0.7874 Anomaly AUC:0.9376
[2024-03-10 20:00:18,759][main.py][line:124][INFO] [Epoch:8/20, Batch:209/320]: loss1:0.0099 loss2:0.1778 loss3:0.0000 | AUC:0.7851 Anomaly AUC:0.9348
[2024-03-10 20:00:34,076][main.py][line:124][INFO] [Epoch:8/20, Batch:219/320]: loss1:0.0530 loss2:0.2147 loss3:0.0000 | AUC:0.7785 Anomaly AUC:0.9332
[2024-03-10 20:00:48,364][main.py][line:124][INFO] [Epoch:8/20, Batch:229/320]: loss1:0.0057 loss2:0.1224 loss3:0.0000 | AUC:0.8058 Anomaly AUC:0.9398
[2024-03-10 20:01:03,317][main.py][line:124][INFO] [Epoch:8/20, Batch:239/320]: loss1:0.0089 loss2:0.1600 loss3:0.0000 | AUC:0.8046 Anomaly AUC:0.9389
[2024-03-10 20:01:17,687][main.py][line:124][INFO] [Epoch:8/20, Batch:249/320]: loss1:0.0109 loss2:0.1834 loss3:0.0000 | AUC:0.8137 Anomaly AUC:0.9380
[2024-03-10 20:01:31,776][main.py][line:124][INFO] [Epoch:8/20, Batch:259/320]: loss1:0.0479 loss2:0.1319 loss3:0.0000 | AUC:0.7899 Anomaly AUC:0.9389
[2024-03-10 20:01:46,975][main.py][line:124][INFO] [Epoch:8/20, Batch:269/320]: loss1:0.0089 loss2:0.1305 loss3:0.0000 | AUC:0.7286 Anomaly AUC:0.9281
[2024-03-10 20:02:01,076][main.py][line:124][INFO] [Epoch:8/20, Batch:279/320]: loss1:0.0166 loss2:0.1804 loss3:0.0000 | AUC:0.7423 Anomaly AUC:0.9251
[2024-03-10 20:02:15,490][main.py][line:124][INFO] [Epoch:8/20, Batch:289/320]: loss1:0.0111 loss2:0.1175 loss3:0.0000 | AUC:0.8070 Anomaly AUC:0.9385
[2024-03-10 20:02:33,781][main.py][line:153][INFO] [Epoch:8/20]: lr:0.00010 | loss1:0.0144 loss2:0.1272 loss3:0.0000 | AUC:0.8136 Anomaly AUC:0.9400
[2024-03-10 20:02:49,641][main.py][line:124][INFO] [Epoch:9/20, Batch:9/320]: loss1:0.0074 loss2:0.1193 loss3:0.0000 | AUC:0.8244 Anomaly AUC:0.9428
[2024-03-10 20:03:04,934][main.py][line:124][INFO] [Epoch:9/20, Batch:19/320]: loss1:0.0091 loss2:0.1220 loss3:0.0000 | AUC:0.8118 Anomaly AUC:0.9420
[2024-03-10 20:03:19,375][main.py][line:124][INFO] [Epoch:9/20, Batch:29/320]: loss1:0.0139 loss2:0.1307 loss3:0.0000 | AUC:0.7969 Anomaly AUC:0.9384
[2024-03-10 20:03:33,766][main.py][line:124][INFO] [Epoch:9/20, Batch:39/320]: loss1:0.0064 loss2:0.1380 loss3:0.0000 | AUC:0.7885 Anomaly AUC:0.9332
[2024-03-10 20:03:48,412][main.py][line:124][INFO] [Epoch:9/20, Batch:49/320]: loss1:0.0063 loss2:0.1289 loss3:0.0000 | AUC:0.7635 Anomaly AUC:0.9289
[2024-03-10 20:04:02,558][main.py][line:124][INFO] [Epoch:9/20, Batch:59/320]: loss1:0.0107 loss2:0.1387 loss3:0.0000 | AUC:0.7312 Anomaly AUC:0.9218
[2024-03-10 20:04:16,871][main.py][line:124][INFO] [Epoch:9/20, Batch:69/320]: loss1:0.0216 loss2:0.2126 loss3:0.0000 | AUC:0.7954 Anomaly AUC:0.9369
[2024-03-10 20:04:31,652][main.py][line:124][INFO] [Epoch:9/20, Batch:79/320]: loss1:0.0087 loss2:0.1704 loss3:0.0000 | AUC:0.7946 Anomaly AUC:0.9354
[2024-03-10 20:04:46,742][main.py][line:124][INFO] [Epoch:9/20, Batch:89/320]: loss1:0.0091 loss2:0.1825 loss3:0.0000 | AUC:0.7808 Anomaly AUC:0.9312
[2024-03-10 20:05:01,955][main.py][line:124][INFO] [Epoch:9/20, Batch:99/320]: loss1:0.0181 loss2:0.1524 loss3:0.0000 | AUC:0.7911 Anomaly AUC:0.9326
[2024-03-10 20:05:16,946][main.py][line:124][INFO] [Epoch:9/20, Batch:109/320]: loss1:0.0594 loss2:0.1832 loss3:0.0000 | AUC:0.7690 Anomaly AUC:0.9328
[2024-03-10 20:05:31,923][main.py][line:124][INFO] [Epoch:9/20, Batch:119/320]: loss1:0.0055 loss2:0.1188 loss3:0.0000 | AUC:0.7590 Anomaly AUC:0.9266
[2024-03-10 20:05:47,531][main.py][line:124][INFO] [Epoch:9/20, Batch:129/320]: loss1:0.0065 loss2:0.1216 loss3:0.0000 | AUC:0.7591 Anomaly AUC:0.9281
[2024-03-10 20:06:02,446][main.py][line:124][INFO] [Epoch:9/20, Batch:139/320]: loss1:0.0081 loss2:0.1205 loss3:0.0000 | AUC:0.7795 Anomaly AUC:0.9360
[2024-03-10 20:06:17,575][main.py][line:124][INFO] [Epoch:9/20, Batch:149/320]: loss1:0.0086 loss2:0.1027 loss3:0.0000 | AUC:0.8046 Anomaly AUC:0.9416
[2024-03-10 20:06:31,939][main.py][line:124][INFO] [Epoch:9/20, Batch:159/320]: loss1:0.0176 loss2:0.1560 loss3:0.0000 | AUC:0.8030 Anomaly AUC:0.9402
[2024-03-10 20:06:46,074][main.py][line:124][INFO] [Epoch:9/20, Batch:169/320]: loss1:0.0064 loss2:0.1361 loss3:0.0000 | AUC:0.7867 Anomaly AUC:0.9345
[2024-03-10 20:07:01,063][main.py][line:124][INFO] [Epoch:9/20, Batch:179/320]: loss1:0.0062 loss2:0.1734 loss3:0.0000 | AUC:0.7838 Anomaly AUC:0.9339
[2024-03-10 20:07:16,620][main.py][line:124][INFO] [Epoch:9/20, Batch:189/320]: loss1:0.0086 loss2:0.1301 loss3:0.0000 | AUC:0.7949 Anomaly AUC:0.9384
[2024-03-10 20:07:31,882][main.py][line:124][INFO] [Epoch:9/20, Batch:199/320]: loss1:0.0196 loss2:0.0947 loss3:0.0000 | AUC:0.7772 Anomaly AUC:0.9364
[2024-03-10 20:07:47,697][main.py][line:124][INFO] [Epoch:9/20, Batch:209/320]: loss1:0.1703 loss2:0.2330 loss3:0.0000 | AUC:0.7626 Anomaly AUC:0.9345
[2024-03-10 20:08:02,642][main.py][line:124][INFO] [Epoch:9/20, Batch:219/320]: loss1:0.0223 loss2:0.1396 loss3:0.0000 | AUC:0.7745 Anomaly AUC:0.9315
[2024-03-10 20:08:16,929][main.py][line:124][INFO] [Epoch:9/20, Batch:229/320]: loss1:0.0160 loss2:0.1170 loss3:0.0000 | AUC:0.8105 Anomaly AUC:0.9417
[2024-03-10 20:08:31,229][main.py][line:124][INFO] [Epoch:9/20, Batch:239/320]: loss1:0.0999 loss2:0.1893 loss3:0.0000 | AUC:0.7987 Anomaly AUC:0.9396
[2024-03-10 20:08:45,934][main.py][line:124][INFO] [Epoch:9/20, Batch:249/320]: loss1:0.0090 loss2:0.1813 loss3:0.0000 | AUC:0.7417 Anomaly AUC:0.9221
[2024-03-10 20:09:01,524][main.py][line:124][INFO] [Epoch:9/20, Batch:259/320]: loss1:0.0782 loss2:0.2493 loss3:0.0000 | AUC:0.7885 Anomaly AUC:0.9355
[2024-03-10 20:09:16,624][main.py][line:124][INFO] [Epoch:9/20, Batch:269/320]: loss1:0.0186 loss2:0.1675 loss3:0.0000 | AUC:0.7557 Anomaly AUC:0.9265
[2024-03-10 20:09:31,598][main.py][line:124][INFO] [Epoch:9/20, Batch:279/320]: loss1:0.0221 loss2:0.1022 loss3:0.0000 | AUC:0.7607 Anomaly AUC:0.9265
[2024-03-10 20:09:46,695][main.py][line:124][INFO] [Epoch:9/20, Batch:289/320]: loss1:0.0085 loss2:0.1279 loss3:0.0000 | AUC:0.8218 Anomaly AUC:0.9405
[2024-03-10 20:10:03,791][main.py][line:153][INFO] [Epoch:9/20]: lr:0.00010 | loss1:0.0098 loss2:0.2038 loss3:0.0000 | AUC:0.8212 Anomaly AUC:0.9414
[2024-03-10 20:10:19,492][main.py][line:124][INFO] [Epoch:10/20, Batch:9/320]: loss1:0.0081 loss2:0.1652 loss3:0.0000 | AUC:0.7933 Anomaly AUC:0.9347
[2024-03-10 20:10:33,667][main.py][line:124][INFO] [Epoch:10/20, Batch:19/320]: loss1:0.0100 loss2:0.1087 loss3:0.0000 | AUC:0.7956 Anomaly AUC:0.9364
[2024-03-10 20:10:47,823][main.py][line:124][INFO] [Epoch:10/20, Batch:29/320]: loss1:0.0098 loss2:0.1368 loss3:0.0000 | AUC:0.7950 Anomaly AUC:0.9362
[2024-03-10 20:11:02,909][main.py][line:124][INFO] [Epoch:10/20, Batch:39/320]: loss1:0.0072 loss2:0.1289 loss3:0.0000 | AUC:0.8012 Anomaly AUC:0.9374
[2024-03-10 20:11:18,461][main.py][line:124][INFO] [Epoch:10/20, Batch:49/320]: loss1:0.0052 loss2:0.1701 loss3:0.0000 | AUC:0.7635 Anomaly AUC:0.9310
[2024-03-10 20:11:32,525][main.py][line:124][INFO] [Epoch:10/20, Batch:59/320]: loss1:0.0068 loss2:0.0829 loss3:0.0000 | AUC:0.7580 Anomaly AUC:0.9306
[2024-03-10 20:11:46,793][main.py][line:124][INFO] [Epoch:10/20, Batch:69/320]: loss1:0.0051 loss2:0.1078 loss3:0.0000 | AUC:0.7587 Anomaly AUC:0.9296
[2024-03-10 20:12:02,206][main.py][line:124][INFO] [Epoch:10/20, Batch:79/320]: loss1:0.0064 loss2:0.0846 loss3:0.0000 | AUC:0.8162 Anomaly AUC:0.9421
[2024-03-10 20:12:17,329][main.py][line:124][INFO] [Epoch:10/20, Batch:89/320]: loss1:0.0072 loss2:0.1747 loss3:0.0000 | AUC:0.8208 Anomaly AUC:0.9420
[2024-03-10 20:12:31,662][main.py][line:124][INFO] [Epoch:10/20, Batch:99/320]: loss1:0.0306 loss2:0.0963 loss3:0.0000 | AUC:0.8217 Anomaly AUC:0.9419
[2024-03-10 20:12:45,833][main.py][line:124][INFO] [Epoch:10/20, Batch:109/320]: loss1:0.0107 loss2:0.1128 loss3:0.0000 | AUC:0.8185 Anomaly AUC:0.9431
[2024-03-10 20:13:00,490][main.py][line:124][INFO] [Epoch:10/20, Batch:119/320]: loss1:0.0072 loss2:0.0832 loss3:0.0000 | AUC:0.7824 Anomaly AUC:0.9339
[2024-03-10 20:13:16,142][main.py][line:124][INFO] [Epoch:10/20, Batch:129/320]: loss1:0.0063 loss2:0.1602 loss3:0.0000 | AUC:0.8114 Anomaly AUC:0.9425
[2024-03-10 20:13:30,178][main.py][line:124][INFO] [Epoch:10/20, Batch:139/320]: loss1:0.0164 loss2:0.1060 loss3:0.0000 | AUC:0.7565 Anomaly AUC:0.9291
[2024-03-10 20:13:44,207][main.py][line:124][INFO] [Epoch:10/20, Batch:149/320]: loss1:0.0165 loss2:0.1482 loss3:0.0000 | AUC:0.7693 Anomaly AUC:0.9350
[2024-03-10 20:13:59,644][main.py][line:124][INFO] [Epoch:10/20, Batch:159/320]: loss1:0.1198 loss2:0.2217 loss3:0.0000 | AUC:0.7498 Anomaly AUC:0.9299
[2024-03-10 20:14:15,186][main.py][line:124][INFO] [Epoch:10/20, Batch:169/320]: loss1:0.0154 loss2:0.1179 loss3:0.0000 | AUC:0.7726 Anomaly AUC:0.9344
[2024-03-10 20:14:31,189][main.py][line:124][INFO] [Epoch:10/20, Batch:179/320]: loss1:0.0282 loss2:0.1502 loss3:0.0000 | AUC:0.8137 Anomaly AUC:0.9414
[2024-03-10 20:14:46,866][main.py][line:124][INFO] [Epoch:10/20, Batch:189/320]: loss1:0.0119 loss2:0.1028 loss3:0.0000 | AUC:0.7712 Anomaly AUC:0.9337
[2024-03-10 20:15:01,170][main.py][line:124][INFO] [Epoch:10/20, Batch:199/320]: loss1:0.0117 loss2:0.1264 loss3:0.0000 | AUC:0.7744 Anomaly AUC:0.9339
[2024-03-10 20:15:15,797][main.py][line:124][INFO] [Epoch:10/20, Batch:209/320]: loss1:0.0047 loss2:0.0931 loss3:0.0000 | AUC:0.7850 Anomaly AUC:0.9349
[2024-03-10 20:15:31,342][main.py][line:124][INFO] [Epoch:10/20, Batch:219/320]: loss1:0.0078 loss2:0.1202 loss3:0.0000 | AUC:0.8000 Anomaly AUC:0.9373
[2024-03-10 20:15:46,857][main.py][line:124][INFO] [Epoch:10/20, Batch:229/320]: loss1:0.0074 loss2:0.1255 loss3:0.0000 | AUC:0.7803 Anomaly AUC:0.9346
[2024-03-10 20:16:01,123][main.py][line:124][INFO] [Epoch:10/20, Batch:239/320]: loss1:0.0081 loss2:0.1752 loss3:0.0000 | AUC:0.7675 Anomaly AUC:0.9295
[2024-03-10 20:16:15,304][main.py][line:124][INFO] [Epoch:10/20, Batch:249/320]: loss1:0.0091 loss2:0.1337 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9367
[2024-03-10 20:16:30,285][main.py][line:124][INFO] [Epoch:10/20, Batch:259/320]: loss1:0.0154 loss2:0.1388 loss3:0.0000 | AUC:0.7793 Anomaly AUC:0.9347
[2024-03-10 20:16:44,587][main.py][line:124][INFO] [Epoch:10/20, Batch:269/320]: loss1:0.0682 loss2:0.1599 loss3:0.0000 | AUC:0.7467 Anomaly AUC:0.9269
[2024-03-10 20:16:58,835][main.py][line:124][INFO] [Epoch:10/20, Batch:279/320]: loss1:0.0294 loss2:0.1567 loss3:0.0000 | AUC:0.7476 Anomaly AUC:0.9317
[2024-03-10 20:17:13,648][main.py][line:124][INFO] [Epoch:10/20, Batch:289/320]: loss1:0.0584 loss2:0.1664 loss3:0.0000 | AUC:0.7012 Anomaly AUC:0.9117
[2024-03-10 20:17:31,119][main.py][line:153][INFO] [Epoch:10/20]: lr:0.00010 | loss1:0.0082 loss2:0.1565 loss3:0.0000 | AUC:0.7951 Anomaly AUC:0.9344
[2024-03-10 20:17:47,295][main.py][line:124][INFO] [Epoch:11/20, Batch:9/320]: loss1:0.0146 loss2:0.1018 loss3:0.0000 | AUC:0.7331 Anomaly AUC:0.9256
[2024-03-10 20:18:01,713][main.py][line:124][INFO] [Epoch:11/20, Batch:19/320]: loss1:0.0102 loss2:0.1680 loss3:0.0000 | AUC:0.7360 Anomaly AUC:0.9248
[2024-03-10 20:18:15,918][main.py][line:124][INFO] [Epoch:11/20, Batch:29/320]: loss1:0.0521 loss2:0.1481 loss3:0.0000 | AUC:0.7063 Anomaly AUC:0.9246
[2024-03-10 20:18:31,491][main.py][line:124][INFO] [Epoch:11/20, Batch:39/320]: loss1:0.0182 loss2:0.1906 loss3:0.0000 | AUC:0.7351 Anomaly AUC:0.9288
[2024-03-10 20:18:46,933][main.py][line:124][INFO] [Epoch:11/20, Batch:49/320]: loss1:0.0166 loss2:0.0859 loss3:0.0000 | AUC:0.7141 Anomaly AUC:0.9211
[2024-03-10 20:19:02,040][main.py][line:124][INFO] [Epoch:11/20, Batch:59/320]: loss1:0.0894 loss2:0.2124 loss3:0.0000 | AUC:0.7899 Anomaly AUC:0.9391
[2024-03-10 20:19:17,387][main.py][line:124][INFO] [Epoch:11/20, Batch:69/320]: loss1:0.0124 loss2:0.1115 loss3:0.0000 | AUC:0.8246 Anomaly AUC:0.9467
[2024-03-10 20:19:32,429][main.py][line:124][INFO] [Epoch:11/20, Batch:79/320]: loss1:0.0082 loss2:0.1109 loss3:0.0000 | AUC:0.7910 Anomaly AUC:0.9380
[2024-03-10 20:19:47,668][main.py][line:124][INFO] [Epoch:11/20, Batch:89/320]: loss1:0.0042 loss2:0.0944 loss3:0.0000 | AUC:0.8145 Anomaly AUC:0.9425
[2024-03-10 20:20:02,435][main.py][line:124][INFO] [Epoch:11/20, Batch:99/320]: loss1:0.0181 loss2:0.1590 loss3:0.0000 | AUC:0.7809 Anomaly AUC:0.9331
[2024-03-10 20:20:17,565][main.py][line:124][INFO] [Epoch:11/20, Batch:109/320]: loss1:0.0105 loss2:0.1098 loss3:0.0000 | AUC:0.7838 Anomaly AUC:0.9332
[2024-03-10 20:20:32,465][main.py][line:124][INFO] [Epoch:11/20, Batch:119/320]: loss1:0.0094 loss2:0.0750 loss3:0.0000 | AUC:0.8029 Anomaly AUC:0.9400
[2024-03-10 20:20:47,286][main.py][line:124][INFO] [Epoch:11/20, Batch:129/320]: loss1:0.0140 loss2:0.1569 loss3:0.0000 | AUC:0.8121 Anomaly AUC:0.9416
[2024-03-10 20:21:02,615][main.py][line:124][INFO] [Epoch:11/20, Batch:139/320]: loss1:0.1077 loss2:0.1624 loss3:0.0000 | AUC:0.8144 Anomaly AUC:0.9424
[2024-03-10 20:21:17,853][main.py][line:124][INFO] [Epoch:11/20, Batch:149/320]: loss1:0.0082 loss2:0.0908 loss3:0.0000 | AUC:0.8278 Anomaly AUC:0.9458
[2024-03-10 20:21:32,968][main.py][line:124][INFO] [Epoch:11/20, Batch:159/320]: loss1:0.0586 loss2:0.1422 loss3:0.0000 | AUC:0.8060 Anomaly AUC:0.9405
[2024-03-10 20:21:47,775][main.py][line:124][INFO] [Epoch:11/20, Batch:169/320]: loss1:0.0244 loss2:0.0976 loss3:0.0000 | AUC:0.8281 Anomaly AUC:0.9449
[2024-03-10 20:22:02,553][main.py][line:124][INFO] [Epoch:11/20, Batch:179/320]: loss1:0.0731 loss2:0.2272 loss3:0.0000 | AUC:0.7685 Anomaly AUC:0.9291
[2024-03-10 20:22:17,963][main.py][line:124][INFO] [Epoch:11/20, Batch:189/320]: loss1:0.0629 loss2:0.1327 loss3:0.0000 | AUC:0.7888 Anomaly AUC:0.9377
[2024-03-10 20:22:32,835][main.py][line:124][INFO] [Epoch:11/20, Batch:199/320]: loss1:0.0241 loss2:0.0713 loss3:0.0000 | AUC:0.8014 Anomaly AUC:0.9407
[2024-03-10 20:22:48,297][main.py][line:124][INFO] [Epoch:11/20, Batch:209/320]: loss1:0.0509 loss2:0.0987 loss3:0.0000 | AUC:0.8074 Anomaly AUC:0.9428
[2024-03-10 20:23:02,552][main.py][line:124][INFO] [Epoch:11/20, Batch:219/320]: loss1:0.0194 loss2:0.1166 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9358
[2024-03-10 20:23:16,864][main.py][line:124][INFO] [Epoch:11/20, Batch:229/320]: loss1:0.0306 loss2:0.1184 loss3:0.0000 | AUC:0.7808 Anomaly AUC:0.9327
[2024-03-10 20:23:31,693][main.py][line:124][INFO] [Epoch:11/20, Batch:239/320]: loss1:0.0063 loss2:0.1274 loss3:0.0000 | AUC:0.7966 Anomaly AUC:0.9379
[2024-03-10 20:23:46,055][main.py][line:124][INFO] [Epoch:11/20, Batch:249/320]: loss1:0.1061 loss2:0.1538 loss3:0.0000 | AUC:0.8334 Anomaly AUC:0.9469
[2024-03-10 20:24:00,166][main.py][line:124][INFO] [Epoch:11/20, Batch:259/320]: loss1:0.0069 loss2:0.1841 loss3:0.0000 | AUC:0.8190 Anomaly AUC:0.9401
[2024-03-10 20:24:15,246][main.py][line:124][INFO] [Epoch:11/20, Batch:269/320]: loss1:0.0266 loss2:0.1451 loss3:0.0000 | AUC:0.8346 Anomaly AUC:0.9447
[2024-03-10 20:24:29,545][main.py][line:124][INFO] [Epoch:11/20, Batch:279/320]: loss1:0.0158 loss2:0.1558 loss3:0.0000 | AUC:0.8226 Anomaly AUC:0.9426
[2024-03-10 20:24:43,699][main.py][line:124][INFO] [Epoch:11/20, Batch:289/320]: loss1:0.0102 loss2:0.1533 loss3:0.0000 | AUC:0.8148 Anomaly AUC:0.9437
[2024-03-10 20:25:00,103][main.py][line:153][INFO] [Epoch:11/20]: lr:0.00010 | loss1:0.0110 loss2:0.1249 loss3:0.0000 | AUC:0.8206 Anomaly AUC:0.9448
[2024-03-10 20:25:14,595][main.py][line:124][INFO] [Epoch:12/20, Batch:9/320]: loss1:0.0054 loss2:0.1086 loss3:0.0000 | AUC:0.7896 Anomaly AUC:0.9380
[2024-03-10 20:25:28,661][main.py][line:124][INFO] [Epoch:12/20, Batch:19/320]: loss1:0.0058 loss2:0.1140 loss3:0.0000 | AUC:0.7796 Anomaly AUC:0.9355
[2024-03-10 20:25:42,656][main.py][line:124][INFO] [Epoch:12/20, Batch:29/320]: loss1:0.0661 loss2:0.1000 loss3:0.0000 | AUC:0.8051 Anomaly AUC:0.9416
[2024-03-10 20:25:56,765][main.py][line:124][INFO] [Epoch:12/20, Batch:39/320]: loss1:0.0089 loss2:0.1007 loss3:0.0000 | AUC:0.8064 Anomaly AUC:0.9387
[2024-03-10 20:26:11,627][main.py][line:124][INFO] [Epoch:12/20, Batch:49/320]: loss1:0.0054 loss2:0.1298 loss3:0.0000 | AUC:0.8191 Anomaly AUC:0.9420
[2024-03-10 20:26:26,669][main.py][line:124][INFO] [Epoch:12/20, Batch:59/320]: loss1:0.0056 loss2:0.0793 loss3:0.0000 | AUC:0.8039 Anomaly AUC:0.9396
[2024-03-10 20:26:41,652][main.py][line:124][INFO] [Epoch:12/20, Batch:69/320]: loss1:0.0055 loss2:0.1221 loss3:0.0000 | AUC:0.7982 Anomaly AUC:0.9375
[2024-03-10 20:26:56,176][main.py][line:124][INFO] [Epoch:12/20, Batch:79/320]: loss1:0.0137 loss2:0.1003 loss3:0.0000 | AUC:0.8303 Anomaly AUC:0.9463
[2024-03-10 20:27:11,161][main.py][line:124][INFO] [Epoch:12/20, Batch:89/320]: loss1:0.0114 loss2:0.0651 loss3:0.0000 | AUC:0.8134 Anomaly AUC:0.9427
[2024-03-10 20:27:26,212][main.py][line:124][INFO] [Epoch:12/20, Batch:99/320]: loss1:0.0050 loss2:0.1058 loss3:0.0000 | AUC:0.7837 Anomaly AUC:0.9357
[2024-03-10 20:27:41,289][main.py][line:124][INFO] [Epoch:12/20, Batch:109/320]: loss1:0.0081 loss2:0.0781 loss3:0.0000 | AUC:0.7857 Anomaly AUC:0.9343
[2024-03-10 20:27:56,230][main.py][line:124][INFO] [Epoch:12/20, Batch:119/320]: loss1:0.0071 loss2:0.1027 loss3:0.0000 | AUC:0.7962 Anomaly AUC:0.9370
[2024-03-10 20:28:11,180][main.py][line:124][INFO] [Epoch:12/20, Batch:129/320]: loss1:0.0099 loss2:0.1365 loss3:0.0000 | AUC:0.7904 Anomaly AUC:0.9357
[2024-03-10 20:28:25,721][main.py][line:124][INFO] [Epoch:12/20, Batch:139/320]: loss1:0.0057 loss2:0.1005 loss3:0.0000 | AUC:0.7947 Anomaly AUC:0.9364
[2024-03-10 20:28:40,274][main.py][line:124][INFO] [Epoch:12/20, Batch:149/320]: loss1:0.0075 loss2:0.0914 loss3:0.0000 | AUC:0.8064 Anomaly AUC:0.9390
[2024-03-10 20:28:55,197][main.py][line:124][INFO] [Epoch:12/20, Batch:159/320]: loss1:0.0079 loss2:0.1058 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9399
[2024-03-10 20:29:09,988][main.py][line:124][INFO] [Epoch:12/20, Batch:169/320]: loss1:0.0081 loss2:0.1210 loss3:0.0000 | AUC:0.8048 Anomaly AUC:0.9384
[2024-03-10 20:29:25,189][main.py][line:124][INFO] [Epoch:12/20, Batch:179/320]: loss1:0.0078 loss2:0.1129 loss3:0.0000 | AUC:0.8084 Anomaly AUC:0.9397
[2024-03-10 20:29:39,975][main.py][line:124][INFO] [Epoch:12/20, Batch:189/320]: loss1:0.0059 loss2:0.0792 loss3:0.0000 | AUC:0.8032 Anomaly AUC:0.9387
[2024-03-10 20:29:55,099][main.py][line:124][INFO] [Epoch:12/20, Batch:199/320]: loss1:0.0114 loss2:0.1471 loss3:0.0000 | AUC:0.7915 Anomaly AUC:0.9368
[2024-03-10 20:30:10,055][main.py][line:124][INFO] [Epoch:12/20, Batch:209/320]: loss1:0.0080 loss2:0.1108 loss3:0.0000 | AUC:0.7916 Anomaly AUC:0.9382
[2024-03-10 20:30:25,233][main.py][line:124][INFO] [Epoch:12/20, Batch:219/320]: loss1:0.0113 loss2:0.0568 loss3:0.0000 | AUC:0.8120 Anomaly AUC:0.9429
[2024-03-10 20:30:40,096][main.py][line:124][INFO] [Epoch:12/20, Batch:229/320]: loss1:0.0084 loss2:0.1162 loss3:0.0000 | AUC:0.7968 Anomaly AUC:0.9382
[2024-03-10 20:30:55,054][main.py][line:124][INFO] [Epoch:12/20, Batch:239/320]: loss1:0.0081 loss2:0.0394 loss3:0.0000 | AUC:0.7753 Anomaly AUC:0.9317
[2024-03-10 20:31:10,365][main.py][line:124][INFO] [Epoch:12/20, Batch:249/320]: loss1:0.0094 loss2:0.0712 loss3:0.0000 | AUC:0.7689 Anomaly AUC:0.9328
[2024-03-10 20:31:25,460][main.py][line:124][INFO] [Epoch:12/20, Batch:259/320]: loss1:0.0059 loss2:0.0564 loss3:0.0000 | AUC:0.7852 Anomaly AUC:0.9365
[2024-03-10 20:31:40,236][main.py][line:124][INFO] [Epoch:12/20, Batch:269/320]: loss1:0.0299 loss2:0.0857 loss3:0.0000 | AUC:0.7755 Anomaly AUC:0.9326
[2024-03-10 20:31:55,236][main.py][line:124][INFO] [Epoch:12/20, Batch:279/320]: loss1:0.0084 loss2:0.1081 loss3:0.0000 | AUC:0.8267 Anomaly AUC:0.9447
[2024-03-10 20:32:09,801][main.py][line:124][INFO] [Epoch:12/20, Batch:289/320]: loss1:0.0748 loss2:0.1038 loss3:0.0000 | AUC:0.7975 Anomaly AUC:0.9379
[2024-03-10 20:32:25,372][main.py][line:153][INFO] [Epoch:12/20]: lr:0.00010 | loss1:0.1122 loss2:0.1989 loss3:0.0000 | AUC:0.7921 Anomaly AUC:0.9346
[2024-03-10 20:32:39,842][main.py][line:124][INFO] [Epoch:13/20, Batch:9/320]: loss1:0.0150 loss2:0.1489 loss3:0.0000 | AUC:0.7736 Anomaly AUC:0.9329
[2024-03-10 20:32:54,132][main.py][line:124][INFO] [Epoch:13/20, Batch:19/320]: loss1:0.0370 loss2:0.1228 loss3:0.0000 | AUC:0.7579 Anomaly AUC:0.9304
[2024-03-10 20:33:09,990][main.py][line:124][INFO] [Epoch:13/20, Batch:29/320]: loss1:0.0090 loss2:0.0822 loss3:0.0000 | AUC:0.7667 Anomaly AUC:0.9305
[2024-03-10 20:33:24,112][main.py][line:124][INFO] [Epoch:13/20, Batch:39/320]: loss1:0.0070 loss2:0.0612 loss3:0.0000 | AUC:0.7792 Anomaly AUC:0.9353
[2024-03-10 20:33:38,122][main.py][line:124][INFO] [Epoch:13/20, Batch:49/320]: loss1:0.0088 loss2:0.1092 loss3:0.0000 | AUC:0.7688 Anomaly AUC:0.9359
[2024-03-10 20:33:53,472][main.py][line:124][INFO] [Epoch:13/20, Batch:59/320]: loss1:0.0115 loss2:0.0946 loss3:0.0000 | AUC:0.7473 Anomaly AUC:0.9282
[2024-03-10 20:34:07,832][main.py][line:124][INFO] [Epoch:13/20, Batch:69/320]: loss1:0.0070 loss2:0.0999 loss3:0.0000 | AUC:0.8046 Anomaly AUC:0.9401
[2024-03-10 20:34:21,998][main.py][line:124][INFO] [Epoch:13/20, Batch:79/320]: loss1:0.0571 loss2:0.1285 loss3:0.0000 | AUC:0.8071 Anomaly AUC:0.9409
[2024-03-10 20:34:37,516][main.py][line:124][INFO] [Epoch:13/20, Batch:89/320]: loss1:0.0204 loss2:0.0835 loss3:0.0000 | AUC:0.8017 Anomaly AUC:0.9380
[2024-03-10 20:34:51,595][main.py][line:124][INFO] [Epoch:13/20, Batch:99/320]: loss1:0.0410 loss2:0.0759 loss3:0.0000 | AUC:0.7865 Anomaly AUC:0.9355
[2024-03-10 20:35:06,399][main.py][line:124][INFO] [Epoch:13/20, Batch:109/320]: loss1:0.0051 loss2:0.0627 loss3:0.0000 | AUC:0.7892 Anomaly AUC:0.9359
[2024-03-10 20:35:20,512][main.py][line:124][INFO] [Epoch:13/20, Batch:119/320]: loss1:0.0082 loss2:0.0522 loss3:0.0000 | AUC:0.8046 Anomaly AUC:0.9397
[2024-03-10 20:35:34,693][main.py][line:124][INFO] [Epoch:13/20, Batch:129/320]: loss1:0.0737 loss2:0.1757 loss3:0.0000 | AUC:0.7974 Anomaly AUC:0.9383
[2024-03-10 20:35:49,520][main.py][line:124][INFO] [Epoch:13/20, Batch:139/320]: loss1:0.0084 loss2:0.1028 loss3:0.0000 | AUC:0.7928 Anomaly AUC:0.9372
[2024-03-10 20:36:03,601][main.py][line:124][INFO] [Epoch:13/20, Batch:149/320]: loss1:0.0045 loss2:0.0743 loss3:0.0000 | AUC:0.7701 Anomaly AUC:0.9331
[2024-03-10 20:36:17,896][main.py][line:124][INFO] [Epoch:13/20, Batch:159/320]: loss1:0.0093 loss2:0.0963 loss3:0.0000 | AUC:0.7850 Anomaly AUC:0.9343
[2024-03-10 20:36:33,436][main.py][line:124][INFO] [Epoch:13/20, Batch:169/320]: loss1:0.0076 loss2:0.0975 loss3:0.0000 | AUC:0.7815 Anomaly AUC:0.9319
[2024-03-10 20:36:47,783][main.py][line:124][INFO] [Epoch:13/20, Batch:179/320]: loss1:0.0073 loss2:0.0820 loss3:0.0000 | AUC:0.7777 Anomaly AUC:0.9319
[2024-03-10 20:37:02,078][main.py][line:124][INFO] [Epoch:13/20, Batch:189/320]: loss1:0.0158 loss2:0.0820 loss3:0.0000 | AUC:0.7848 Anomaly AUC:0.9357
[2024-03-10 20:37:17,326][main.py][line:124][INFO] [Epoch:13/20, Batch:199/320]: loss1:0.0077 loss2:0.0550 loss3:0.0000 | AUC:0.7830 Anomaly AUC:0.9348
[2024-03-10 20:37:31,667][main.py][line:124][INFO] [Epoch:13/20, Batch:209/320]: loss1:0.0188 loss2:0.1334 loss3:0.0000 | AUC:0.7930 Anomaly AUC:0.9369
[2024-03-10 20:37:46,341][main.py][line:124][INFO] [Epoch:13/20, Batch:219/320]: loss1:0.0544 loss2:0.1677 loss3:0.0000 | AUC:0.7799 Anomaly AUC:0.9323
[2024-03-10 20:38:02,301][main.py][line:124][INFO] [Epoch:13/20, Batch:229/320]: loss1:0.0064 loss2:0.1221 loss3:0.0000 | AUC:0.8098 Anomaly AUC:0.9365
[2024-03-10 20:38:17,625][main.py][line:124][INFO] [Epoch:13/20, Batch:239/320]: loss1:0.0474 loss2:0.1242 loss3:0.0000 | AUC:0.7699 Anomaly AUC:0.9312
[2024-03-10 20:38:32,626][main.py][line:124][INFO] [Epoch:13/20, Batch:249/320]: loss1:0.0116 loss2:0.0600 loss3:0.0000 | AUC:0.7254 Anomaly AUC:0.9229
[2024-03-10 20:38:46,844][main.py][line:124][INFO] [Epoch:13/20, Batch:259/320]: loss1:0.0154 loss2:0.1653 loss3:0.0000 | AUC:0.7502 Anomaly AUC:0.9297
[2024-03-10 20:39:01,029][main.py][line:124][INFO] [Epoch:13/20, Batch:269/320]: loss1:0.0543 loss2:0.1598 loss3:0.0000 | AUC:0.7726 Anomaly AUC:0.9360
[2024-03-10 20:39:16,669][main.py][line:124][INFO] [Epoch:13/20, Batch:279/320]: loss1:0.0058 loss2:0.1339 loss3:0.0000 | AUC:0.7658 Anomaly AUC:0.9331
[2024-03-10 20:39:30,766][main.py][line:124][INFO] [Epoch:13/20, Batch:289/320]: loss1:0.0059 loss2:0.0942 loss3:0.0000 | AUC:0.7529 Anomaly AUC:0.9272
[2024-03-10 20:39:44,711][main.py][line:153][INFO] [Epoch:13/20]: lr:0.00010 | loss1:0.1109 loss2:0.2547 loss3:0.0000 | AUC:0.7658 Anomaly AUC:0.9306
[2024-03-10 20:40:01,760][main.py][line:124][INFO] [Epoch:14/20, Batch:9/320]: loss1:0.0183 loss2:0.0890 loss3:0.0000 | AUC:0.7859 Anomaly AUC:0.9348
[2024-03-10 20:40:18,325][main.py][line:124][INFO] [Epoch:14/20, Batch:19/320]: loss1:0.0176 loss2:0.0729 loss3:0.0000 | AUC:0.7351 Anomaly AUC:0.9209
[2024-03-10 20:40:32,571][main.py][line:124][INFO] [Epoch:14/20, Batch:29/320]: loss1:0.0046 loss2:0.1143 loss3:0.0000 | AUC:0.7870 Anomaly AUC:0.9356
[2024-03-10 20:40:46,806][main.py][line:124][INFO] [Epoch:14/20, Batch:39/320]: loss1:0.0059 loss2:0.0582 loss3:0.0000 | AUC:0.8349 Anomaly AUC:0.9476
[2024-03-10 20:41:01,938][main.py][line:124][INFO] [Epoch:14/20, Batch:49/320]: loss1:0.0134 loss2:0.1185 loss3:0.0000 | AUC:0.8211 Anomaly AUC:0.9441
[2024-03-10 20:41:16,106][main.py][line:124][INFO] [Epoch:14/20, Batch:59/320]: loss1:0.0053 loss2:0.0973 loss3:0.0000 | AUC:0.7800 Anomaly AUC:0.9323
[2024-03-10 20:41:30,389][main.py][line:124][INFO] [Epoch:14/20, Batch:69/320]: loss1:0.0089 loss2:0.0674 loss3:0.0000 | AUC:0.8042 Anomaly AUC:0.9389
[2024-03-10 20:41:46,733][main.py][line:124][INFO] [Epoch:14/20, Batch:79/320]: loss1:0.0080 loss2:0.0911 loss3:0.0000 | AUC:0.7465 Anomaly AUC:0.9295
[2024-03-10 20:42:02,093][main.py][line:124][INFO] [Epoch:14/20, Batch:89/320]: loss1:0.0085 loss2:0.0807 loss3:0.0000 | AUC:0.8003 Anomaly AUC:0.9416
[2024-03-10 20:42:16,359][main.py][line:124][INFO] [Epoch:14/20, Batch:99/320]: loss1:0.0118 loss2:0.0891 loss3:0.0000 | AUC:0.7940 Anomaly AUC:0.9403
[2024-03-10 20:42:30,427][main.py][line:124][INFO] [Epoch:14/20, Batch:109/320]: loss1:0.0121 loss2:0.1292 loss3:0.0000 | AUC:0.7484 Anomaly AUC:0.9277
[2024-03-10 20:42:45,131][main.py][line:124][INFO] [Epoch:14/20, Batch:119/320]: loss1:0.0065 loss2:0.0810 loss3:0.0000 | AUC:0.7997 Anomaly AUC:0.9392
[2024-03-10 20:43:00,407][main.py][line:124][INFO] [Epoch:14/20, Batch:129/320]: loss1:0.0165 loss2:0.0561 loss3:0.0000 | AUC:0.8052 Anomaly AUC:0.9409
[2024-03-10 20:43:16,667][main.py][line:124][INFO] [Epoch:14/20, Batch:139/320]: loss1:0.0087 loss2:0.0540 loss3:0.0000 | AUC:0.7639 Anomaly AUC:0.9295
[2024-03-10 20:43:31,726][main.py][line:124][INFO] [Epoch:14/20, Batch:149/320]: loss1:0.0110 loss2:0.1285 loss3:0.0000 | AUC:0.7661 Anomaly AUC:0.9301
[2024-03-10 20:43:46,813][main.py][line:124][INFO] [Epoch:14/20, Batch:159/320]: loss1:0.0095 loss2:0.0607 loss3:0.0000 | AUC:0.7937 Anomaly AUC:0.9360
[2024-03-10 20:44:02,250][main.py][line:124][INFO] [Epoch:14/20, Batch:169/320]: loss1:0.0681 loss2:0.1724 loss3:0.0000 | AUC:0.7660 Anomaly AUC:0.9275
[2024-03-10 20:44:16,508][main.py][line:124][INFO] [Epoch:14/20, Batch:179/320]: loss1:0.0249 loss2:0.0524 loss3:0.0000 | AUC:0.7745 Anomaly AUC:0.9329
[2024-03-10 20:44:30,595][main.py][line:124][INFO] [Epoch:14/20, Batch:189/320]: loss1:0.0110 loss2:0.1159 loss3:0.0000 | AUC:0.8128 Anomaly AUC:0.9402
[2024-03-10 20:44:45,798][main.py][line:124][INFO] [Epoch:14/20, Batch:199/320]: loss1:0.0961 loss2:0.1392 loss3:0.0000 | AUC:0.8026 Anomaly AUC:0.9363
[2024-03-10 20:45:01,079][main.py][line:124][INFO] [Epoch:14/20, Batch:209/320]: loss1:0.0611 loss2:0.0980 loss3:0.0000 | AUC:0.7689 Anomaly AUC:0.9293
[2024-03-10 20:45:16,386][main.py][line:124][INFO] [Epoch:14/20, Batch:219/320]: loss1:0.0098 loss2:0.0627 loss3:0.0000 | AUC:0.7593 Anomaly AUC:0.9355
[2024-03-10 20:45:31,786][main.py][line:124][INFO] [Epoch:14/20, Batch:229/320]: loss1:0.0561 loss2:0.0988 loss3:0.0000 | AUC:0.7667 Anomaly AUC:0.9362
[2024-03-10 20:45:46,160][main.py][line:124][INFO] [Epoch:14/20, Batch:239/320]: loss1:0.0108 loss2:0.0922 loss3:0.0000 | AUC:0.8138 Anomaly AUC:0.9438
[2024-03-10 20:46:00,557][main.py][line:124][INFO] [Epoch:14/20, Batch:249/320]: loss1:0.0167 loss2:0.0792 loss3:0.0000 | AUC:0.8187 Anomaly AUC:0.9437
[2024-03-10 20:46:16,084][main.py][line:124][INFO] [Epoch:14/20, Batch:259/320]: loss1:0.0685 loss2:0.1257 loss3:0.0000 | AUC:0.8030 Anomaly AUC:0.9388
[2024-03-10 20:46:31,728][main.py][line:124][INFO] [Epoch:14/20, Batch:269/320]: loss1:0.0180 loss2:0.0797 loss3:0.0000 | AUC:0.7805 Anomaly AUC:0.9362
[2024-03-10 20:46:47,325][main.py][line:124][INFO] [Epoch:14/20, Batch:279/320]: loss1:0.1087 loss2:0.1322 loss3:0.0000 | AUC:0.7670 Anomaly AUC:0.9348
[2024-03-10 20:47:02,499][main.py][line:124][INFO] [Epoch:14/20, Batch:289/320]: loss1:0.0106 loss2:0.0651 loss3:0.0000 | AUC:0.7806 Anomaly AUC:0.9374
[2024-03-10 20:47:19,198][main.py][line:153][INFO] [Epoch:14/20]: lr:0.00010 | loss1:0.0424 loss2:0.0745 loss3:0.0000 | AUC:0.7817 Anomaly AUC:0.9375
[2024-03-10 20:47:33,722][main.py][line:124][INFO] [Epoch:15/20, Batch:9/320]: loss1:0.0171 loss2:0.0602 loss3:0.0000 | AUC:0.7310 Anomaly AUC:0.9237
[2024-03-10 20:47:48,065][main.py][line:124][INFO] [Epoch:15/20, Batch:19/320]: loss1:0.0054 loss2:0.0673 loss3:0.0000 | AUC:0.7894 Anomaly AUC:0.9375
[2024-03-10 20:48:03,151][main.py][line:124][INFO] [Epoch:15/20, Batch:29/320]: loss1:0.0063 loss2:0.0722 loss3:0.0000 | AUC:0.8018 Anomaly AUC:0.9397
[2024-03-10 20:48:18,007][main.py][line:124][INFO] [Epoch:15/20, Batch:39/320]: loss1:0.0081 loss2:0.1090 loss3:0.0000 | AUC:0.7941 Anomaly AUC:0.9379
[2024-03-10 20:48:33,666][main.py][line:124][INFO] [Epoch:15/20, Batch:49/320]: loss1:0.0157 loss2:0.0891 loss3:0.0000 | AUC:0.7603 Anomaly AUC:0.9322
[2024-03-10 20:48:48,376][main.py][line:124][INFO] [Epoch:15/20, Batch:59/320]: loss1:0.0071 loss2:0.0696 loss3:0.0000 | AUC:0.7672 Anomaly AUC:0.9363
[2024-03-10 20:49:03,459][main.py][line:124][INFO] [Epoch:15/20, Batch:69/320]: loss1:0.0077 loss2:0.0478 loss3:0.0000 | AUC:0.7722 Anomaly AUC:0.9355
[2024-03-10 20:49:19,110][main.py][line:124][INFO] [Epoch:15/20, Batch:79/320]: loss1:0.0103 loss2:0.0874 loss3:0.0000 | AUC:0.7816 Anomaly AUC:0.9353
[2024-03-10 20:49:33,991][main.py][line:124][INFO] [Epoch:15/20, Batch:89/320]: loss1:0.1221 loss2:0.1251 loss3:0.0000 | AUC:0.7826 Anomaly AUC:0.9347
[2024-03-10 20:49:48,798][main.py][line:124][INFO] [Epoch:15/20, Batch:99/320]: loss1:0.0324 loss2:0.0749 loss3:0.0000 | AUC:0.7877 Anomaly AUC:0.9371
[2024-03-10 20:50:03,311][main.py][line:124][INFO] [Epoch:15/20, Batch:109/320]: loss1:0.0102 loss2:0.0509 loss3:0.0000 | AUC:0.7937 Anomaly AUC:0.9396
[2024-03-10 20:50:18,209][main.py][line:124][INFO] [Epoch:15/20, Batch:119/320]: loss1:0.0063 loss2:0.1041 loss3:0.0000 | AUC:0.7883 Anomaly AUC:0.9362
[2024-03-10 20:50:33,553][main.py][line:124][INFO] [Epoch:15/20, Batch:129/320]: loss1:0.0073 loss2:0.0472 loss3:0.0000 | AUC:0.7630 Anomaly AUC:0.9288
[2024-03-10 20:50:48,674][main.py][line:124][INFO] [Epoch:15/20, Batch:139/320]: loss1:0.0353 loss2:0.0900 loss3:0.0000 | AUC:0.7893 Anomaly AUC:0.9360
[2024-03-10 20:51:02,963][main.py][line:124][INFO] [Epoch:15/20, Batch:149/320]: loss1:0.0707 loss2:0.0952 loss3:0.0000 | AUC:0.8028 Anomaly AUC:0.9388
[2024-03-10 20:51:16,929][main.py][line:124][INFO] [Epoch:15/20, Batch:159/320]: loss1:0.0106 loss2:0.1096 loss3:0.0000 | AUC:0.7844 Anomaly AUC:0.9372
[2024-03-10 20:51:31,283][main.py][line:124][INFO] [Epoch:15/20, Batch:169/320]: loss1:0.0250 loss2:0.0854 loss3:0.0000 | AUC:0.8050 Anomaly AUC:0.9416
[2024-03-10 20:51:46,277][main.py][line:124][INFO] [Epoch:15/20, Batch:179/320]: loss1:0.0091 loss2:0.0924 loss3:0.0000 | AUC:0.7984 Anomaly AUC:0.9394
[2024-03-10 20:52:00,905][main.py][line:124][INFO] [Epoch:15/20, Batch:189/320]: loss1:0.0124 loss2:0.0509 loss3:0.0000 | AUC:0.7923 Anomaly AUC:0.9347
[2024-03-10 20:52:15,597][main.py][line:124][INFO] [Epoch:15/20, Batch:199/320]: loss1:0.0075 loss2:0.0600 loss3:0.0000 | AUC:0.7743 Anomaly AUC:0.9336
[2024-03-10 20:52:30,905][main.py][line:124][INFO] [Epoch:15/20, Batch:209/320]: loss1:0.0155 loss2:0.0568 loss3:0.0000 | AUC:0.7563 Anomaly AUC:0.9278
[2024-03-10 20:52:45,141][main.py][line:124][INFO] [Epoch:15/20, Batch:219/320]: loss1:0.0052 loss2:0.0755 loss3:0.0000 | AUC:0.7791 Anomaly AUC:0.9341
[2024-03-10 20:52:59,831][main.py][line:124][INFO] [Epoch:15/20, Batch:229/320]: loss1:0.0079 loss2:0.0720 loss3:0.0000 | AUC:0.7939 Anomaly AUC:0.9361
[2024-03-10 20:53:14,206][main.py][line:124][INFO] [Epoch:15/20, Batch:239/320]: loss1:0.0110 loss2:0.0570 loss3:0.0000 | AUC:0.8032 Anomaly AUC:0.9364
[2024-03-10 20:53:29,092][main.py][line:124][INFO] [Epoch:15/20, Batch:249/320]: loss1:0.0073 loss2:0.0505 loss3:0.0000 | AUC:0.8142 Anomaly AUC:0.9388
[2024-03-10 20:53:43,880][main.py][line:124][INFO] [Epoch:15/20, Batch:259/320]: loss1:0.0113 loss2:0.0531 loss3:0.0000 | AUC:0.8117 Anomaly AUC:0.9389
[2024-03-10 20:53:58,162][main.py][line:124][INFO] [Epoch:15/20, Batch:269/320]: loss1:0.0125 loss2:0.0489 loss3:0.0000 | AUC:0.8119 Anomaly AUC:0.9393
[2024-03-10 20:54:12,520][main.py][line:124][INFO] [Epoch:15/20, Batch:279/320]: loss1:0.0036 loss2:0.0662 loss3:0.0000 | AUC:0.8013 Anomaly AUC:0.9368
[2024-03-10 20:54:26,702][main.py][line:124][INFO] [Epoch:15/20, Batch:289/320]: loss1:0.0075 loss2:0.0969 loss3:0.0000 | AUC:0.7924 Anomaly AUC:0.9354
[2024-03-10 20:54:40,298][main.py][line:153][INFO] [Epoch:15/20]: lr:0.00010 | loss1:0.0177 loss2:0.0450 loss3:0.0000 | AUC:0.7868 Anomaly AUC:0.9341
[2024-03-10 20:54:56,396][main.py][line:124][INFO] [Epoch:16/20, Batch:9/320]: loss1:0.0058 loss2:0.0973 loss3:0.0000 | AUC:0.7638 Anomaly AUC:0.9260
[2024-03-10 20:55:10,625][main.py][line:124][INFO] [Epoch:16/20, Batch:19/320]: loss1:0.0220 loss2:0.1064 loss3:0.0000 | AUC:0.8014 Anomaly AUC:0.9379
[2024-03-10 20:55:24,653][main.py][line:124][INFO] [Epoch:16/20, Batch:29/320]: loss1:0.0204 loss2:0.1094 loss3:0.0000 | AUC:0.8155 Anomaly AUC:0.9415
[2024-03-10 20:55:39,894][main.py][line:124][INFO] [Epoch:16/20, Batch:39/320]: loss1:0.0367 loss2:0.1754 loss3:0.0000 | AUC:0.7800 Anomaly AUC:0.9339
[2024-03-10 20:55:54,195][main.py][line:124][INFO] [Epoch:16/20, Batch:49/320]: loss1:0.0299 loss2:0.0892 loss3:0.0000 | AUC:0.7836 Anomaly AUC:0.9339
[2024-03-10 20:56:09,150][main.py][line:124][INFO] [Epoch:16/20, Batch:59/320]: loss1:0.0124 loss2:0.0978 loss3:0.0000 | AUC:0.7897 Anomaly AUC:0.9326
[2024-03-10 20:56:23,470][main.py][line:124][INFO] [Epoch:16/20, Batch:69/320]: loss1:0.0127 loss2:0.0424 loss3:0.0000 | AUC:0.8080 Anomaly AUC:0.9412
[2024-03-10 20:56:37,749][main.py][line:124][INFO] [Epoch:16/20, Batch:79/320]: loss1:0.0135 loss2:0.1018 loss3:0.0000 | AUC:0.7734 Anomaly AUC:0.9380
[2024-03-10 20:56:52,392][main.py][line:124][INFO] [Epoch:16/20, Batch:89/320]: loss1:0.0311 loss2:0.1146 loss3:0.0000 | AUC:0.8028 Anomaly AUC:0.9419
[2024-03-10 20:57:06,709][main.py][line:124][INFO] [Epoch:16/20, Batch:99/320]: loss1:0.0523 loss2:0.0925 loss3:0.0000 | AUC:0.7296 Anomaly AUC:0.9238
[2024-03-10 20:57:21,318][main.py][line:124][INFO] [Epoch:16/20, Batch:109/320]: loss1:0.0175 loss2:0.1093 loss3:0.0000 | AUC:0.7269 Anomaly AUC:0.9211
[2024-03-10 20:57:35,487][main.py][line:124][INFO] [Epoch:16/20, Batch:119/320]: loss1:0.0101 loss2:0.1037 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9426
[2024-03-10 20:57:49,773][main.py][line:124][INFO] [Epoch:16/20, Batch:129/320]: loss1:0.0091 loss2:0.1110 loss3:0.0000 | AUC:0.8219 Anomaly AUC:0.9445
[2024-03-10 20:58:05,149][main.py][line:124][INFO] [Epoch:16/20, Batch:139/320]: loss1:0.0110 loss2:0.0862 loss3:0.0000 | AUC:0.8098 Anomaly AUC:0.9420
[2024-03-10 20:58:19,394][main.py][line:124][INFO] [Epoch:16/20, Batch:149/320]: loss1:0.0084 loss2:0.0833 loss3:0.0000 | AUC:0.7940 Anomaly AUC:0.9415
[2024-03-10 20:58:33,696][main.py][line:124][INFO] [Epoch:16/20, Batch:159/320]: loss1:0.0126 loss2:0.0544 loss3:0.0000 | AUC:0.7759 Anomaly AUC:0.9373
[2024-03-10 20:58:48,796][main.py][line:124][INFO] [Epoch:16/20, Batch:169/320]: loss1:0.0087 loss2:0.0700 loss3:0.0000 | AUC:0.7886 Anomaly AUC:0.9341
[2024-03-10 20:59:02,895][main.py][line:124][INFO] [Epoch:16/20, Batch:179/320]: loss1:0.0201 loss2:0.0491 loss3:0.0000 | AUC:0.8013 Anomaly AUC:0.9410
[2024-03-10 20:59:17,643][main.py][line:124][INFO] [Epoch:16/20, Batch:189/320]: loss1:0.0109 loss2:0.0715 loss3:0.0000 | AUC:0.7833 Anomaly AUC:0.9385
[2024-03-10 20:59:31,981][main.py][line:124][INFO] [Epoch:16/20, Batch:199/320]: loss1:0.0162 loss2:0.0543 loss3:0.0000 | AUC:0.7992 Anomaly AUC:0.9405
[2024-03-10 20:59:46,143][main.py][line:124][INFO] [Epoch:16/20, Batch:209/320]: loss1:0.0784 loss2:0.1353 loss3:0.0000 | AUC:0.7669 Anomaly AUC:0.9295
[2024-03-10 21:00:00,905][main.py][line:124][INFO] [Epoch:16/20, Batch:219/320]: loss1:0.0083 loss2:0.0784 loss3:0.0000 | AUC:0.7426 Anomaly AUC:0.9257
[2024-03-10 21:00:16,244][main.py][line:124][INFO] [Epoch:16/20, Batch:229/320]: loss1:0.0316 loss2:0.0793 loss3:0.0000 | AUC:0.7265 Anomaly AUC:0.9246
[2024-03-10 21:00:32,344][main.py][line:124][INFO] [Epoch:16/20, Batch:239/320]: loss1:0.0081 loss2:0.0696 loss3:0.0000 | AUC:0.8115 Anomaly AUC:0.9413
[2024-03-10 21:00:46,638][main.py][line:124][INFO] [Epoch:16/20, Batch:249/320]: loss1:0.0110 loss2:0.0887 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9369
[2024-03-10 21:01:01,008][main.py][line:124][INFO] [Epoch:16/20, Batch:259/320]: loss1:0.0119 loss2:0.0814 loss3:0.0000 | AUC:0.7938 Anomaly AUC:0.9386
[2024-03-10 21:01:15,590][main.py][line:124][INFO] [Epoch:16/20, Batch:269/320]: loss1:0.0095 loss2:0.0943 loss3:0.0000 | AUC:0.7914 Anomaly AUC:0.9363
[2024-03-10 21:01:29,841][main.py][line:124][INFO] [Epoch:16/20, Batch:279/320]: loss1:0.1056 loss2:0.1369 loss3:0.0000 | AUC:0.7882 Anomaly AUC:0.9321
[2024-03-10 21:01:44,176][main.py][line:124][INFO] [Epoch:16/20, Batch:289/320]: loss1:0.0090 loss2:0.0787 loss3:0.0000 | AUC:0.7992 Anomaly AUC:0.9393
[2024-03-10 21:02:01,397][main.py][line:153][INFO] [Epoch:16/20]: lr:0.00010 | loss1:0.0109 loss2:0.0395 loss3:0.0000 | AUC:0.8174 Anomaly AUC:0.9442
[2024-03-10 21:02:16,058][main.py][line:124][INFO] [Epoch:17/20, Batch:9/320]: loss1:0.0115 loss2:0.0986 loss3:0.0000 | AUC:0.8169 Anomaly AUC:0.9437
[2024-03-10 21:02:30,273][main.py][line:124][INFO] [Epoch:17/20, Batch:19/320]: loss1:0.0149 loss2:0.0421 loss3:0.0000 | AUC:0.8051 Anomaly AUC:0.9399
[2024-03-10 21:02:45,980][main.py][line:124][INFO] [Epoch:17/20, Batch:29/320]: loss1:0.0042 loss2:0.0497 loss3:0.0000 | AUC:0.8133 Anomaly AUC:0.9415
[2024-03-10 21:03:01,933][main.py][line:124][INFO] [Epoch:17/20, Batch:39/320]: loss1:0.0058 loss2:0.1229 loss3:0.0000 | AUC:0.8007 Anomaly AUC:0.9385
[2024-03-10 21:03:17,265][main.py][line:124][INFO] [Epoch:17/20, Batch:49/320]: loss1:0.0058 loss2:0.0711 loss3:0.0000 | AUC:0.7831 Anomaly AUC:0.9341
[2024-03-10 21:03:33,335][main.py][line:124][INFO] [Epoch:17/20, Batch:59/320]: loss1:0.0066 loss2:0.0603 loss3:0.0000 | AUC:0.7814 Anomaly AUC:0.9347
[2024-03-10 21:03:48,996][main.py][line:124][INFO] [Epoch:17/20, Batch:69/320]: loss1:0.0064 loss2:0.0486 loss3:0.0000 | AUC:0.7866 Anomaly AUC:0.9365
[2024-03-10 21:04:04,821][main.py][line:124][INFO] [Epoch:17/20, Batch:79/320]: loss1:0.0062 loss2:0.0385 loss3:0.0000 | AUC:0.7887 Anomaly AUC:0.9366
[2024-03-10 21:04:20,227][main.py][line:124][INFO] [Epoch:17/20, Batch:89/320]: loss1:0.0034 loss2:0.0344 loss3:0.0000 | AUC:0.7900 Anomaly AUC:0.9355
[2024-03-10 21:04:35,099][main.py][line:124][INFO] [Epoch:17/20, Batch:99/320]: loss1:0.0083 loss2:0.0622 loss3:0.0000 | AUC:0.7926 Anomaly AUC:0.9380
[2024-03-10 21:04:50,411][main.py][line:124][INFO] [Epoch:17/20, Batch:109/320]: loss1:0.0070 loss2:0.0390 loss3:0.0000 | AUC:0.8101 Anomaly AUC:0.9423
[2024-03-10 21:05:05,597][main.py][line:124][INFO] [Epoch:17/20, Batch:119/320]: loss1:0.0077 loss2:0.0394 loss3:0.0000 | AUC:0.8080 Anomaly AUC:0.9416
[2024-03-10 21:05:21,158][main.py][line:124][INFO] [Epoch:17/20, Batch:129/320]: loss1:0.0146 loss2:0.0808 loss3:0.0000 | AUC:0.7818 Anomaly AUC:0.9362
[2024-03-10 21:05:36,528][main.py][line:124][INFO] [Epoch:17/20, Batch:139/320]: loss1:0.0080 loss2:0.0554 loss3:0.0000 | AUC:0.8304 Anomaly AUC:0.9467
[2024-03-10 21:05:51,336][main.py][line:124][INFO] [Epoch:17/20, Batch:149/320]: loss1:0.0113 loss2:0.0442 loss3:0.0000 | AUC:0.8077 Anomaly AUC:0.9429
[2024-03-10 21:06:06,596][main.py][line:124][INFO] [Epoch:17/20, Batch:159/320]: loss1:0.0083 loss2:0.0705 loss3:0.0000 | AUC:0.7713 Anomaly AUC:0.9325
[2024-03-10 21:06:21,939][main.py][line:124][INFO] [Epoch:17/20, Batch:169/320]: loss1:0.0064 loss2:0.0513 loss3:0.0000 | AUC:0.7739 Anomaly AUC:0.9312
[2024-03-10 21:06:36,815][main.py][line:124][INFO] [Epoch:17/20, Batch:179/320]: loss1:0.0064 loss2:0.0400 loss3:0.0000 | AUC:0.7617 Anomaly AUC:0.9296
[2024-03-10 21:06:51,532][main.py][line:124][INFO] [Epoch:17/20, Batch:189/320]: loss1:0.0120 loss2:0.0658 loss3:0.0000 | AUC:0.7586 Anomaly AUC:0.9295
[2024-03-10 21:07:06,617][main.py][line:124][INFO] [Epoch:17/20, Batch:199/320]: loss1:0.0074 loss2:0.0732 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9331
[2024-03-10 21:07:21,533][main.py][line:124][INFO] [Epoch:17/20, Batch:209/320]: loss1:0.0084 loss2:0.0659 loss3:0.0000 | AUC:0.7534 Anomaly AUC:0.9273
[2024-03-10 21:07:36,490][main.py][line:124][INFO] [Epoch:17/20, Batch:219/320]: loss1:0.0498 loss2:0.0835 loss3:0.0000 | AUC:0.7873 Anomaly AUC:0.9392
[2024-03-10 21:07:51,355][main.py][line:124][INFO] [Epoch:17/20, Batch:229/320]: loss1:0.0171 loss2:0.0468 loss3:0.0000 | AUC:0.7769 Anomaly AUC:0.9375
[2024-03-10 21:08:06,234][main.py][line:124][INFO] [Epoch:17/20, Batch:239/320]: loss1:0.0092 loss2:0.0408 loss3:0.0000 | AUC:0.7605 Anomaly AUC:0.9307
[2024-03-10 21:08:21,006][main.py][line:124][INFO] [Epoch:17/20, Batch:249/320]: loss1:0.0083 loss2:0.0701 loss3:0.0000 | AUC:0.7329 Anomaly AUC:0.9204
[2024-03-10 21:08:36,237][main.py][line:124][INFO] [Epoch:17/20, Batch:259/320]: loss1:0.1223 loss2:0.1708 loss3:0.0000 | AUC:0.8035 Anomaly AUC:0.9372
[2024-03-10 21:08:51,131][main.py][line:124][INFO] [Epoch:17/20, Batch:269/320]: loss1:0.0075 loss2:0.0578 loss3:0.0000 | AUC:0.8187 Anomaly AUC:0.9414
[2024-03-10 21:09:05,854][main.py][line:124][INFO] [Epoch:17/20, Batch:279/320]: loss1:0.0160 loss2:0.0610 loss3:0.0000 | AUC:0.8155 Anomaly AUC:0.9423
[2024-03-10 21:09:20,927][main.py][line:124][INFO] [Epoch:17/20, Batch:289/320]: loss1:0.0060 loss2:0.0974 loss3:0.0000 | AUC:0.7849 Anomaly AUC:0.9383
[2024-03-10 21:09:37,791][main.py][line:153][INFO] [Epoch:17/20]: lr:0.00010 | loss1:0.0084 loss2:0.0881 loss3:0.0000 | AUC:0.7723 Anomaly AUC:0.9342
[2024-03-10 21:09:52,804][main.py][line:124][INFO] [Epoch:18/20, Batch:9/320]: loss1:0.0087 loss2:0.0566 loss3:0.0000 | AUC:0.7388 Anomaly AUC:0.9277
[2024-03-10 21:10:07,437][main.py][line:124][INFO] [Epoch:18/20, Batch:19/320]: loss1:0.0201 loss2:0.0893 loss3:0.0000 | AUC:0.7647 Anomaly AUC:0.9337
[2024-03-10 21:10:21,646][main.py][line:124][INFO] [Epoch:18/20, Batch:29/320]: loss1:0.0355 loss2:0.1246 loss3:0.0000 | AUC:0.8135 Anomaly AUC:0.9440
[2024-03-10 21:10:35,872][main.py][line:124][INFO] [Epoch:18/20, Batch:39/320]: loss1:0.0098 loss2:0.0430 loss3:0.0000 | AUC:0.7286 Anomaly AUC:0.9266
[2024-03-10 21:10:50,071][main.py][line:124][INFO] [Epoch:18/20, Batch:49/320]: loss1:0.0186 loss2:0.0747 loss3:0.0000 | AUC:0.7044 Anomaly AUC:0.9245
[2024-03-10 21:11:04,436][main.py][line:124][INFO] [Epoch:18/20, Batch:59/320]: loss1:0.0242 loss2:0.1032 loss3:0.0000 | AUC:0.7629 Anomaly AUC:0.9328
[2024-03-10 21:11:19,189][main.py][line:124][INFO] [Epoch:18/20, Batch:69/320]: loss1:0.0078 loss2:0.0568 loss3:0.0000 | AUC:0.7779 Anomaly AUC:0.9350
[2024-03-10 21:11:34,295][main.py][line:124][INFO] [Epoch:18/20, Batch:79/320]: loss1:0.0057 loss2:0.0305 loss3:0.0000 | AUC:0.7402 Anomaly AUC:0.9295
[2024-03-10 21:11:48,592][main.py][line:124][INFO] [Epoch:18/20, Batch:89/320]: loss1:0.0079 loss2:0.0673 loss3:0.0000 | AUC:0.7345 Anomaly AUC:0.9273
[2024-03-10 21:12:03,329][main.py][line:124][INFO] [Epoch:18/20, Batch:99/320]: loss1:0.0067 loss2:0.0949 loss3:0.0000 | AUC:0.7367 Anomaly AUC:0.9279
[2024-03-10 21:12:17,678][main.py][line:124][INFO] [Epoch:18/20, Batch:109/320]: loss1:0.0156 loss2:0.0427 loss3:0.0000 | AUC:0.7260 Anomaly AUC:0.9252
[2024-03-10 21:12:31,836][main.py][line:124][INFO] [Epoch:18/20, Batch:119/320]: loss1:0.0059 loss2:0.0319 loss3:0.0000 | AUC:0.7233 Anomaly AUC:0.9192
[2024-03-10 21:12:45,874][main.py][line:124][INFO] [Epoch:18/20, Batch:129/320]: loss1:0.0095 loss2:0.0407 loss3:0.0000 | AUC:0.7720 Anomaly AUC:0.9330
[2024-03-10 21:13:00,036][main.py][line:124][INFO] [Epoch:18/20, Batch:139/320]: loss1:0.0077 loss2:0.0537 loss3:0.0000 | AUC:0.7435 Anomaly AUC:0.9312
[2024-03-10 21:13:15,077][main.py][line:124][INFO] [Epoch:18/20, Batch:149/320]: loss1:0.0082 loss2:0.0408 loss3:0.0000 | AUC:0.7327 Anomaly AUC:0.9255
[2024-03-10 21:13:30,121][main.py][line:124][INFO] [Epoch:18/20, Batch:159/320]: loss1:0.0073 loss2:0.0689 loss3:0.0000 | AUC:0.7535 Anomaly AUC:0.9302
[2024-03-10 21:13:44,975][main.py][line:124][INFO] [Epoch:18/20, Batch:169/320]: loss1:0.0181 loss2:0.0740 loss3:0.0000 | AUC:0.6974 Anomaly AUC:0.9122
[2024-03-10 21:13:59,444][main.py][line:124][INFO] [Epoch:18/20, Batch:179/320]: loss1:0.0470 loss2:0.1227 loss3:0.0000 | AUC:0.7602 Anomaly AUC:0.9295
[2024-03-10 21:14:13,772][main.py][line:124][INFO] [Epoch:18/20, Batch:189/320]: loss1:0.0914 loss2:0.2339 loss3:0.0000 | AUC:0.8059 Anomaly AUC:0.9411
[2024-03-10 21:14:28,461][main.py][line:124][INFO] [Epoch:18/20, Batch:199/320]: loss1:0.0082 loss2:0.0571 loss3:0.0000 | AUC:0.7708 Anomaly AUC:0.9302
[2024-03-10 21:14:42,622][main.py][line:124][INFO] [Epoch:18/20, Batch:209/320]: loss1:0.0242 loss2:0.0513 loss3:0.0000 | AUC:0.8129 Anomaly AUC:0.9406
[2024-03-10 21:14:57,244][main.py][line:124][INFO] [Epoch:18/20, Batch:219/320]: loss1:0.0637 loss2:0.1085 loss3:0.0000 | AUC:0.8160 Anomaly AUC:0.9425
[2024-03-10 21:15:11,540][main.py][line:124][INFO] [Epoch:18/20, Batch:229/320]: loss1:0.0130 loss2:0.0585 loss3:0.0000 | AUC:0.8105 Anomaly AUC:0.9389
[2024-03-10 21:15:25,883][main.py][line:124][INFO] [Epoch:18/20, Batch:239/320]: loss1:0.0066 loss2:0.0638 loss3:0.0000 | AUC:0.8164 Anomaly AUC:0.9417
[2024-03-10 21:15:39,986][main.py][line:124][INFO] [Epoch:18/20, Batch:249/320]: loss1:0.0118 loss2:0.0888 loss3:0.0000 | AUC:0.8001 Anomaly AUC:0.9381
[2024-03-10 21:15:54,346][main.py][line:124][INFO] [Epoch:18/20, Batch:259/320]: loss1:0.0093 loss2:0.0348 loss3:0.0000 | AUC:0.7961 Anomaly AUC:0.9375
[2024-03-10 21:16:08,528][main.py][line:124][INFO] [Epoch:18/20, Batch:269/320]: loss1:0.0083 loss2:0.0526 loss3:0.0000 | AUC:0.7787 Anomaly AUC:0.9333
[2024-03-10 21:16:22,713][main.py][line:124][INFO] [Epoch:18/20, Batch:279/320]: loss1:0.0093 loss2:0.0554 loss3:0.0000 | AUC:0.7662 Anomaly AUC:0.9301
[2024-03-10 21:16:36,855][main.py][line:124][INFO] [Epoch:18/20, Batch:289/320]: loss1:0.0101 loss2:0.0789 loss3:0.0000 | AUC:0.7851 Anomaly AUC:0.9356
[2024-03-10 21:16:52,975][main.py][line:153][INFO] [Epoch:18/20]: lr:0.00010 | loss1:0.0069 loss2:0.0318 loss3:0.0000 | AUC:0.7796 Anomaly AUC:0.9332
[2024-03-10 21:17:07,254][main.py][line:124][INFO] [Epoch:19/20, Batch:9/320]: loss1:0.0069 loss2:0.0325 loss3:0.0000 | AUC:0.7675 Anomaly AUC:0.9302
[2024-03-10 21:17:21,279][main.py][line:124][INFO] [Epoch:19/20, Batch:19/320]: loss1:0.0146 loss2:0.0528 loss3:0.0000 | AUC:0.7742 Anomaly AUC:0.9312
[2024-03-10 21:17:35,448][main.py][line:124][INFO] [Epoch:19/20, Batch:29/320]: loss1:0.0074 loss2:0.0400 loss3:0.0000 | AUC:0.7805 Anomaly AUC:0.9325
[2024-03-10 21:17:49,650][main.py][line:124][INFO] [Epoch:19/20, Batch:39/320]: loss1:0.0090 loss2:0.0370 loss3:0.0000 | AUC:0.7705 Anomaly AUC:0.9292
[2024-03-10 21:18:03,898][main.py][line:124][INFO] [Epoch:19/20, Batch:49/320]: loss1:0.0067 loss2:0.0369 loss3:0.0000 | AUC:0.7610 Anomaly AUC:0.9272
[2024-03-10 21:18:17,927][main.py][line:124][INFO] [Epoch:19/20, Batch:59/320]: loss1:0.0061 loss2:0.0386 loss3:0.0000 | AUC:0.7568 Anomaly AUC:0.9273
[2024-03-10 21:18:32,025][main.py][line:124][INFO] [Epoch:19/20, Batch:69/320]: loss1:0.0157 loss2:0.0640 loss3:0.0000 | AUC:0.7686 Anomaly AUC:0.9324
[2024-03-10 21:18:46,204][main.py][line:124][INFO] [Epoch:19/20, Batch:79/320]: loss1:0.0118 loss2:0.0333 loss3:0.0000 | AUC:0.7641 Anomaly AUC:0.9310
[2024-03-10 21:19:00,447][main.py][line:124][INFO] [Epoch:19/20, Batch:89/320]: loss1:0.0054 loss2:0.0255 loss3:0.0000 | AUC:0.7622 Anomaly AUC:0.9291
[2024-03-10 21:19:14,686][main.py][line:124][INFO] [Epoch:19/20, Batch:99/320]: loss1:0.0086 loss2:0.0460 loss3:0.0000 | AUC:0.7740 Anomaly AUC:0.9309
[2024-03-10 21:19:28,899][main.py][line:124][INFO] [Epoch:19/20, Batch:109/320]: loss1:0.0075 loss2:0.0271 loss3:0.0000 | AUC:0.7608 Anomaly AUC:0.9276
[2024-03-10 21:19:43,127][main.py][line:124][INFO] [Epoch:19/20, Batch:119/320]: loss1:0.0082 loss2:0.0434 loss3:0.0000 | AUC:0.7613 Anomaly AUC:0.9288
[2024-03-10 21:19:57,177][main.py][line:124][INFO] [Epoch:19/20, Batch:129/320]: loss1:0.0120 loss2:0.0279 loss3:0.0000 | AUC:0.7672 Anomaly AUC:0.9305
[2024-03-10 21:20:11,445][main.py][line:124][INFO] [Epoch:19/20, Batch:139/320]: loss1:0.0053 loss2:0.0293 loss3:0.0000 | AUC:0.7701 Anomaly AUC:0.9310
[2024-03-10 21:20:25,724][main.py][line:124][INFO] [Epoch:19/20, Batch:149/320]: loss1:0.0071 loss2:0.0370 loss3:0.0000 | AUC:0.7626 Anomaly AUC:0.9288
[2024-03-10 21:20:39,955][main.py][line:124][INFO] [Epoch:19/20, Batch:159/320]: loss1:0.0059 loss2:0.0263 loss3:0.0000 | AUC:0.7659 Anomaly AUC:0.9299
[2024-03-10 21:20:54,268][main.py][line:124][INFO] [Epoch:19/20, Batch:169/320]: loss1:0.0055 loss2:0.0726 loss3:0.0000 | AUC:0.7761 Anomaly AUC:0.9331
[2024-03-10 21:21:08,548][main.py][line:124][INFO] [Epoch:19/20, Batch:179/320]: loss1:0.0070 loss2:0.0272 loss3:0.0000 | AUC:0.7865 Anomaly AUC:0.9384
[2024-03-10 21:21:22,913][main.py][line:124][INFO] [Epoch:19/20, Batch:189/320]: loss1:0.0080 loss2:0.0418 loss3:0.0000 | AUC:0.8049 Anomaly AUC:0.9416
[2024-03-10 21:21:37,591][main.py][line:124][INFO] [Epoch:19/20, Batch:199/320]: loss1:0.0101 loss2:0.0317 loss3:0.0000 | AUC:0.8011 Anomaly AUC:0.9391
[2024-03-10 21:21:52,071][main.py][line:124][INFO] [Epoch:19/20, Batch:209/320]: loss1:0.0397 loss2:0.0915 loss3:0.0000 | AUC:0.8265 Anomaly AUC:0.9447
[2024-03-10 21:22:06,395][main.py][line:124][INFO] [Epoch:19/20, Batch:219/320]: loss1:0.0057 loss2:0.0614 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9349
[2024-03-10 21:22:20,462][main.py][line:124][INFO] [Epoch:19/20, Batch:229/320]: loss1:0.0158 loss2:0.0686 loss3:0.0000 | AUC:0.7660 Anomaly AUC:0.9296
[2024-03-10 21:22:34,928][main.py][line:124][INFO] [Epoch:19/20, Batch:239/320]: loss1:0.0385 loss2:0.0843 loss3:0.0000 | AUC:0.7266 Anomaly AUC:0.9250
[2024-03-10 21:22:49,135][main.py][line:124][INFO] [Epoch:19/20, Batch:249/320]: loss1:0.0074 loss2:0.0575 loss3:0.0000 | AUC:0.7297 Anomaly AUC:0.9217
[2024-03-10 21:23:03,439][main.py][line:124][INFO] [Epoch:19/20, Batch:259/320]: loss1:0.0086 loss2:0.0461 loss3:0.0000 | AUC:0.7316 Anomaly AUC:0.9295
[2024-03-10 21:23:18,292][main.py][line:124][INFO] [Epoch:19/20, Batch:269/320]: loss1:0.0228 loss2:0.0774 loss3:0.0000 | AUC:0.7172 Anomaly AUC:0.9270
[2024-03-10 21:23:32,592][main.py][line:124][INFO] [Epoch:19/20, Batch:279/320]: loss1:0.0063 loss2:0.0787 loss3:0.0000 | AUC:0.7177 Anomaly AUC:0.9242
[2024-03-10 21:23:46,724][main.py][line:124][INFO] [Epoch:19/20, Batch:289/320]: loss1:0.0057 loss2:0.0342 loss3:0.0000 | AUC:0.7006 Anomaly AUC:0.9158
[2024-03-10 21:24:02,414][main.py][line:153][INFO] [Epoch:19/20]: lr:0.00010 | loss1:0.0061 loss2:0.0449 loss3:0.0000 | AUC:0.7896 Anomaly AUC:0.9340
[2024-03-10 21:24:17,174][main.py][line:124][INFO] [Epoch:20/20, Batch:9/320]: loss1:0.0104 loss2:0.0393 loss3:0.0000 | AUC:0.8252 Anomaly AUC:0.9419
[2024-03-10 21:24:31,318][main.py][line:124][INFO] [Epoch:20/20, Batch:19/320]: loss1:0.0061 loss2:0.0643 loss3:0.0000 | AUC:0.8132 Anomaly AUC:0.9394
[2024-03-10 21:24:45,584][main.py][line:124][INFO] [Epoch:20/20, Batch:29/320]: loss1:0.0054 loss2:0.0268 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9360
[2024-03-10 21:24:59,709][main.py][line:124][INFO] [Epoch:20/20, Batch:39/320]: loss1:0.0073 loss2:0.0300 loss3:0.0000 | AUC:0.7870 Anomaly AUC:0.9341
[2024-03-10 21:25:13,844][main.py][line:124][INFO] [Epoch:20/20, Batch:49/320]: loss1:0.0074 loss2:0.0329 loss3:0.0000 | AUC:0.7789 Anomaly AUC:0.9326
[2024-03-10 21:25:28,029][main.py][line:124][INFO] [Epoch:20/20, Batch:59/320]: loss1:0.0103 loss2:0.0475 loss3:0.0000 | AUC:0.7788 Anomaly AUC:0.9332
[2024-03-10 21:25:42,363][main.py][line:124][INFO] [Epoch:20/20, Batch:69/320]: loss1:0.0396 loss2:0.0803 loss3:0.0000 | AUC:0.7833 Anomaly AUC:0.9358
[2024-03-10 21:25:56,881][main.py][line:124][INFO] [Epoch:20/20, Batch:79/320]: loss1:0.0072 loss2:0.1206 loss3:0.0000 | AUC:0.7652 Anomaly AUC:0.9311
[2024-03-10 21:26:11,270][main.py][line:124][INFO] [Epoch:20/20, Batch:89/320]: loss1:0.0128 loss2:0.0337 loss3:0.0000 | AUC:0.7520 Anomaly AUC:0.9259
[2024-03-10 21:26:25,340][main.py][line:124][INFO] [Epoch:20/20, Batch:99/320]: loss1:0.0091 loss2:0.0737 loss3:0.0000 | AUC:0.7388 Anomaly AUC:0.9254
[2024-03-10 21:26:39,455][main.py][line:124][INFO] [Epoch:20/20, Batch:109/320]: loss1:0.0105 loss2:0.0366 loss3:0.0000 | AUC:0.6979 Anomaly AUC:0.9193
[2024-03-10 21:26:53,709][main.py][line:124][INFO] [Epoch:20/20, Batch:119/320]: loss1:0.0288 loss2:0.0814 loss3:0.0000 | AUC:0.6402 Anomaly AUC:0.9014
[2024-03-10 21:27:07,851][main.py][line:124][INFO] [Epoch:20/20, Batch:129/320]: loss1:0.0116 loss2:0.0487 loss3:0.0000 | AUC:0.6585 Anomaly AUC:0.9077
[2024-03-10 21:27:21,956][main.py][line:124][INFO] [Epoch:20/20, Batch:139/320]: loss1:0.0180 loss2:0.0361 loss3:0.0000 | AUC:0.6299 Anomaly AUC:0.8936
[2024-03-10 21:27:36,090][main.py][line:124][INFO] [Epoch:20/20, Batch:149/320]: loss1:0.0179 loss2:0.0711 loss3:0.0000 | AUC:0.7480 Anomaly AUC:0.9264
[2024-03-10 21:27:50,186][main.py][line:124][INFO] [Epoch:20/20, Batch:159/320]: loss1:0.0928 loss2:0.0980 loss3:0.0000 | AUC:0.7693 Anomaly AUC:0.9326
[2024-03-10 21:28:04,419][main.py][line:124][INFO] [Epoch:20/20, Batch:169/320]: loss1:0.0146 loss2:0.0516 loss3:0.0000 | AUC:0.7265 Anomaly AUC:0.9218
[2024-03-10 21:28:18,717][main.py][line:124][INFO] [Epoch:20/20, Batch:179/320]: loss1:0.0112 loss2:0.0656 loss3:0.0000 | AUC:0.7767 Anomaly AUC:0.9355
[2024-03-10 21:28:32,784][main.py][line:124][INFO] [Epoch:20/20, Batch:189/320]: loss1:0.0097 loss2:0.0510 loss3:0.0000 | AUC:0.7845 Anomaly AUC:0.9364
[2024-03-10 21:28:46,914][main.py][line:124][INFO] [Epoch:20/20, Batch:199/320]: loss1:0.0105 loss2:0.0686 loss3:0.0000 | AUC:0.7546 Anomaly AUC:0.9303
[2024-03-10 21:29:01,003][main.py][line:124][INFO] [Epoch:20/20, Batch:209/320]: loss1:0.0103 loss2:0.1142 loss3:0.0000 | AUC:0.6926 Anomaly AUC:0.9212
[2024-03-10 21:29:15,259][main.py][line:124][INFO] [Epoch:20/20, Batch:219/320]: loss1:0.0138 loss2:0.0431 loss3:0.0000 | AUC:0.7156 Anomaly AUC:0.9219
[2024-03-10 21:29:29,485][main.py][line:124][INFO] [Epoch:20/20, Batch:229/320]: loss1:0.0091 loss2:0.0519 loss3:0.0000 | AUC:0.7456 Anomaly AUC:0.9267
[2024-03-10 21:29:43,658][main.py][line:124][INFO] [Epoch:20/20, Batch:239/320]: loss1:0.0115 loss2:0.0587 loss3:0.0000 | AUC:0.7636 Anomaly AUC:0.9295
[2024-03-10 21:29:57,942][main.py][line:124][INFO] [Epoch:20/20, Batch:249/320]: loss1:0.0104 loss2:0.0971 loss3:0.0000 | AUC:0.7863 Anomaly AUC:0.9360
[2024-03-10 21:30:12,133][main.py][line:124][INFO] [Epoch:20/20, Batch:259/320]: loss1:0.0094 loss2:0.0897 loss3:0.0000 | AUC:0.7602 Anomaly AUC:0.9313
[2024-03-10 21:30:26,362][main.py][line:124][INFO] [Epoch:20/20, Batch:269/320]: loss1:0.0104 loss2:0.0667 loss3:0.0000 | AUC:0.7692 Anomaly AUC:0.9325
[2024-03-10 21:30:40,597][main.py][line:124][INFO] [Epoch:20/20, Batch:279/320]: loss1:0.0376 loss2:0.1690 loss3:0.0000 | AUC:0.7615 Anomaly AUC:0.9310
[2024-03-10 21:30:54,777][main.py][line:124][INFO] [Epoch:20/20, Batch:289/320]: loss1:0.0094 loss2:0.1214 loss3:0.0000 | AUC:0.7524 Anomaly AUC:0.9273
[2024-03-10 21:31:10,986][main.py][line:153][INFO] [Epoch:20/20]: lr:0.00010 | loss1:0.2487 loss2:0.3125 loss3:0.0000 | AUC:0.6851 Anomaly AUC:0.9139
[2024-03-10 21:31:11,029][main.py][line:171][INFO] Training completes in 140m 40s | best AUCAP:0.8349 Anomaly AUC:0.9476

[2024-03-10 21:58:19,441][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.6, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-10 21:58:19,701][main.py][line:259][INFO] total params:7.7702M
[2024-03-10 21:58:19,701][main.py][line:268][INFO] Test Mode
[2024-03-10 21:58:19,701][main.py][line:37][INFO] loading pretrained checkpoint from ./ckpt/xd__current.pkl.
[2024-03-10 21:58:34,508][infer.py][line:94][INFO] offline AUC:0.9478 Anomaly-AUC:0.8331 AP:0.8364 FAR:0.0007 | Complete in 0m 15s

[2024-03-11 11:49:25,225][main.py][line:182][INFO] Config:{'dataset': 'xd-violence', 'model_name': 'xd_', 'metrics': 'AP', 'feat_prefix': './data/xd-i3d', 'train_list': './list/xd/train.list', 'test_list': './list/xd/test.list', 'token_feat': './list/xd/xd-prompt.npy', 'gt': './list/xd/xd-gt.npy', 'win_size': 9, 'gamma': 0.06, 'bias': 0.02, 'norm': False, 't_step': 3, 'temp': 0.05, 'seed': 2, 'test_bs': 5, 'smooth': 'slide', 'kappa': 2, 'ckpt_path': './ckpt/xd__current.pkl', 'a_nums': 50, 'n_nums': 50, 'k': 20, 'lamda': 0.8, 'alpha': 0.6, 'margin': 100, 'max_epoch': 20, 'clip_feat_prefix': '/home/yukaneko/dev/CLIP-TSA_dataset/xd/features/', 'result_dir': './result/xd/', 'feat_dim': 1024, 'head_num': 1, 'hid_dim': 128, 'out_dim': 300, 'lr': 0.0001, 'dropout': 0.5, 'train_bs': 32, 'max_seqlen': 200, 'workers': 8, 'save_dir': './ckpt/', 'logs_dir': './log_info.log'}
[2024-03-11 11:49:28,353][main.py][line:259][INFO] total params:7.7702M
[2024-03-11 11:49:28,354][main.py][line:262][INFO] Training Mode
[2024-03-11 11:49:28,354][main.py][line:70][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Pdropout()
    (dropout2): Pdropout()
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (UR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (hard_atten): HardAttention(
        (scorer): MLP(
          (fc1): Linear(in_features=512, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=1, bias=True)
          (relu): ReLU()
          (sigmoid): Sigmoid()
        )
        (hard_att): PerturbedTopK()
      )
    )
    (hard_atten): HardAttention(
      (scorer): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=1, bias=True)
        (relu): ReLU()
        (sigmoid): Sigmoid()
      )
      (hard_att): PerturbedTopK()
    )
    (fc1): FC_layer(
      (fc): Sequential(
        (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
        (1): ReLU()
      )
      (dropout): Dropout(p=0.05, inplace=False)
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(3,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)

[2024-03-11 11:49:28,354][main.py][line:71][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.005
)

[2024-03-11 11:49:41,513][main.py][line:76][INFO] Random initialize AUCAP:0.2093 Anomaly AUC:0.44796
[2024-03-11 11:50:48,893][main.py][line:153][INFO] [Epoch:1/20]: lr:0.00010 | loss1:0.2464 loss2:0.5034 loss3:0.0000 | AUC:0.7545 Anomaly AUC:0.9264
[2024-03-11 11:51:03,417][main.py][line:124][INFO] [Epoch:2/20, Batch:9/320]: loss1:0.1167 loss2:0.4392 loss3:0.0000 | AUC:0.7691 Anomaly AUC:0.9286
[2024-03-11 11:51:17,634][main.py][line:124][INFO] [Epoch:2/20, Batch:19/320]: loss1:0.1609 loss2:0.4365 loss3:0.0000 | AUC:0.7568 Anomaly AUC:0.9237
[2024-03-11 11:51:31,996][main.py][line:124][INFO] [Epoch:2/20, Batch:29/320]: loss1:0.2373 loss2:0.5000 loss3:0.0000 | AUC:0.7704 Anomaly AUC:0.9341
[2024-03-11 11:51:46,345][main.py][line:124][INFO] [Epoch:2/20, Batch:39/320]: loss1:0.2045 loss2:0.4830 loss3:0.0000 | AUC:0.7906 Anomaly AUC:0.9356
[2024-03-11 11:52:00,716][main.py][line:124][INFO] [Epoch:2/20, Batch:49/320]: loss1:0.2436 loss2:0.4574 loss3:0.0000 | AUC:0.7704 Anomaly AUC:0.9327
[2024-03-11 11:52:15,064][main.py][line:124][INFO] [Epoch:2/20, Batch:59/320]: loss1:0.0925 loss2:0.3617 loss3:0.0000 | AUC:0.7535 Anomaly AUC:0.9216
[2024-03-11 11:52:29,207][main.py][line:124][INFO] [Epoch:2/20, Batch:69/320]: loss1:0.2101 loss2:0.4415 loss3:0.0000 | AUC:0.7616 Anomaly AUC:0.9263
[2024-03-11 11:52:43,498][main.py][line:124][INFO] [Epoch:2/20, Batch:79/320]: loss1:0.1851 loss2:0.4809 loss3:0.0000 | AUC:0.7794 Anomaly AUC:0.9312
[2024-03-11 11:52:57,815][main.py][line:124][INFO] [Epoch:2/20, Batch:89/320]: loss1:0.2219 loss2:0.4996 loss3:0.0000 | AUC:0.7683 Anomaly AUC:0.9257
[2024-03-11 11:53:12,230][main.py][line:124][INFO] [Epoch:2/20, Batch:99/320]: loss1:0.1313 loss2:0.4292 loss3:0.0000 | AUC:0.7950 Anomaly AUC:0.9390
[2024-03-11 11:53:26,687][main.py][line:124][INFO] [Epoch:2/20, Batch:109/320]: loss1:0.1103 loss2:0.3984 loss3:0.0000 | AUC:0.7562 Anomaly AUC:0.9244
[2024-03-11 11:53:40,982][main.py][line:124][INFO] [Epoch:2/20, Batch:119/320]: loss1:0.1815 loss2:0.3451 loss3:0.0000 | AUC:0.7465 Anomaly AUC:0.9190
[2024-03-11 11:53:55,307][main.py][line:124][INFO] [Epoch:2/20, Batch:129/320]: loss1:0.1141 loss2:0.4093 loss3:0.0000 | AUC:0.7882 Anomaly AUC:0.9347
[2024-03-11 11:54:09,556][main.py][line:124][INFO] [Epoch:2/20, Batch:139/320]: loss1:0.2021 loss2:0.5081 loss3:0.0000 | AUC:0.8100 Anomaly AUC:0.9370
[2024-03-11 11:54:23,876][main.py][line:124][INFO] [Epoch:2/20, Batch:149/320]: loss1:0.0707 loss2:0.3084 loss3:0.0000 | AUC:0.7165 Anomaly AUC:0.9096
[2024-03-11 11:54:38,197][main.py][line:124][INFO] [Epoch:2/20, Batch:159/320]: loss1:0.1311 loss2:0.4646 loss3:0.0000 | AUC:0.8295 Anomaly AUC:0.9441
[2024-03-11 11:54:52,473][main.py][line:124][INFO] [Epoch:2/20, Batch:169/320]: loss1:0.0870 loss2:0.4212 loss3:0.0000 | AUC:0.7277 Anomaly AUC:0.9124
[2024-03-11 11:55:06,867][main.py][line:124][INFO] [Epoch:2/20, Batch:179/320]: loss1:0.1562 loss2:0.5019 loss3:0.0000 | AUC:0.7391 Anomaly AUC:0.9157
[2024-03-11 11:55:21,248][main.py][line:124][INFO] [Epoch:2/20, Batch:189/320]: loss1:0.1550 loss2:0.4495 loss3:0.0000 | AUC:0.7765 Anomaly AUC:0.9298
[2024-03-11 11:55:35,722][main.py][line:124][INFO] [Epoch:2/20, Batch:199/320]: loss1:0.1494 loss2:0.4189 loss3:0.0000 | AUC:0.7366 Anomaly AUC:0.9145
[2024-03-11 11:55:50,133][main.py][line:124][INFO] [Epoch:2/20, Batch:209/320]: loss1:0.1599 loss2:0.4196 loss3:0.0000 | AUC:0.7644 Anomaly AUC:0.9228
[2024-03-11 11:56:04,453][main.py][line:124][INFO] [Epoch:2/20, Batch:219/320]: loss1:0.1106 loss2:0.3564 loss3:0.0000 | AUC:0.7867 Anomaly AUC:0.9283
[2024-03-11 11:56:18,605][main.py][line:124][INFO] [Epoch:2/20, Batch:229/320]: loss1:0.2240 loss2:0.4714 loss3:0.0000 | AUC:0.7773 Anomaly AUC:0.9273
[2024-03-11 11:56:32,938][main.py][line:124][INFO] [Epoch:2/20, Batch:239/320]: loss1:0.0863 loss2:0.4107 loss3:0.0000 | AUC:0.7763 Anomaly AUC:0.9306
[2024-03-11 11:56:47,402][main.py][line:124][INFO] [Epoch:2/20, Batch:249/320]: loss1:0.0580 loss2:0.3229 loss3:0.0000 | AUC:0.7941 Anomaly AUC:0.9317
[2024-03-11 11:57:01,728][main.py][line:124][INFO] [Epoch:2/20, Batch:259/320]: loss1:0.1109 loss2:0.3415 loss3:0.0000 | AUC:0.7662 Anomaly AUC:0.9216
[2024-03-11 11:57:15,888][main.py][line:124][INFO] [Epoch:2/20, Batch:269/320]: loss1:0.0702 loss2:0.3740 loss3:0.0000 | AUC:0.7724 Anomaly AUC:0.9263
[2024-03-11 11:57:29,959][main.py][line:124][INFO] [Epoch:2/20, Batch:279/320]: loss1:0.0482 loss2:0.2813 loss3:0.0000 | AUC:0.8238 Anomaly AUC:0.9427
[2024-03-11 11:57:44,197][main.py][line:124][INFO] [Epoch:2/20, Batch:289/320]: loss1:0.1291 loss2:0.3580 loss3:0.0000 | AUC:0.7829 Anomaly AUC:0.9327
[2024-03-11 11:57:57,993][main.py][line:153][INFO] [Epoch:2/20]: lr:0.00010 | loss1:0.1180 loss2:0.3177 loss3:0.0000 | AUC:0.7821 Anomaly AUC:0.9336
[2024-03-11 11:58:12,867][main.py][line:124][INFO] [Epoch:3/20, Batch:9/320]: loss1:0.1479 loss2:0.3760 loss3:0.0000 | AUC:0.7963 Anomaly AUC:0.9371
[2024-03-11 11:58:27,159][main.py][line:124][INFO] [Epoch:3/20, Batch:19/320]: loss1:0.1039 loss2:0.3313 loss3:0.0000 | AUC:0.7815 Anomaly AUC:0.9332
[2024-03-11 11:58:41,392][main.py][line:124][INFO] [Epoch:3/20, Batch:29/320]: loss1:0.1496 loss2:0.3911 loss3:0.0000 | AUC:0.7912 Anomaly AUC:0.9359
[2024-03-11 11:58:55,654][main.py][line:124][INFO] [Epoch:3/20, Batch:39/320]: loss1:0.0588 loss2:0.3060 loss3:0.0000 | AUC:0.7922 Anomaly AUC:0.9321
[2024-03-11 11:59:10,014][main.py][line:124][INFO] [Epoch:3/20, Batch:49/320]: loss1:0.0687 loss2:0.3998 loss3:0.0000 | AUC:0.7899 Anomaly AUC:0.9292
[2024-03-11 11:59:24,217][main.py][line:124][INFO] [Epoch:3/20, Batch:59/320]: loss1:0.1367 loss2:0.3939 loss3:0.0000 | AUC:0.7801 Anomaly AUC:0.9284
[2024-03-11 11:59:38,552][main.py][line:124][INFO] [Epoch:3/20, Batch:69/320]: loss1:0.0704 loss2:0.3263 loss3:0.0000 | AUC:0.7523 Anomaly AUC:0.9167
[2024-03-11 11:59:52,887][main.py][line:124][INFO] [Epoch:3/20, Batch:79/320]: loss1:0.0772 loss2:0.3084 loss3:0.0000 | AUC:0.7887 Anomaly AUC:0.9336
[2024-03-11 12:00:07,008][main.py][line:124][INFO] [Epoch:3/20, Batch:89/320]: loss1:0.0809 loss2:0.2969 loss3:0.0000 | AUC:0.8010 Anomaly AUC:0.9371
[2024-03-11 12:00:21,330][main.py][line:124][INFO] [Epoch:3/20, Batch:99/320]: loss1:0.1003 loss2:0.2914 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9327
[2024-03-11 12:00:35,688][main.py][line:124][INFO] [Epoch:3/20, Batch:109/320]: loss1:0.0923 loss2:0.3187 loss3:0.0000 | AUC:0.7836 Anomaly AUC:0.9269
[2024-03-11 12:00:50,189][main.py][line:124][INFO] [Epoch:3/20, Batch:119/320]: loss1:0.0538 loss2:0.3558 loss3:0.0000 | AUC:0.7827 Anomaly AUC:0.9299
[2024-03-11 12:01:04,489][main.py][line:124][INFO] [Epoch:3/20, Batch:129/320]: loss1:0.0461 loss2:0.2460 loss3:0.0000 | AUC:0.7948 Anomaly AUC:0.9342
[2024-03-11 12:01:18,819][main.py][line:124][INFO] [Epoch:3/20, Batch:139/320]: loss1:0.0297 loss2:0.3159 loss3:0.0000 | AUC:0.7822 Anomaly AUC:0.9274
[2024-03-11 12:01:33,171][main.py][line:124][INFO] [Epoch:3/20, Batch:149/320]: loss1:0.0887 loss2:0.2926 loss3:0.0000 | AUC:0.7388 Anomaly AUC:0.9126
[2024-03-11 12:01:47,481][main.py][line:124][INFO] [Epoch:3/20, Batch:159/320]: loss1:0.0800 loss2:0.3083 loss3:0.0000 | AUC:0.7604 Anomaly AUC:0.9249
[2024-03-11 12:02:01,672][main.py][line:124][INFO] [Epoch:3/20, Batch:169/320]: loss1:0.0728 loss2:0.3112 loss3:0.0000 | AUC:0.7976 Anomaly AUC:0.9355
[2024-03-11 12:02:15,962][main.py][line:124][INFO] [Epoch:3/20, Batch:179/320]: loss1:0.0492 loss2:0.3451 loss3:0.0000 | AUC:0.8016 Anomaly AUC:0.9376
[2024-03-11 12:02:29,965][main.py][line:124][INFO] [Epoch:3/20, Batch:189/320]: loss1:0.1423 loss2:0.3774 loss3:0.0000 | AUC:0.7864 Anomaly AUC:0.9285
[2024-03-11 12:02:44,121][main.py][line:124][INFO] [Epoch:3/20, Batch:199/320]: loss1:0.0779 loss2:0.3151 loss3:0.0000 | AUC:0.7763 Anomaly AUC:0.9232
[2024-03-11 12:02:58,664][main.py][line:124][INFO] [Epoch:3/20, Batch:209/320]: loss1:0.0383 loss2:0.2725 loss3:0.0000 | AUC:0.7395 Anomaly AUC:0.9132
[2024-03-11 12:03:12,793][main.py][line:124][INFO] [Epoch:3/20, Batch:219/320]: loss1:0.0434 loss2:0.3064 loss3:0.0000 | AUC:0.8188 Anomaly AUC:0.9398
[2024-03-11 12:03:27,691][main.py][line:124][INFO] [Epoch:3/20, Batch:229/320]: loss1:0.2404 loss2:0.4192 loss3:0.0000 | AUC:0.7908 Anomaly AUC:0.9260
[2024-03-11 12:03:42,191][main.py][line:124][INFO] [Epoch:3/20, Batch:239/320]: loss1:0.0812 loss2:0.3569 loss3:0.0000 | AUC:0.7952 Anomaly AUC:0.9336
[2024-03-11 12:03:57,015][main.py][line:124][INFO] [Epoch:3/20, Batch:249/320]: loss1:0.0398 loss2:0.2463 loss3:0.0000 | AUC:0.8180 Anomaly AUC:0.9411
[2024-03-11 12:04:11,385][main.py][line:124][INFO] [Epoch:3/20, Batch:259/320]: loss1:0.0304 loss2:0.2123 loss3:0.0000 | AUC:0.7744 Anomaly AUC:0.9241
[2024-03-11 12:04:25,635][main.py][line:124][INFO] [Epoch:3/20, Batch:269/320]: loss1:0.0538 loss2:0.3300 loss3:0.0000 | AUC:0.8075 Anomaly AUC:0.9365
[2024-03-11 12:04:39,722][main.py][line:124][INFO] [Epoch:3/20, Batch:279/320]: loss1:0.0207 loss2:0.3095 loss3:0.0000 | AUC:0.7947 Anomaly AUC:0.9286
[2024-03-11 12:04:53,876][main.py][line:124][INFO] [Epoch:3/20, Batch:289/320]: loss1:0.0953 loss2:0.3596 loss3:0.0000 | AUC:0.7722 Anomaly AUC:0.9235
[2024-03-11 12:05:08,854][main.py][line:153][INFO] [Epoch:3/20]: lr:0.00010 | loss1:0.0666 loss2:0.3283 loss3:0.0000 | AUC:0.7659 Anomaly AUC:0.9263
[2024-03-11 12:05:24,284][main.py][line:124][INFO] [Epoch:4/20, Batch:9/320]: loss1:0.0363 loss2:0.2325 loss3:0.0000 | AUC:0.7997 Anomaly AUC:0.9365
[2024-03-11 12:05:38,665][main.py][line:124][INFO] [Epoch:4/20, Batch:19/320]: loss1:0.0361 loss2:0.2171 loss3:0.0000 | AUC:0.7959 Anomaly AUC:0.9344
[2024-03-11 12:05:53,177][main.py][line:124][INFO] [Epoch:4/20, Batch:29/320]: loss1:0.0311 loss2:0.2739 loss3:0.0000 | AUC:0.7886 Anomaly AUC:0.9326
[2024-03-11 12:06:07,889][main.py][line:124][INFO] [Epoch:4/20, Batch:39/320]: loss1:0.0133 loss2:0.2942 loss3:0.0000 | AUC:0.7961 Anomaly AUC:0.9348
[2024-03-11 12:06:23,121][main.py][line:124][INFO] [Epoch:4/20, Batch:49/320]: loss1:0.0917 loss2:0.3130 loss3:0.0000 | AUC:0.7783 Anomaly AUC:0.9302
[2024-03-11 12:06:37,540][main.py][line:124][INFO] [Epoch:4/20, Batch:59/320]: loss1:0.0194 loss2:0.2671 loss3:0.0000 | AUC:0.7510 Anomaly AUC:0.9204
[2024-03-11 12:06:51,662][main.py][line:124][INFO] [Epoch:4/20, Batch:69/320]: loss1:0.0940 loss2:0.3481 loss3:0.0000 | AUC:0.7695 Anomaly AUC:0.9274
[2024-03-11 12:07:05,800][main.py][line:124][INFO] [Epoch:4/20, Batch:79/320]: loss1:0.0128 loss2:0.2058 loss3:0.0000 | AUC:0.7918 Anomaly AUC:0.9332
[2024-03-11 12:07:20,181][main.py][line:124][INFO] [Epoch:4/20, Batch:89/320]: loss1:0.0986 loss2:0.3048 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9249
[2024-03-11 12:07:34,536][main.py][line:124][INFO] [Epoch:4/20, Batch:99/320]: loss1:0.1242 loss2:0.2620 loss3:0.0000 | AUC:0.7663 Anomaly AUC:0.9289
[2024-03-11 12:07:48,663][main.py][line:124][INFO] [Epoch:4/20, Batch:109/320]: loss1:0.0405 loss2:0.2464 loss3:0.0000 | AUC:0.8061 Anomaly AUC:0.9380
[2024-03-11 12:08:02,972][main.py][line:124][INFO] [Epoch:4/20, Batch:119/320]: loss1:0.0680 loss2:0.2144 loss3:0.0000 | AUC:0.7901 Anomaly AUC:0.9329
[2024-03-11 12:08:17,284][main.py][line:124][INFO] [Epoch:4/20, Batch:129/320]: loss1:0.1859 loss2:0.3005 loss3:0.0000 | AUC:0.7827 Anomaly AUC:0.9304
[2024-03-11 12:08:32,008][main.py][line:124][INFO] [Epoch:4/20, Batch:139/320]: loss1:0.0147 loss2:0.2943 loss3:0.0000 | AUC:0.7927 Anomaly AUC:0.9321
[2024-03-11 12:08:46,963][main.py][line:124][INFO] [Epoch:4/20, Batch:149/320]: loss1:0.0122 loss2:0.2853 loss3:0.0000 | AUC:0.7828 Anomaly AUC:0.9292
[2024-03-11 12:09:02,016][main.py][line:124][INFO] [Epoch:4/20, Batch:159/320]: loss1:0.0175 loss2:0.2189 loss3:0.0000 | AUC:0.7828 Anomaly AUC:0.9299
[2024-03-11 12:09:16,481][main.py][line:124][INFO] [Epoch:4/20, Batch:169/320]: loss1:0.0683 loss2:0.2589 loss3:0.0000 | AUC:0.7856 Anomaly AUC:0.9303
[2024-03-11 12:09:30,661][main.py][line:124][INFO] [Epoch:4/20, Batch:179/320]: loss1:0.0304 loss2:0.1642 loss3:0.0000 | AUC:0.7644 Anomaly AUC:0.9251
[2024-03-11 12:09:44,875][main.py][line:124][INFO] [Epoch:4/20, Batch:189/320]: loss1:0.0106 loss2:0.2693 loss3:0.0000 | AUC:0.8015 Anomaly AUC:0.9356
[2024-03-11 12:09:58,988][main.py][line:124][INFO] [Epoch:4/20, Batch:199/320]: loss1:0.0534 loss2:0.2986 loss3:0.0000 | AUC:0.8027 Anomaly AUC:0.9354
[2024-03-11 12:10:13,221][main.py][line:124][INFO] [Epoch:4/20, Batch:209/320]: loss1:0.0786 loss2:0.2686 loss3:0.0000 | AUC:0.7944 Anomaly AUC:0.9351
[2024-03-11 12:10:27,404][main.py][line:124][INFO] [Epoch:4/20, Batch:219/320]: loss1:0.0164 loss2:0.2998 loss3:0.0000 | AUC:0.8023 Anomaly AUC:0.9374
[2024-03-11 12:10:41,667][main.py][line:124][INFO] [Epoch:4/20, Batch:229/320]: loss1:0.1202 loss2:0.2973 loss3:0.0000 | AUC:0.7807 Anomaly AUC:0.9286
[2024-03-11 12:10:56,063][main.py][line:124][INFO] [Epoch:4/20, Batch:239/320]: loss1:0.0180 loss2:0.2266 loss3:0.0000 | AUC:0.7582 Anomaly AUC:0.9198
[2024-03-11 12:11:11,229][main.py][line:124][INFO] [Epoch:4/20, Batch:249/320]: loss1:0.1113 loss2:0.2359 loss3:0.0000 | AUC:0.7838 Anomaly AUC:0.9321
[2024-03-11 12:11:25,562][main.py][line:124][INFO] [Epoch:4/20, Batch:259/320]: loss1:0.1225 loss2:0.2813 loss3:0.0000 | AUC:0.7959 Anomaly AUC:0.9351
[2024-03-11 12:11:39,872][main.py][line:124][INFO] [Epoch:4/20, Batch:269/320]: loss1:0.0482 loss2:0.2368 loss3:0.0000 | AUC:0.8117 Anomaly AUC:0.9393
[2024-03-11 12:11:54,846][main.py][line:124][INFO] [Epoch:4/20, Batch:279/320]: loss1:0.0189 loss2:0.2094 loss3:0.0000 | AUC:0.8103 Anomaly AUC:0.9398
[2024-03-11 12:12:09,270][main.py][line:124][INFO] [Epoch:4/20, Batch:289/320]: loss1:0.0317 loss2:0.2288 loss3:0.0000 | AUC:0.7960 Anomaly AUC:0.9362
[2024-03-11 12:12:25,259][main.py][line:153][INFO] [Epoch:4/20]: lr:0.00010 | loss1:0.0455 loss2:0.2078 loss3:0.0000 | AUC:0.7897 Anomaly AUC:0.9334
[2024-03-11 12:12:41,104][main.py][line:124][INFO] [Epoch:5/20, Batch:9/320]: loss1:0.0152 loss2:0.2569 loss3:0.0000 | AUC:0.7846 Anomaly AUC:0.9302
[2024-03-11 12:12:56,101][main.py][line:124][INFO] [Epoch:5/20, Batch:19/320]: loss1:0.0230 loss2:0.1938 loss3:0.0000 | AUC:0.7765 Anomaly AUC:0.9294
[2024-03-11 12:13:10,670][main.py][line:124][INFO] [Epoch:5/20, Batch:29/320]: loss1:0.0174 loss2:0.2596 loss3:0.0000 | AUC:0.8059 Anomaly AUC:0.9366
[2024-03-11 12:13:25,529][main.py][line:124][INFO] [Epoch:5/20, Batch:39/320]: loss1:0.0062 loss2:0.1991 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9362
[2024-03-11 12:13:39,882][main.py][line:124][INFO] [Epoch:5/20, Batch:49/320]: loss1:0.0222 loss2:0.1657 loss3:0.0000 | AUC:0.8001 Anomaly AUC:0.9376
[2024-03-11 12:13:54,268][main.py][line:124][INFO] [Epoch:5/20, Batch:59/320]: loss1:0.0181 loss2:0.1981 loss3:0.0000 | AUC:0.7816 Anomaly AUC:0.9305
[2024-03-11 12:14:09,464][main.py][line:124][INFO] [Epoch:5/20, Batch:69/320]: loss1:0.1991 loss2:0.3474 loss3:0.0000 | AUC:0.8104 Anomaly AUC:0.9390
[2024-03-11 12:14:23,677][main.py][line:124][INFO] [Epoch:5/20, Batch:79/320]: loss1:0.0793 loss2:0.2598 loss3:0.0000 | AUC:0.8042 Anomaly AUC:0.9380
[2024-03-11 12:14:38,056][main.py][line:124][INFO] [Epoch:5/20, Batch:89/320]: loss1:0.1796 loss2:0.3765 loss3:0.0000 | AUC:0.7834 Anomaly AUC:0.9347
[2024-03-11 12:14:52,898][main.py][line:124][INFO] [Epoch:5/20, Batch:99/320]: loss1:0.1601 loss2:0.3182 loss3:0.0000 | AUC:0.7575 Anomaly AUC:0.9271
[2024-03-11 12:15:08,089][main.py][line:124][INFO] [Epoch:5/20, Batch:109/320]: loss1:0.0609 loss2:0.2527 loss3:0.0000 | AUC:0.7714 Anomaly AUC:0.9316
[2024-03-11 12:15:23,068][main.py][line:124][INFO] [Epoch:5/20, Batch:119/320]: loss1:0.0616 loss2:0.2516 loss3:0.0000 | AUC:0.8159 Anomaly AUC:0.9383
[2024-03-11 12:15:37,694][main.py][line:124][INFO] [Epoch:5/20, Batch:129/320]: loss1:0.0891 loss2:0.3523 loss3:0.0000 | AUC:0.7719 Anomaly AUC:0.9237
[2024-03-11 12:15:52,467][main.py][line:124][INFO] [Epoch:5/20, Batch:139/320]: loss1:0.0442 loss2:0.2014 loss3:0.0000 | AUC:0.7778 Anomaly AUC:0.9291
[2024-03-11 12:16:07,336][main.py][line:124][INFO] [Epoch:5/20, Batch:149/320]: loss1:0.0166 loss2:0.2030 loss3:0.0000 | AUC:0.8003 Anomaly AUC:0.9309
[2024-03-11 12:16:21,641][main.py][line:124][INFO] [Epoch:5/20, Batch:159/320]: loss1:0.0146 loss2:0.2579 loss3:0.0000 | AUC:0.8042 Anomaly AUC:0.9351
[2024-03-11 12:16:36,327][main.py][line:124][INFO] [Epoch:5/20, Batch:169/320]: loss1:0.0142 loss2:0.1644 loss3:0.0000 | AUC:0.7511 Anomaly AUC:0.9244
[2024-03-11 12:16:50,837][main.py][line:124][INFO] [Epoch:5/20, Batch:179/320]: loss1:0.0123 loss2:0.2138 loss3:0.0000 | AUC:0.7834 Anomaly AUC:0.9290
[2024-03-11 12:17:05,612][main.py][line:124][INFO] [Epoch:5/20, Batch:189/320]: loss1:0.0774 loss2:0.2007 loss3:0.0000 | AUC:0.7859 Anomaly AUC:0.9303
[2024-03-11 12:17:20,482][main.py][line:124][INFO] [Epoch:5/20, Batch:199/320]: loss1:0.0566 loss2:0.2444 loss3:0.0000 | AUC:0.7977 Anomaly AUC:0.9357
[2024-03-11 12:17:35,463][main.py][line:124][INFO] [Epoch:5/20, Batch:209/320]: loss1:0.0374 loss2:0.1837 loss3:0.0000 | AUC:0.8009 Anomaly AUC:0.9369
[2024-03-11 12:17:50,423][main.py][line:124][INFO] [Epoch:5/20, Batch:219/320]: loss1:0.0314 loss2:0.2627 loss3:0.0000 | AUC:0.8065 Anomaly AUC:0.9401
[2024-03-11 12:18:04,869][main.py][line:124][INFO] [Epoch:5/20, Batch:229/320]: loss1:0.0344 loss2:0.2446 loss3:0.0000 | AUC:0.8035 Anomaly AUC:0.9380
[2024-03-11 12:18:19,774][main.py][line:124][INFO] [Epoch:5/20, Batch:239/320]: loss1:0.0164 loss2:0.2259 loss3:0.0000 | AUC:0.7875 Anomaly AUC:0.9326
[2024-03-11 12:18:34,433][main.py][line:124][INFO] [Epoch:5/20, Batch:249/320]: loss1:0.0131 loss2:0.1685 loss3:0.0000 | AUC:0.7467 Anomaly AUC:0.9212
[2024-03-11 12:18:49,617][main.py][line:124][INFO] [Epoch:5/20, Batch:259/320]: loss1:0.0297 loss2:0.2586 loss3:0.0000 | AUC:0.7867 Anomaly AUC:0.9330
[2024-03-11 12:19:04,775][main.py][line:124][INFO] [Epoch:5/20, Batch:269/320]: loss1:0.0157 loss2:0.1511 loss3:0.0000 | AUC:0.8038 Anomaly AUC:0.9366
[2024-03-11 12:19:19,071][main.py][line:124][INFO] [Epoch:5/20, Batch:279/320]: loss1:0.0637 loss2:0.2152 loss3:0.0000 | AUC:0.7730 Anomaly AUC:0.9286
[2024-03-11 12:19:33,927][main.py][line:124][INFO] [Epoch:5/20, Batch:289/320]: loss1:0.0120 loss2:0.1856 loss3:0.0000 | AUC:0.7846 Anomaly AUC:0.9341
[2024-03-11 12:19:48,579][main.py][line:153][INFO] [Epoch:5/20]: lr:0.00010 | loss1:0.0202 loss2:0.2193 loss3:0.0000 | AUC:0.7573 Anomaly AUC:0.9242
[2024-03-11 12:20:03,912][main.py][line:124][INFO] [Epoch:6/20, Batch:9/320]: loss1:0.0147 loss2:0.1718 loss3:0.0000 | AUC:0.7839 Anomaly AUC:0.9311
[2024-03-11 12:20:18,127][main.py][line:124][INFO] [Epoch:6/20, Batch:19/320]: loss1:0.0166 loss2:0.1471 loss3:0.0000 | AUC:0.8118 Anomaly AUC:0.9414
[2024-03-11 12:20:32,488][main.py][line:124][INFO] [Epoch:6/20, Batch:29/320]: loss1:0.0067 loss2:0.1486 loss3:0.0000 | AUC:0.7614 Anomaly AUC:0.9239
[2024-03-11 12:20:46,955][main.py][line:124][INFO] [Epoch:6/20, Batch:39/320]: loss1:0.0826 loss2:0.2320 loss3:0.0000 | AUC:0.8058 Anomaly AUC:0.9394
[2024-03-11 12:21:01,328][main.py][line:124][INFO] [Epoch:6/20, Batch:49/320]: loss1:0.0071 loss2:0.2152 loss3:0.0000 | AUC:0.8029 Anomaly AUC:0.9376
[2024-03-11 12:21:16,153][main.py][line:124][INFO] [Epoch:6/20, Batch:59/320]: loss1:0.0385 loss2:0.2484 loss3:0.0000 | AUC:0.7896 Anomaly AUC:0.9339
[2024-03-11 12:21:30,436][main.py][line:124][INFO] [Epoch:6/20, Batch:69/320]: loss1:0.0677 loss2:0.1992 loss3:0.0000 | AUC:0.7788 Anomaly AUC:0.9331
[2024-03-11 12:21:44,715][main.py][line:124][INFO] [Epoch:6/20, Batch:79/320]: loss1:0.0994 loss2:0.2421 loss3:0.0000 | AUC:0.8097 Anomaly AUC:0.9380
[2024-03-11 12:21:58,893][main.py][line:124][INFO] [Epoch:6/20, Batch:89/320]: loss1:0.0626 loss2:0.2180 loss3:0.0000 | AUC:0.8030 Anomaly AUC:0.9366
[2024-03-11 12:22:13,393][main.py][line:124][INFO] [Epoch:6/20, Batch:99/320]: loss1:0.0137 loss2:0.2395 loss3:0.0000 | AUC:0.8053 Anomaly AUC:0.9404
[2024-03-11 12:22:27,588][main.py][line:124][INFO] [Epoch:6/20, Batch:109/320]: loss1:0.0612 loss2:0.2480 loss3:0.0000 | AUC:0.8067 Anomaly AUC:0.9397
[2024-03-11 12:22:41,922][main.py][line:124][INFO] [Epoch:6/20, Batch:119/320]: loss1:0.0437 loss2:0.1504 loss3:0.0000 | AUC:0.7728 Anomaly AUC:0.9312
[2024-03-11 12:22:56,363][main.py][line:124][INFO] [Epoch:6/20, Batch:129/320]: loss1:0.0150 loss2:0.1191 loss3:0.0000 | AUC:0.7593 Anomaly AUC:0.9262
[2024-03-11 12:23:11,254][main.py][line:124][INFO] [Epoch:6/20, Batch:139/320]: loss1:0.0167 loss2:0.2386 loss3:0.0000 | AUC:0.7849 Anomaly AUC:0.9299
[2024-03-11 12:23:25,789][main.py][line:124][INFO] [Epoch:6/20, Batch:149/320]: loss1:0.0107 loss2:0.2379 loss3:0.0000 | AUC:0.7728 Anomaly AUC:0.9274
[2024-03-11 12:23:40,733][main.py][line:124][INFO] [Epoch:6/20, Batch:159/320]: loss1:0.0075 loss2:0.1830 loss3:0.0000 | AUC:0.8013 Anomaly AUC:0.9366
[2024-03-11 12:23:55,090][main.py][line:124][INFO] [Epoch:6/20, Batch:169/320]: loss1:0.0407 loss2:0.2039 loss3:0.0000 | AUC:0.8000 Anomaly AUC:0.9331
[2024-03-11 12:24:09,671][main.py][line:124][INFO] [Epoch:6/20, Batch:179/320]: loss1:0.0183 loss2:0.2006 loss3:0.0000 | AUC:0.7876 Anomaly AUC:0.9313
[2024-03-11 12:24:24,502][main.py][line:124][INFO] [Epoch:6/20, Batch:189/320]: loss1:0.0272 loss2:0.2569 loss3:0.0000 | AUC:0.7808 Anomaly AUC:0.9294
[2024-03-11 12:24:38,870][main.py][line:124][INFO] [Epoch:6/20, Batch:199/320]: loss1:0.0096 loss2:0.1761 loss3:0.0000 | AUC:0.8004 Anomaly AUC:0.9381
[2024-03-11 12:24:53,952][main.py][line:124][INFO] [Epoch:6/20, Batch:209/320]: loss1:0.0857 loss2:0.2461 loss3:0.0000 | AUC:0.7650 Anomaly AUC:0.9208
[2024-03-11 12:25:08,319][main.py][line:124][INFO] [Epoch:6/20, Batch:219/320]: loss1:0.0390 loss2:0.2110 loss3:0.0000 | AUC:0.7597 Anomaly AUC:0.9237
[2024-03-11 12:25:22,647][main.py][line:124][INFO] [Epoch:6/20, Batch:229/320]: loss1:0.0143 loss2:0.2516 loss3:0.0000 | AUC:0.7907 Anomaly AUC:0.9321
[2024-03-11 12:25:36,820][main.py][line:124][INFO] [Epoch:6/20, Batch:239/320]: loss1:0.0096 loss2:0.1833 loss3:0.0000 | AUC:0.7891 Anomaly AUC:0.9309
[2024-03-11 12:25:50,933][main.py][line:124][INFO] [Epoch:6/20, Batch:249/320]: loss1:0.0116 loss2:0.1510 loss3:0.0000 | AUC:0.7832 Anomaly AUC:0.9314
[2024-03-11 12:26:05,073][main.py][line:124][INFO] [Epoch:6/20, Batch:259/320]: loss1:0.0351 loss2:0.1951 loss3:0.0000 | AUC:0.8070 Anomaly AUC:0.9382
[2024-03-11 12:26:19,349][main.py][line:124][INFO] [Epoch:6/20, Batch:269/320]: loss1:0.0172 loss2:0.1569 loss3:0.0000 | AUC:0.7944 Anomaly AUC:0.9367
[2024-03-11 12:26:33,581][main.py][line:124][INFO] [Epoch:6/20, Batch:279/320]: loss1:0.0155 loss2:0.2076 loss3:0.0000 | AUC:0.8163 Anomaly AUC:0.9413
[2024-03-11 12:26:47,748][main.py][line:124][INFO] [Epoch:6/20, Batch:289/320]: loss1:0.0272 loss2:0.2022 loss3:0.0000 | AUC:0.7698 Anomaly AUC:0.9281
[2024-03-11 12:27:02,572][main.py][line:153][INFO] [Epoch:6/20]: lr:0.00010 | loss1:0.0092 loss2:0.1631 loss3:0.0000 | AUC:0.7615 Anomaly AUC:0.9290
[2024-03-11 12:27:18,592][main.py][line:124][INFO] [Epoch:7/20, Batch:9/320]: loss1:0.0394 loss2:0.1972 loss3:0.0000 | AUC:0.7930 Anomaly AUC:0.9348
[2024-03-11 12:27:32,689][main.py][line:124][INFO] [Epoch:7/20, Batch:19/320]: loss1:0.0057 loss2:0.1611 loss3:0.0000 | AUC:0.7630 Anomaly AUC:0.9236
[2024-03-11 12:27:47,195][main.py][line:124][INFO] [Epoch:7/20, Batch:29/320]: loss1:0.0094 loss2:0.2075 loss3:0.0000 | AUC:0.7716 Anomaly AUC:0.9275
[2024-03-11 12:28:01,616][main.py][line:124][INFO] [Epoch:7/20, Batch:39/320]: loss1:0.0208 loss2:0.1905 loss3:0.0000 | AUC:0.7389 Anomaly AUC:0.9194
[2024-03-11 12:28:15,774][main.py][line:124][INFO] [Epoch:7/20, Batch:49/320]: loss1:0.0112 loss2:0.1596 loss3:0.0000 | AUC:0.7738 Anomaly AUC:0.9307
[2024-03-11 12:28:30,479][main.py][line:124][INFO] [Epoch:7/20, Batch:59/320]: loss1:0.0234 loss2:0.1611 loss3:0.0000 | AUC:0.8070 Anomaly AUC:0.9398
[2024-03-11 12:28:45,878][main.py][line:124][INFO] [Epoch:7/20, Batch:69/320]: loss1:0.0064 loss2:0.1225 loss3:0.0000 | AUC:0.7956 Anomaly AUC:0.9361
[2024-03-11 12:29:00,057][main.py][line:124][INFO] [Epoch:7/20, Batch:79/320]: loss1:0.0189 loss2:0.1414 loss3:0.0000 | AUC:0.7994 Anomaly AUC:0.9357
[2024-03-11 12:29:14,561][main.py][line:124][INFO] [Epoch:7/20, Batch:89/320]: loss1:0.0130 loss2:0.1613 loss3:0.0000 | AUC:0.7402 Anomaly AUC:0.9200
[2024-03-11 12:29:28,809][main.py][line:124][INFO] [Epoch:7/20, Batch:99/320]: loss1:0.0097 loss2:0.1805 loss3:0.0000 | AUC:0.7813 Anomaly AUC:0.9321
[2024-03-11 12:29:43,063][main.py][line:124][INFO] [Epoch:7/20, Batch:109/320]: loss1:0.0085 loss2:0.1424 loss3:0.0000 | AUC:0.7916 Anomaly AUC:0.9341
[2024-03-11 12:29:58,708][main.py][line:124][INFO] [Epoch:7/20, Batch:119/320]: loss1:0.0195 loss2:0.1839 loss3:0.0000 | AUC:0.7894 Anomaly AUC:0.9365
[2024-03-11 12:30:14,592][main.py][line:124][INFO] [Epoch:7/20, Batch:129/320]: loss1:0.0284 loss2:0.1075 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9374
[2024-03-11 12:30:29,115][main.py][line:124][INFO] [Epoch:7/20, Batch:139/320]: loss1:0.0074 loss2:0.1827 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9393
[2024-03-11 12:30:43,413][main.py][line:124][INFO] [Epoch:7/20, Batch:149/320]: loss1:0.0085 loss2:0.2105 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9366
[2024-03-11 12:30:58,324][main.py][line:124][INFO] [Epoch:7/20, Batch:159/320]: loss1:0.0074 loss2:0.1095 loss3:0.0000 | AUC:0.7988 Anomaly AUC:0.9376
[2024-03-11 12:31:12,563][main.py][line:124][INFO] [Epoch:7/20, Batch:169/320]: loss1:0.0065 loss2:0.1331 loss3:0.0000 | AUC:0.7870 Anomaly AUC:0.9318
[2024-03-11 12:31:27,035][main.py][line:124][INFO] [Epoch:7/20, Batch:179/320]: loss1:0.0075 loss2:0.1522 loss3:0.0000 | AUC:0.7970 Anomaly AUC:0.9373
[2024-03-11 12:31:41,399][main.py][line:124][INFO] [Epoch:7/20, Batch:189/320]: loss1:0.0292 loss2:0.1849 loss3:0.0000 | AUC:0.8121 Anomaly AUC:0.9423
[2024-03-11 12:31:55,524][main.py][line:124][INFO] [Epoch:7/20, Batch:199/320]: loss1:0.1059 loss2:0.1626 loss3:0.0000 | AUC:0.7860 Anomaly AUC:0.9372
[2024-03-11 12:32:10,608][main.py][line:124][INFO] [Epoch:7/20, Batch:209/320]: loss1:0.0658 loss2:0.2206 loss3:0.0000 | AUC:0.7457 Anomaly AUC:0.9172
[2024-03-11 12:32:24,804][main.py][line:124][INFO] [Epoch:7/20, Batch:219/320]: loss1:0.0071 loss2:0.1926 loss3:0.0000 | AUC:0.7969 Anomaly AUC:0.9339
[2024-03-11 12:32:38,966][main.py][line:124][INFO] [Epoch:7/20, Batch:229/320]: loss1:0.0087 loss2:0.1151 loss3:0.0000 | AUC:0.7873 Anomaly AUC:0.9326
[2024-03-11 12:32:54,518][main.py][line:124][INFO] [Epoch:7/20, Batch:239/320]: loss1:0.0083 loss2:0.1461 loss3:0.0000 | AUC:0.8120 Anomaly AUC:0.9406
[2024-03-11 12:33:08,736][main.py][line:124][INFO] [Epoch:7/20, Batch:249/320]: loss1:0.0377 loss2:0.1980 loss3:0.0000 | AUC:0.8109 Anomaly AUC:0.9403
[2024-03-11 12:33:23,111][main.py][line:124][INFO] [Epoch:7/20, Batch:259/320]: loss1:0.0241 loss2:0.1344 loss3:0.0000 | AUC:0.7955 Anomaly AUC:0.9344
[2024-03-11 12:33:37,444][main.py][line:124][INFO] [Epoch:7/20, Batch:269/320]: loss1:0.0161 loss2:0.2350 loss3:0.0000 | AUC:0.8023 Anomaly AUC:0.9342
[2024-03-11 12:33:51,836][main.py][line:124][INFO] [Epoch:7/20, Batch:279/320]: loss1:0.0090 loss2:0.1585 loss3:0.0000 | AUC:0.7650 Anomaly AUC:0.9282
[2024-03-11 12:34:06,517][main.py][line:124][INFO] [Epoch:7/20, Batch:289/320]: loss1:0.0237 loss2:0.2000 loss3:0.0000 | AUC:0.7557 Anomaly AUC:0.9265
[2024-03-11 12:34:20,264][main.py][line:153][INFO] [Epoch:7/20]: lr:0.00010 | loss1:0.0076 loss2:0.1376 loss3:0.0000 | AUC:0.7592 Anomaly AUC:0.9240
[2024-03-11 12:34:34,739][main.py][line:124][INFO] [Epoch:8/20, Batch:9/320]: loss1:0.0146 loss2:0.1115 loss3:0.0000 | AUC:0.7961 Anomaly AUC:0.9362
[2024-03-11 12:34:49,625][main.py][line:124][INFO] [Epoch:8/20, Batch:19/320]: loss1:0.0063 loss2:0.1354 loss3:0.0000 | AUC:0.7986 Anomaly AUC:0.9367
[2024-03-11 12:35:04,688][main.py][line:124][INFO] [Epoch:8/20, Batch:29/320]: loss1:0.0123 loss2:0.1016 loss3:0.0000 | AUC:0.7955 Anomaly AUC:0.9347
[2024-03-11 12:35:19,791][main.py][line:124][INFO] [Epoch:8/20, Batch:39/320]: loss1:0.0118 loss2:0.1203 loss3:0.0000 | AUC:0.7833 Anomaly AUC:0.9315
[2024-03-11 12:35:35,200][main.py][line:124][INFO] [Epoch:8/20, Batch:49/320]: loss1:0.0101 loss2:0.1621 loss3:0.0000 | AUC:0.8023 Anomaly AUC:0.9370
[2024-03-11 12:35:49,538][main.py][line:124][INFO] [Epoch:8/20, Batch:59/320]: loss1:0.0697 loss2:0.2041 loss3:0.0000 | AUC:0.7755 Anomaly AUC:0.9293
[2024-03-11 12:36:03,760][main.py][line:124][INFO] [Epoch:8/20, Batch:69/320]: loss1:0.0182 loss2:0.1612 loss3:0.0000 | AUC:0.7713 Anomaly AUC:0.9286
[2024-03-11 12:36:18,098][main.py][line:124][INFO] [Epoch:8/20, Batch:79/320]: loss1:0.0226 loss2:0.1260 loss3:0.0000 | AUC:0.7897 Anomaly AUC:0.9344
[2024-03-11 12:36:33,131][main.py][line:124][INFO] [Epoch:8/20, Batch:89/320]: loss1:0.0289 loss2:0.1234 loss3:0.0000 | AUC:0.7906 Anomaly AUC:0.9306
[2024-03-11 12:36:47,835][main.py][line:124][INFO] [Epoch:8/20, Batch:99/320]: loss1:0.0057 loss2:0.1613 loss3:0.0000 | AUC:0.7976 Anomaly AUC:0.9347
[2024-03-11 12:37:02,760][main.py][line:124][INFO] [Epoch:8/20, Batch:109/320]: loss1:0.0065 loss2:0.1456 loss3:0.0000 | AUC:0.7948 Anomaly AUC:0.9360
[2024-03-11 12:37:16,975][main.py][line:124][INFO] [Epoch:8/20, Batch:119/320]: loss1:0.0240 loss2:0.1771 loss3:0.0000 | AUC:0.7877 Anomaly AUC:0.9316
[2024-03-11 12:37:31,150][main.py][line:124][INFO] [Epoch:8/20, Batch:129/320]: loss1:0.0529 loss2:0.2225 loss3:0.0000 | AUC:0.7879 Anomaly AUC:0.9288
[2024-03-11 12:37:45,242][main.py][line:124][INFO] [Epoch:8/20, Batch:139/320]: loss1:0.0193 loss2:0.1099 loss3:0.0000 | AUC:0.8068 Anomaly AUC:0.9386
[2024-03-11 12:37:59,491][main.py][line:124][INFO] [Epoch:8/20, Batch:149/320]: loss1:0.0197 loss2:0.1530 loss3:0.0000 | AUC:0.8137 Anomaly AUC:0.9415
[2024-03-11 12:38:13,737][main.py][line:124][INFO] [Epoch:8/20, Batch:159/320]: loss1:0.0260 loss2:0.1842 loss3:0.0000 | AUC:0.7464 Anomaly AUC:0.9214
[2024-03-11 12:38:27,985][main.py][line:124][INFO] [Epoch:8/20, Batch:169/320]: loss1:0.0771 loss2:0.1337 loss3:0.0000 | AUC:0.7732 Anomaly AUC:0.9266
[2024-03-11 12:38:42,807][main.py][line:124][INFO] [Epoch:8/20, Batch:179/320]: loss1:0.0344 loss2:0.2078 loss3:0.0000 | AUC:0.8002 Anomaly AUC:0.9367
[2024-03-11 12:38:57,469][main.py][line:124][INFO] [Epoch:8/20, Batch:189/320]: loss1:0.0455 loss2:0.1408 loss3:0.0000 | AUC:0.8080 Anomaly AUC:0.9413
[2024-03-11 12:39:12,323][main.py][line:124][INFO] [Epoch:8/20, Batch:199/320]: loss1:0.0109 loss2:0.1175 loss3:0.0000 | AUC:0.7925 Anomaly AUC:0.9375
[2024-03-11 12:39:27,106][main.py][line:124][INFO] [Epoch:8/20, Batch:209/320]: loss1:0.0092 loss2:0.1274 loss3:0.0000 | AUC:0.7801 Anomaly AUC:0.9314
[2024-03-11 12:39:41,580][main.py][line:124][INFO] [Epoch:8/20, Batch:219/320]: loss1:0.0052 loss2:0.1220 loss3:0.0000 | AUC:0.7690 Anomaly AUC:0.9240
[2024-03-11 12:39:56,497][main.py][line:124][INFO] [Epoch:8/20, Batch:229/320]: loss1:0.0064 loss2:0.1090 loss3:0.0000 | AUC:0.7864 Anomaly AUC:0.9322
[2024-03-11 12:40:10,849][main.py][line:124][INFO] [Epoch:8/20, Batch:239/320]: loss1:0.0060 loss2:0.1350 loss3:0.0000 | AUC:0.8055 Anomaly AUC:0.9385
[2024-03-11 12:40:25,216][main.py][line:124][INFO] [Epoch:8/20, Batch:249/320]: loss1:0.0073 loss2:0.1231 loss3:0.0000 | AUC:0.7990 Anomaly AUC:0.9372
[2024-03-11 12:40:40,072][main.py][line:124][INFO] [Epoch:8/20, Batch:259/320]: loss1:0.0172 loss2:0.0853 loss3:0.0000 | AUC:0.7880 Anomaly AUC:0.9339
[2024-03-11 12:40:54,985][main.py][line:124][INFO] [Epoch:8/20, Batch:269/320]: loss1:0.0066 loss2:0.1090 loss3:0.0000 | AUC:0.7618 Anomaly AUC:0.9240
[2024-03-11 12:41:09,813][main.py][line:124][INFO] [Epoch:8/20, Batch:279/320]: loss1:0.1316 loss2:0.2172 loss3:0.0000 | AUC:0.7795 Anomaly AUC:0.9260
[2024-03-11 12:41:24,212][main.py][line:124][INFO] [Epoch:8/20, Batch:289/320]: loss1:0.0773 loss2:0.1660 loss3:0.0000 | AUC:0.7925 Anomaly AUC:0.9355
[2024-03-11 12:41:39,175][main.py][line:153][INFO] [Epoch:8/20]: lr:0.00010 | loss1:0.0096 loss2:0.1028 loss3:0.0000 | AUC:0.8215 Anomaly AUC:0.9442
[2024-03-11 12:41:55,192][main.py][line:124][INFO] [Epoch:9/20, Batch:9/320]: loss1:0.0101 loss2:0.0937 loss3:0.0000 | AUC:0.8074 Anomaly AUC:0.9416
[2024-03-11 12:42:09,680][main.py][line:124][INFO] [Epoch:9/20, Batch:19/320]: loss1:0.0164 loss2:0.1378 loss3:0.0000 | AUC:0.7898 Anomaly AUC:0.9343
[2024-03-11 12:42:24,719][main.py][line:124][INFO] [Epoch:9/20, Batch:29/320]: loss1:0.0134 loss2:0.0812 loss3:0.0000 | AUC:0.8162 Anomaly AUC:0.9429
[2024-03-11 12:42:39,163][main.py][line:124][INFO] [Epoch:9/20, Batch:39/320]: loss1:0.0129 loss2:0.1173 loss3:0.0000 | AUC:0.7965 Anomaly AUC:0.9339
[2024-03-11 12:42:53,521][main.py][line:124][INFO] [Epoch:9/20, Batch:49/320]: loss1:0.0053 loss2:0.1157 loss3:0.0000 | AUC:0.7803 Anomaly AUC:0.9322
[2024-03-11 12:43:08,038][main.py][line:124][INFO] [Epoch:9/20, Batch:59/320]: loss1:0.0130 loss2:0.1096 loss3:0.0000 | AUC:0.7536 Anomaly AUC:0.9278
[2024-03-11 12:43:22,300][main.py][line:124][INFO] [Epoch:9/20, Batch:69/320]: loss1:0.0081 loss2:0.1321 loss3:0.0000 | AUC:0.7803 Anomaly AUC:0.9335
[2024-03-11 12:43:36,727][main.py][line:124][INFO] [Epoch:9/20, Batch:79/320]: loss1:0.0858 loss2:0.2152 loss3:0.0000 | AUC:0.7643 Anomaly AUC:0.9256
[2024-03-11 12:43:50,978][main.py][line:124][INFO] [Epoch:9/20, Batch:89/320]: loss1:0.0181 loss2:0.1585 loss3:0.0000 | AUC:0.7949 Anomaly AUC:0.9341
[2024-03-11 12:44:05,127][main.py][line:124][INFO] [Epoch:9/20, Batch:99/320]: loss1:0.0349 loss2:0.2496 loss3:0.0000 | AUC:0.7806 Anomaly AUC:0.9330
[2024-03-11 12:44:20,152][main.py][line:124][INFO] [Epoch:9/20, Batch:109/320]: loss1:0.0315 loss2:0.1523 loss3:0.0000 | AUC:0.7587 Anomaly AUC:0.9353
[2024-03-11 12:44:34,716][main.py][line:124][INFO] [Epoch:9/20, Batch:119/320]: loss1:0.0109 loss2:0.0904 loss3:0.0000 | AUC:0.7849 Anomaly AUC:0.9344
[2024-03-11 12:44:49,174][main.py][line:124][INFO] [Epoch:9/20, Batch:129/320]: loss1:0.0100 loss2:0.1226 loss3:0.0000 | AUC:0.7915 Anomaly AUC:0.9368
[2024-03-11 12:45:04,746][main.py][line:124][INFO] [Epoch:9/20, Batch:139/320]: loss1:0.0151 loss2:0.1152 loss3:0.0000 | AUC:0.7999 Anomaly AUC:0.9402
[2024-03-11 12:45:19,676][main.py][line:124][INFO] [Epoch:9/20, Batch:149/320]: loss1:0.0065 loss2:0.1041 loss3:0.0000 | AUC:0.7891 Anomaly AUC:0.9349
[2024-03-11 12:45:34,790][main.py][line:124][INFO] [Epoch:9/20, Batch:159/320]: loss1:0.0056 loss2:0.1415 loss3:0.0000 | AUC:0.7686 Anomaly AUC:0.9300
[2024-03-11 12:45:49,199][main.py][line:124][INFO] [Epoch:9/20, Batch:169/320]: loss1:0.0065 loss2:0.1051 loss3:0.0000 | AUC:0.7688 Anomaly AUC:0.9296
[2024-03-11 12:46:03,417][main.py][line:124][INFO] [Epoch:9/20, Batch:179/320]: loss1:0.0126 loss2:0.1595 loss3:0.0000 | AUC:0.8033 Anomaly AUC:0.9386
[2024-03-11 12:46:17,648][main.py][line:124][INFO] [Epoch:9/20, Batch:189/320]: loss1:0.0081 loss2:0.1382 loss3:0.0000 | AUC:0.8204 Anomaly AUC:0.9417
[2024-03-11 12:46:32,050][main.py][line:124][INFO] [Epoch:9/20, Batch:199/320]: loss1:0.0076 loss2:0.0801 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9416
[2024-03-11 12:46:46,364][main.py][line:124][INFO] [Epoch:9/20, Batch:209/320]: loss1:0.0945 loss2:0.1773 loss3:0.0000 | AUC:0.8034 Anomaly AUC:0.9394
[2024-03-11 12:47:00,625][main.py][line:124][INFO] [Epoch:9/20, Batch:219/320]: loss1:0.0205 loss2:0.1195 loss3:0.0000 | AUC:0.7864 Anomaly AUC:0.9349
[2024-03-11 12:47:15,048][main.py][line:124][INFO] [Epoch:9/20, Batch:229/320]: loss1:0.0076 loss2:0.1170 loss3:0.0000 | AUC:0.8110 Anomaly AUC:0.9384
[2024-03-11 12:47:29,571][main.py][line:124][INFO] [Epoch:9/20, Batch:239/320]: loss1:0.0131 loss2:0.1512 loss3:0.0000 | AUC:0.7585 Anomaly AUC:0.9307
[2024-03-11 12:47:43,936][main.py][line:124][INFO] [Epoch:9/20, Batch:249/320]: loss1:0.0127 loss2:0.1615 loss3:0.0000 | AUC:0.7468 Anomaly AUC:0.9209
[2024-03-11 12:47:58,100][main.py][line:124][INFO] [Epoch:9/20, Batch:259/320]: loss1:0.1313 loss2:0.2475 loss3:0.0000 | AUC:0.7847 Anomaly AUC:0.9354
[2024-03-11 12:48:12,210][main.py][line:124][INFO] [Epoch:9/20, Batch:269/320]: loss1:0.0141 loss2:0.1647 loss3:0.0000 | AUC:0.8025 Anomaly AUC:0.9370
[2024-03-11 12:48:27,061][main.py][line:124][INFO] [Epoch:9/20, Batch:279/320]: loss1:0.0308 loss2:0.1036 loss3:0.0000 | AUC:0.7748 Anomaly AUC:0.9308
[2024-03-11 12:48:41,875][main.py][line:124][INFO] [Epoch:9/20, Batch:289/320]: loss1:0.0126 loss2:0.1215 loss3:0.0000 | AUC:0.7645 Anomaly AUC:0.9251
[2024-03-11 12:48:55,801][main.py][line:153][INFO] [Epoch:9/20]: lr:0.00010 | loss1:0.0480 loss2:0.2234 loss3:0.0000 | AUC:0.7648 Anomaly AUC:0.9274
[2024-03-11 12:49:11,277][main.py][line:124][INFO] [Epoch:10/20, Batch:9/320]: loss1:0.0742 loss2:0.2249 loss3:0.0000 | AUC:0.7866 Anomaly AUC:0.9318
[2024-03-11 12:49:25,448][main.py][line:124][INFO] [Epoch:10/20, Batch:19/320]: loss1:0.0978 loss2:0.1318 loss3:0.0000 | AUC:0.8127 Anomaly AUC:0.9406
[2024-03-11 12:49:39,785][main.py][line:124][INFO] [Epoch:10/20, Batch:29/320]: loss1:0.0086 loss2:0.1361 loss3:0.0000 | AUC:0.8177 Anomaly AUC:0.9430
[2024-03-11 12:49:54,713][main.py][line:124][INFO] [Epoch:10/20, Batch:39/320]: loss1:0.0083 loss2:0.1113 loss3:0.0000 | AUC:0.8321 Anomaly AUC:0.9456
[2024-03-11 12:50:09,009][main.py][line:124][INFO] [Epoch:10/20, Batch:49/320]: loss1:0.0080 loss2:0.1600 loss3:0.0000 | AUC:0.7438 Anomaly AUC:0.9234
[2024-03-11 12:50:23,654][main.py][line:124][INFO] [Epoch:10/20, Batch:59/320]: loss1:0.1167 loss2:0.1627 loss3:0.0000 | AUC:0.7933 Anomaly AUC:0.9351
[2024-03-11 12:50:38,172][main.py][line:124][INFO] [Epoch:10/20, Batch:69/320]: loss1:0.0570 loss2:0.1288 loss3:0.0000 | AUC:0.7885 Anomaly AUC:0.9318
[2024-03-11 12:50:52,759][main.py][line:124][INFO] [Epoch:10/20, Batch:79/320]: loss1:0.0166 loss2:0.0828 loss3:0.0000 | AUC:0.7969 Anomaly AUC:0.9371
[2024-03-11 12:51:07,577][main.py][line:124][INFO] [Epoch:10/20, Batch:89/320]: loss1:0.0066 loss2:0.1522 loss3:0.0000 | AUC:0.8265 Anomaly AUC:0.9426
[2024-03-11 12:51:21,888][main.py][line:124][INFO] [Epoch:10/20, Batch:99/320]: loss1:0.0224 loss2:0.0746 loss3:0.0000 | AUC:0.8088 Anomaly AUC:0.9394
[2024-03-11 12:51:36,266][main.py][line:124][INFO] [Epoch:10/20, Batch:109/320]: loss1:0.0078 loss2:0.0762 loss3:0.0000 | AUC:0.8069 Anomaly AUC:0.9403
[2024-03-11 12:51:50,661][main.py][line:124][INFO] [Epoch:10/20, Batch:119/320]: loss1:0.0118 loss2:0.0798 loss3:0.0000 | AUC:0.7918 Anomaly AUC:0.9383
[2024-03-11 12:52:05,310][main.py][line:124][INFO] [Epoch:10/20, Batch:129/320]: loss1:0.0088 loss2:0.0892 loss3:0.0000 | AUC:0.7863 Anomaly AUC:0.9353
[2024-03-11 12:52:20,282][main.py][line:124][INFO] [Epoch:10/20, Batch:139/320]: loss1:0.0092 loss2:0.0767 loss3:0.0000 | AUC:0.7790 Anomaly AUC:0.9337
[2024-03-11 12:52:34,545][main.py][line:124][INFO] [Epoch:10/20, Batch:149/320]: loss1:0.0061 loss2:0.0794 loss3:0.0000 | AUC:0.8018 Anomaly AUC:0.9399
[2024-03-11 12:52:49,282][main.py][line:124][INFO] [Epoch:10/20, Batch:159/320]: loss1:0.0784 loss2:0.1428 loss3:0.0000 | AUC:0.7935 Anomaly AUC:0.9362
[2024-03-11 12:53:03,437][main.py][line:124][INFO] [Epoch:10/20, Batch:169/320]: loss1:0.0145 loss2:0.0861 loss3:0.0000 | AUC:0.7834 Anomaly AUC:0.9333
[2024-03-11 12:53:17,492][main.py][line:124][INFO] [Epoch:10/20, Batch:179/320]: loss1:0.0144 loss2:0.1431 loss3:0.0000 | AUC:0.7856 Anomaly AUC:0.9350
[2024-03-11 12:53:32,187][main.py][line:124][INFO] [Epoch:10/20, Batch:189/320]: loss1:0.0093 loss2:0.0779 loss3:0.0000 | AUC:0.8036 Anomaly AUC:0.9390
[2024-03-11 12:53:46,901][main.py][line:124][INFO] [Epoch:10/20, Batch:199/320]: loss1:0.0085 loss2:0.1063 loss3:0.0000 | AUC:0.8272 Anomaly AUC:0.9457
[2024-03-11 12:54:01,315][main.py][line:124][INFO] [Epoch:10/20, Batch:209/320]: loss1:0.0061 loss2:0.0991 loss3:0.0000 | AUC:0.8084 Anomaly AUC:0.9386
[2024-03-11 12:54:15,501][main.py][line:124][INFO] [Epoch:10/20, Batch:219/320]: loss1:0.0082 loss2:0.0767 loss3:0.0000 | AUC:0.7806 Anomaly AUC:0.9326
[2024-03-11 12:54:29,697][main.py][line:124][INFO] [Epoch:10/20, Batch:229/320]: loss1:0.0077 loss2:0.0667 loss3:0.0000 | AUC:0.7934 Anomaly AUC:0.9380
[2024-03-11 12:54:44,075][main.py][line:124][INFO] [Epoch:10/20, Batch:239/320]: loss1:0.0083 loss2:0.1584 loss3:0.0000 | AUC:0.7823 Anomaly AUC:0.9355
[2024-03-11 12:54:58,340][main.py][line:124][INFO] [Epoch:10/20, Batch:249/320]: loss1:0.0092 loss2:0.1017 loss3:0.0000 | AUC:0.8116 Anomaly AUC:0.9406
[2024-03-11 12:55:12,518][main.py][line:124][INFO] [Epoch:10/20, Batch:259/320]: loss1:0.0107 loss2:0.1019 loss3:0.0000 | AUC:0.8111 Anomaly AUC:0.9399
[2024-03-11 12:55:26,712][main.py][line:124][INFO] [Epoch:10/20, Batch:269/320]: loss1:0.0092 loss2:0.1131 loss3:0.0000 | AUC:0.8085 Anomaly AUC:0.9388
[2024-03-11 12:55:40,981][main.py][line:124][INFO] [Epoch:10/20, Batch:279/320]: loss1:0.1181 loss2:0.1988 loss3:0.0000 | AUC:0.8146 Anomaly AUC:0.9397
[2024-03-11 12:55:55,099][main.py][line:124][INFO] [Epoch:10/20, Batch:289/320]: loss1:0.0123 loss2:0.1109 loss3:0.0000 | AUC:0.8166 Anomaly AUC:0.9413
[2024-03-11 12:56:09,218][main.py][line:153][INFO] [Epoch:10/20]: lr:0.00010 | loss1:0.0222 loss2:0.1405 loss3:0.0000 | AUC:0.7930 Anomaly AUC:0.9347
[2024-03-11 12:56:24,884][main.py][line:124][INFO] [Epoch:11/20, Batch:9/320]: loss1:0.0264 loss2:0.1035 loss3:0.0000 | AUC:0.8110 Anomaly AUC:0.9414
[2024-03-11 12:56:39,193][main.py][line:124][INFO] [Epoch:11/20, Batch:19/320]: loss1:0.0162 loss2:0.0875 loss3:0.0000 | AUC:0.7814 Anomaly AUC:0.9376
[2024-03-11 12:56:54,060][main.py][line:124][INFO] [Epoch:11/20, Batch:29/320]: loss1:0.0086 loss2:0.1297 loss3:0.0000 | AUC:0.7637 Anomaly AUC:0.9288
[2024-03-11 12:57:08,503][main.py][line:124][INFO] [Epoch:11/20, Batch:39/320]: loss1:0.0104 loss2:0.1575 loss3:0.0000 | AUC:0.7711 Anomaly AUC:0.9309
[2024-03-11 12:57:22,706][main.py][line:124][INFO] [Epoch:11/20, Batch:49/320]: loss1:0.0082 loss2:0.0683 loss3:0.0000 | AUC:0.7799 Anomaly AUC:0.9329
[2024-03-11 12:57:37,028][main.py][line:124][INFO] [Epoch:11/20, Batch:59/320]: loss1:0.0237 loss2:0.1258 loss3:0.0000 | AUC:0.8065 Anomaly AUC:0.9406
[2024-03-11 12:57:51,270][main.py][line:124][INFO] [Epoch:11/20, Batch:69/320]: loss1:0.0092 loss2:0.0849 loss3:0.0000 | AUC:0.8050 Anomaly AUC:0.9351
[2024-03-11 12:58:05,748][main.py][line:124][INFO] [Epoch:11/20, Batch:79/320]: loss1:0.0302 loss2:0.1017 loss3:0.0000 | AUC:0.8082 Anomaly AUC:0.9382
[2024-03-11 12:58:19,974][main.py][line:124][INFO] [Epoch:11/20, Batch:89/320]: loss1:0.0071 loss2:0.0809 loss3:0.0000 | AUC:0.8255 Anomaly AUC:0.9418
[2024-03-11 12:58:34,122][main.py][line:124][INFO] [Epoch:11/20, Batch:99/320]: loss1:0.0067 loss2:0.1293 loss3:0.0000 | AUC:0.7800 Anomaly AUC:0.9348
[2024-03-11 12:58:48,489][main.py][line:124][INFO] [Epoch:11/20, Batch:109/320]: loss1:0.0079 loss2:0.0892 loss3:0.0000 | AUC:0.7832 Anomaly AUC:0.9346
[2024-03-11 12:59:02,922][main.py][line:124][INFO] [Epoch:11/20, Batch:119/320]: loss1:0.0097 loss2:0.0628 loss3:0.0000 | AUC:0.7917 Anomaly AUC:0.9362
[2024-03-11 12:59:17,825][main.py][line:124][INFO] [Epoch:11/20, Batch:129/320]: loss1:0.0135 loss2:0.1560 loss3:0.0000 | AUC:0.8042 Anomaly AUC:0.9397
[2024-03-11 12:59:32,289][main.py][line:124][INFO] [Epoch:11/20, Batch:139/320]: loss1:0.1440 loss2:0.2140 loss3:0.0000 | AUC:0.7508 Anomaly AUC:0.9243
[2024-03-11 12:59:46,530][main.py][line:124][INFO] [Epoch:11/20, Batch:149/320]: loss1:0.0087 loss2:0.0648 loss3:0.0000 | AUC:0.7960 Anomaly AUC:0.9364
[2024-03-11 13:00:01,377][main.py][line:124][INFO] [Epoch:11/20, Batch:159/320]: loss1:0.0135 loss2:0.0665 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9397
[2024-03-11 13:00:15,729][main.py][line:124][INFO] [Epoch:11/20, Batch:169/320]: loss1:0.0356 loss2:0.1072 loss3:0.0000 | AUC:0.7776 Anomaly AUC:0.9245
[2024-03-11 13:00:29,933][main.py][line:124][INFO] [Epoch:11/20, Batch:179/320]: loss1:0.0484 loss2:0.1564 loss3:0.0000 | AUC:0.8217 Anomaly AUC:0.9427
[2024-03-11 13:00:44,210][main.py][line:124][INFO] [Epoch:11/20, Batch:189/320]: loss1:0.0129 loss2:0.0752 loss3:0.0000 | AUC:0.7883 Anomaly AUC:0.9363
[2024-03-11 13:00:58,270][main.py][line:124][INFO] [Epoch:11/20, Batch:199/320]: loss1:0.0086 loss2:0.0578 loss3:0.0000 | AUC:0.8012 Anomaly AUC:0.9396
[2024-03-11 13:01:12,487][main.py][line:124][INFO] [Epoch:11/20, Batch:209/320]: loss1:0.0104 loss2:0.0813 loss3:0.0000 | AUC:0.8096 Anomaly AUC:0.9426
[2024-03-11 13:01:26,810][main.py][line:124][INFO] [Epoch:11/20, Batch:219/320]: loss1:0.0075 loss2:0.0811 loss3:0.0000 | AUC:0.8029 Anomaly AUC:0.9421
[2024-03-11 13:01:41,065][main.py][line:124][INFO] [Epoch:11/20, Batch:229/320]: loss1:0.0103 loss2:0.0907 loss3:0.0000 | AUC:0.8032 Anomaly AUC:0.9405
[2024-03-11 13:01:55,548][main.py][line:124][INFO] [Epoch:11/20, Batch:239/320]: loss1:0.0069 loss2:0.0853 loss3:0.0000 | AUC:0.7914 Anomaly AUC:0.9355
[2024-03-11 13:02:09,819][main.py][line:124][INFO] [Epoch:11/20, Batch:249/320]: loss1:0.0645 loss2:0.1151 loss3:0.0000 | AUC:0.7841 Anomaly AUC:0.9349
[2024-03-11 13:02:24,003][main.py][line:124][INFO] [Epoch:11/20, Batch:259/320]: loss1:0.0263 loss2:0.1128 loss3:0.0000 | AUC:0.8046 Anomaly AUC:0.9403
[2024-03-11 13:02:38,354][main.py][line:124][INFO] [Epoch:11/20, Batch:269/320]: loss1:0.0100 loss2:0.0706 loss3:0.0000 | AUC:0.8298 Anomaly AUC:0.9435
[2024-03-11 13:02:52,485][main.py][line:124][INFO] [Epoch:11/20, Batch:279/320]: loss1:0.0151 loss2:0.0788 loss3:0.0000 | AUC:0.8058 Anomaly AUC:0.9388
[2024-03-11 13:03:06,431][main.py][line:124][INFO] [Epoch:11/20, Batch:289/320]: loss1:0.0065 loss2:0.1043 loss3:0.0000 | AUC:0.8124 Anomaly AUC:0.9399
[2024-03-11 13:03:21,264][main.py][line:153][INFO] [Epoch:11/20]: lr:0.00010 | loss1:0.0140 loss2:0.1064 loss3:0.0000 | AUC:0.8179 Anomaly AUC:0.9426
[2024-03-11 13:03:36,428][main.py][line:124][INFO] [Epoch:12/20, Batch:9/320]: loss1:0.0224 loss2:0.0816 loss3:0.0000 | AUC:0.8023 Anomaly AUC:0.9400
[2024-03-11 13:03:51,510][main.py][line:124][INFO] [Epoch:12/20, Batch:19/320]: loss1:0.0060 loss2:0.0989 loss3:0.0000 | AUC:0.7935 Anomaly AUC:0.9371
[2024-03-11 13:04:05,945][main.py][line:124][INFO] [Epoch:12/20, Batch:29/320]: loss1:0.0067 loss2:0.0546 loss3:0.0000 | AUC:0.7936 Anomaly AUC:0.9354
[2024-03-11 13:04:20,290][main.py][line:124][INFO] [Epoch:12/20, Batch:39/320]: loss1:0.0121 loss2:0.0644 loss3:0.0000 | AUC:0.8153 Anomaly AUC:0.9424
[2024-03-11 13:04:34,523][main.py][line:124][INFO] [Epoch:12/20, Batch:49/320]: loss1:0.0077 loss2:0.0815 loss3:0.0000 | AUC:0.8179 Anomaly AUC:0.9418
[2024-03-11 13:04:50,108][main.py][line:124][INFO] [Epoch:12/20, Batch:59/320]: loss1:0.0059 loss2:0.0502 loss3:0.0000 | AUC:0.8113 Anomaly AUC:0.9401
[2024-03-11 13:05:05,022][main.py][line:124][INFO] [Epoch:12/20, Batch:69/320]: loss1:0.0074 loss2:0.0915 loss3:0.0000 | AUC:0.7980 Anomaly AUC:0.9363
[2024-03-11 13:05:19,423][main.py][line:124][INFO] [Epoch:12/20, Batch:79/320]: loss1:0.0072 loss2:0.0695 loss3:0.0000 | AUC:0.8027 Anomaly AUC:0.9372
[2024-03-11 13:05:33,835][main.py][line:124][INFO] [Epoch:12/20, Batch:89/320]: loss1:0.0179 loss2:0.0462 loss3:0.0000 | AUC:0.8009 Anomaly AUC:0.9380
[2024-03-11 13:05:48,034][main.py][line:124][INFO] [Epoch:12/20, Batch:99/320]: loss1:0.0081 loss2:0.0728 loss3:0.0000 | AUC:0.8107 Anomaly AUC:0.9408
[2024-03-11 13:06:02,206][main.py][line:124][INFO] [Epoch:12/20, Batch:109/320]: loss1:0.0091 loss2:0.0486 loss3:0.0000 | AUC:0.7961 Anomaly AUC:0.9377
[2024-03-11 13:06:16,402][main.py][line:124][INFO] [Epoch:12/20, Batch:119/320]: loss1:0.0077 loss2:0.0775 loss3:0.0000 | AUC:0.8089 Anomaly AUC:0.9400
[2024-03-11 13:06:30,565][main.py][line:124][INFO] [Epoch:12/20, Batch:129/320]: loss1:0.0113 loss2:0.0943 loss3:0.0000 | AUC:0.7847 Anomaly AUC:0.9350
[2024-03-11 13:06:44,736][main.py][line:124][INFO] [Epoch:12/20, Batch:139/320]: loss1:0.0071 loss2:0.0807 loss3:0.0000 | AUC:0.7736 Anomaly AUC:0.9344
[2024-03-11 13:06:59,101][main.py][line:124][INFO] [Epoch:12/20, Batch:149/320]: loss1:0.0095 loss2:0.0674 loss3:0.0000 | AUC:0.7828 Anomaly AUC:0.9333
[2024-03-11 13:07:13,453][main.py][line:124][INFO] [Epoch:12/20, Batch:159/320]: loss1:0.0086 loss2:0.0806 loss3:0.0000 | AUC:0.8133 Anomaly AUC:0.9394
[2024-03-11 13:07:27,766][main.py][line:124][INFO] [Epoch:12/20, Batch:169/320]: loss1:0.0160 loss2:0.1224 loss3:0.0000 | AUC:0.7740 Anomaly AUC:0.9342
[2024-03-11 13:07:41,810][main.py][line:124][INFO] [Epoch:12/20, Batch:179/320]: loss1:0.0085 loss2:0.1039 loss3:0.0000 | AUC:0.8254 Anomaly AUC:0.9447
[2024-03-11 13:07:56,125][main.py][line:124][INFO] [Epoch:12/20, Batch:189/320]: loss1:0.0317 loss2:0.0829 loss3:0.0000 | AUC:0.8160 Anomaly AUC:0.9445
[2024-03-11 13:08:10,471][main.py][line:124][INFO] [Epoch:12/20, Batch:199/320]: loss1:0.0228 loss2:0.0976 loss3:0.0000 | AUC:0.7819 Anomaly AUC:0.9374
[2024-03-11 13:08:25,110][main.py][line:124][INFO] [Epoch:12/20, Batch:209/320]: loss1:0.0144 loss2:0.0784 loss3:0.0000 | AUC:0.7995 Anomaly AUC:0.9429
[2024-03-11 13:08:39,419][main.py][line:124][INFO] [Epoch:12/20, Batch:219/320]: loss1:0.0125 loss2:0.0498 loss3:0.0000 | AUC:0.8154 Anomaly AUC:0.9441
[2024-03-11 13:08:54,353][main.py][line:124][INFO] [Epoch:12/20, Batch:229/320]: loss1:0.0202 loss2:0.1052 loss3:0.0000 | AUC:0.7932 Anomaly AUC:0.9365
[2024-03-11 13:09:08,735][main.py][line:124][INFO] [Epoch:12/20, Batch:239/320]: loss1:0.0091 loss2:0.0431 loss3:0.0000 | AUC:0.7770 Anomaly AUC:0.9345
[2024-03-11 13:09:23,524][main.py][line:124][INFO] [Epoch:12/20, Batch:249/320]: loss1:0.0111 loss2:0.0901 loss3:0.0000 | AUC:0.7338 Anomaly AUC:0.9224
[2024-03-11 13:09:38,207][main.py][line:124][INFO] [Epoch:12/20, Batch:259/320]: loss1:0.0154 loss2:0.0704 loss3:0.0000 | AUC:0.6330 Anomaly AUC:0.9073
[2024-03-11 13:09:52,389][main.py][line:124][INFO] [Epoch:12/20, Batch:269/320]: loss1:0.0508 loss2:0.0781 loss3:0.0000 | AUC:0.8066 Anomaly AUC:0.9416
[2024-03-11 13:10:06,554][main.py][line:124][INFO] [Epoch:12/20, Batch:279/320]: loss1:0.0084 loss2:0.0924 loss3:0.0000 | AUC:0.8368 Anomaly AUC:0.9484
[2024-03-11 13:10:20,710][main.py][line:124][INFO] [Epoch:12/20, Batch:289/320]: loss1:0.0139 loss2:0.0938 loss3:0.0000 | AUC:0.7552 Anomaly AUC:0.9310
[2024-03-11 13:10:35,606][main.py][line:153][INFO] [Epoch:12/20]: lr:0.00010 | loss1:0.0140 loss2:0.1162 loss3:0.0000 | AUC:0.7809 Anomaly AUC:0.9359
[2024-03-11 13:10:50,813][main.py][line:124][INFO] [Epoch:13/20, Batch:9/320]: loss1:0.1123 loss2:0.2409 loss3:0.0000 | AUC:0.7489 Anomaly AUC:0.9326
[2024-03-11 13:11:04,927][main.py][line:124][INFO] [Epoch:13/20, Batch:19/320]: loss1:0.0136 loss2:0.1856 loss3:0.0000 | AUC:0.7355 Anomaly AUC:0.9249
[2024-03-11 13:11:19,199][main.py][line:124][INFO] [Epoch:13/20, Batch:29/320]: loss1:0.0115 loss2:0.0815 loss3:0.0000 | AUC:0.7895 Anomaly AUC:0.9419
[2024-03-11 13:11:33,422][main.py][line:124][INFO] [Epoch:13/20, Batch:39/320]: loss1:0.0115 loss2:0.0784 loss3:0.0000 | AUC:0.7490 Anomaly AUC:0.9296
[2024-03-11 13:11:47,578][main.py][line:124][INFO] [Epoch:13/20, Batch:49/320]: loss1:0.0365 loss2:0.0755 loss3:0.0000 | AUC:0.7823 Anomaly AUC:0.9380
[2024-03-11 13:12:01,895][main.py][line:124][INFO] [Epoch:13/20, Batch:59/320]: loss1:0.0194 loss2:0.0837 loss3:0.0000 | AUC:0.7239 Anomaly AUC:0.9220
[2024-03-11 13:12:16,215][main.py][line:124][INFO] [Epoch:13/20, Batch:69/320]: loss1:0.0204 loss2:0.0976 loss3:0.0000 | AUC:0.7084 Anomaly AUC:0.9196
[2024-03-11 13:12:30,318][main.py][line:124][INFO] [Epoch:13/20, Batch:79/320]: loss1:0.0660 loss2:0.1220 loss3:0.0000 | AUC:0.8004 Anomaly AUC:0.9371
[2024-03-11 13:12:44,535][main.py][line:124][INFO] [Epoch:13/20, Batch:89/320]: loss1:0.0301 loss2:0.0675 loss3:0.0000 | AUC:0.8033 Anomaly AUC:0.9406
[2024-03-11 13:12:58,953][main.py][line:124][INFO] [Epoch:13/20, Batch:99/320]: loss1:0.1428 loss2:0.1650 loss3:0.0000 | AUC:0.8287 Anomaly AUC:0.9457
[2024-03-11 13:13:13,265][main.py][line:124][INFO] [Epoch:13/20, Batch:109/320]: loss1:0.0185 loss2:0.0559 loss3:0.0000 | AUC:0.8256 Anomaly AUC:0.9453
[2024-03-11 13:13:27,473][main.py][line:124][INFO] [Epoch:13/20, Batch:119/320]: loss1:0.0543 loss2:0.0680 loss3:0.0000 | AUC:0.8089 Anomaly AUC:0.9403
[2024-03-11 13:13:41,797][main.py][line:124][INFO] [Epoch:13/20, Batch:129/320]: loss1:0.0174 loss2:0.0899 loss3:0.0000 | AUC:0.8004 Anomaly AUC:0.9387
[2024-03-11 13:13:56,128][main.py][line:124][INFO] [Epoch:13/20, Batch:139/320]: loss1:0.0200 loss2:0.0770 loss3:0.0000 | AUC:0.7974 Anomaly AUC:0.9363
[2024-03-11 13:14:10,531][main.py][line:124][INFO] [Epoch:13/20, Batch:149/320]: loss1:0.0046 loss2:0.0581 loss3:0.0000 | AUC:0.7893 Anomaly AUC:0.9331
[2024-03-11 13:14:24,902][main.py][line:124][INFO] [Epoch:13/20, Batch:159/320]: loss1:0.0859 loss2:0.1348 loss3:0.0000 | AUC:0.7856 Anomaly AUC:0.9368
[2024-03-11 13:14:39,246][main.py][line:124][INFO] [Epoch:13/20, Batch:169/320]: loss1:0.0110 loss2:0.0693 loss3:0.0000 | AUC:0.7629 Anomaly AUC:0.9330
[2024-03-11 13:14:53,620][main.py][line:124][INFO] [Epoch:13/20, Batch:179/320]: loss1:0.0088 loss2:0.0766 loss3:0.0000 | AUC:0.7825 Anomaly AUC:0.9365
[2024-03-11 13:15:08,577][main.py][line:124][INFO] [Epoch:13/20, Batch:189/320]: loss1:0.0085 loss2:0.1023 loss3:0.0000 | AUC:0.8187 Anomaly AUC:0.9438
[2024-03-11 13:15:22,992][main.py][line:124][INFO] [Epoch:13/20, Batch:199/320]: loss1:0.0112 loss2:0.0443 loss3:0.0000 | AUC:0.7080 Anomaly AUC:0.9197
[2024-03-11 13:15:37,497][main.py][line:124][INFO] [Epoch:13/20, Batch:209/320]: loss1:0.0589 loss2:0.1319 loss3:0.0000 | AUC:0.8031 Anomaly AUC:0.9434
[2024-03-11 13:15:51,917][main.py][line:124][INFO] [Epoch:13/20, Batch:219/320]: loss1:0.1336 loss2:0.2379 loss3:0.0000 | AUC:0.7892 Anomaly AUC:0.9386
[2024-03-11 13:16:06,926][main.py][line:124][INFO] [Epoch:13/20, Batch:229/320]: loss1:0.0112 loss2:0.0681 loss3:0.0000 | AUC:0.7948 Anomaly AUC:0.9375
[2024-03-11 13:16:22,106][main.py][line:124][INFO] [Epoch:13/20, Batch:239/320]: loss1:0.0068 loss2:0.0476 loss3:0.0000 | AUC:0.8024 Anomaly AUC:0.9388
[2024-03-11 13:16:36,550][main.py][line:124][INFO] [Epoch:13/20, Batch:249/320]: loss1:0.0047 loss2:0.0648 loss3:0.0000 | AUC:0.8101 Anomaly AUC:0.9409
[2024-03-11 13:16:51,010][main.py][line:124][INFO] [Epoch:13/20, Batch:259/320]: loss1:0.0115 loss2:0.1081 loss3:0.0000 | AUC:0.7798 Anomaly AUC:0.9354
[2024-03-11 13:17:05,201][main.py][line:124][INFO] [Epoch:13/20, Batch:269/320]: loss1:0.0094 loss2:0.0565 loss3:0.0000 | AUC:0.7742 Anomaly AUC:0.9354
[2024-03-11 13:17:19,375][main.py][line:124][INFO] [Epoch:13/20, Batch:279/320]: loss1:0.0082 loss2:0.1097 loss3:0.0000 | AUC:0.7642 Anomaly AUC:0.9310
[2024-03-11 13:17:33,402][main.py][line:124][INFO] [Epoch:13/20, Batch:289/320]: loss1:0.0598 loss2:0.1475 loss3:0.0000 | AUC:0.7881 Anomaly AUC:0.9372
[2024-03-11 13:17:48,130][main.py][line:153][INFO] [Epoch:13/20]: lr:0.00010 | loss1:0.0844 loss2:0.1794 loss3:0.0000 | AUC:0.7900 Anomaly AUC:0.9369
[2024-03-11 13:18:03,801][main.py][line:124][INFO] [Epoch:14/20, Batch:9/320]: loss1:0.0105 loss2:0.0701 loss3:0.0000 | AUC:0.7615 Anomaly AUC:0.9297
[2024-03-11 13:18:19,203][main.py][line:124][INFO] [Epoch:14/20, Batch:19/320]: loss1:0.0113 loss2:0.0521 loss3:0.0000 | AUC:0.7890 Anomaly AUC:0.9353
[2024-03-11 13:18:33,896][main.py][line:124][INFO] [Epoch:14/20, Batch:29/320]: loss1:0.0093 loss2:0.0825 loss3:0.0000 | AUC:0.7846 Anomaly AUC:0.9353
[2024-03-11 13:18:48,991][main.py][line:124][INFO] [Epoch:14/20, Batch:39/320]: loss1:0.0062 loss2:0.0468 loss3:0.0000 | AUC:0.7931 Anomaly AUC:0.9362
[2024-03-11 13:19:03,956][main.py][line:124][INFO] [Epoch:14/20, Batch:49/320]: loss1:0.0142 loss2:0.0869 loss3:0.0000 | AUC:0.7614 Anomaly AUC:0.9271
[2024-03-11 13:19:19,248][main.py][line:124][INFO] [Epoch:14/20, Batch:59/320]: loss1:0.0074 loss2:0.0619 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9312
[2024-03-11 13:19:34,549][main.py][line:124][INFO] [Epoch:14/20, Batch:69/320]: loss1:0.0122 loss2:0.0458 loss3:0.0000 | AUC:0.7789 Anomaly AUC:0.9355
[2024-03-11 13:19:49,546][main.py][line:124][INFO] [Epoch:14/20, Batch:79/320]: loss1:0.0086 loss2:0.0487 loss3:0.0000 | AUC:0.7813 Anomaly AUC:0.9348
[2024-03-11 13:20:04,679][main.py][line:124][INFO] [Epoch:14/20, Batch:89/320]: loss1:0.0053 loss2:0.0552 loss3:0.0000 | AUC:0.7837 Anomaly AUC:0.9332
[2024-03-11 13:20:19,896][main.py][line:124][INFO] [Epoch:14/20, Batch:99/320]: loss1:0.0181 loss2:0.0702 loss3:0.0000 | AUC:0.7727 Anomaly AUC:0.9302
[2024-03-11 13:20:35,091][main.py][line:124][INFO] [Epoch:14/20, Batch:109/320]: loss1:0.0128 loss2:0.0601 loss3:0.0000 | AUC:0.7357 Anomaly AUC:0.9211
[2024-03-11 13:20:49,356][main.py][line:124][INFO] [Epoch:14/20, Batch:119/320]: loss1:0.1069 loss2:0.1422 loss3:0.0000 | AUC:0.7957 Anomaly AUC:0.9381
[2024-03-11 13:21:03,760][main.py][line:124][INFO] [Epoch:14/20, Batch:129/320]: loss1:0.0134 loss2:0.0894 loss3:0.0000 | AUC:0.8159 Anomaly AUC:0.9389
[2024-03-11 13:21:18,613][main.py][line:124][INFO] [Epoch:14/20, Batch:139/320]: loss1:0.0090 loss2:0.0504 loss3:0.0000 | AUC:0.7906 Anomaly AUC:0.9346
[2024-03-11 13:21:33,359][main.py][line:124][INFO] [Epoch:14/20, Batch:149/320]: loss1:0.0245 loss2:0.1147 loss3:0.0000 | AUC:0.7414 Anomaly AUC:0.9212
[2024-03-11 13:21:47,563][main.py][line:124][INFO] [Epoch:14/20, Batch:159/320]: loss1:0.0100 loss2:0.0554 loss3:0.0000 | AUC:0.8066 Anomaly AUC:0.9393
[2024-03-11 13:22:01,839][main.py][line:124][INFO] [Epoch:14/20, Batch:169/320]: loss1:0.0172 loss2:0.0687 loss3:0.0000 | AUC:0.8165 Anomaly AUC:0.9410
[2024-03-11 13:22:16,177][main.py][line:124][INFO] [Epoch:14/20, Batch:179/320]: loss1:0.0067 loss2:0.0693 loss3:0.0000 | AUC:0.7264 Anomaly AUC:0.9149
[2024-03-11 13:22:30,719][main.py][line:124][INFO] [Epoch:14/20, Batch:189/320]: loss1:0.0288 loss2:0.1304 loss3:0.0000 | AUC:0.8217 Anomaly AUC:0.9433
[2024-03-11 13:22:45,182][main.py][line:124][INFO] [Epoch:14/20, Batch:199/320]: loss1:0.0127 loss2:0.0720 loss3:0.0000 | AUC:0.7665 Anomaly AUC:0.9313
[2024-03-11 13:23:00,174][main.py][line:124][INFO] [Epoch:14/20, Batch:209/320]: loss1:0.0075 loss2:0.0380 loss3:0.0000 | AUC:0.7706 Anomaly AUC:0.9314
[2024-03-11 13:23:15,201][main.py][line:124][INFO] [Epoch:14/20, Batch:219/320]: loss1:0.0073 loss2:0.0476 loss3:0.0000 | AUC:0.7896 Anomaly AUC:0.9335
[2024-03-11 13:23:30,320][main.py][line:124][INFO] [Epoch:14/20, Batch:229/320]: loss1:0.0417 loss2:0.0854 loss3:0.0000 | AUC:0.7719 Anomaly AUC:0.9325
[2024-03-11 13:23:45,608][main.py][line:124][INFO] [Epoch:14/20, Batch:239/320]: loss1:0.0286 loss2:0.0994 loss3:0.0000 | AUC:0.7698 Anomaly AUC:0.9357
[2024-03-11 13:24:00,056][main.py][line:124][INFO] [Epoch:14/20, Batch:249/320]: loss1:0.0071 loss2:0.0399 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9342
[2024-03-11 13:24:14,307][main.py][line:124][INFO] [Epoch:14/20, Batch:259/320]: loss1:0.0148 loss2:0.0423 loss3:0.0000 | AUC:0.7771 Anomaly AUC:0.9359
[2024-03-11 13:24:29,240][main.py][line:124][INFO] [Epoch:14/20, Batch:269/320]: loss1:0.0177 loss2:0.0537 loss3:0.0000 | AUC:0.7428 Anomaly AUC:0.9257
[2024-03-11 13:24:43,688][main.py][line:124][INFO] [Epoch:14/20, Batch:279/320]: loss1:0.0092 loss2:0.0384 loss3:0.0000 | AUC:0.7626 Anomaly AUC:0.9360
[2024-03-11 13:24:57,923][main.py][line:124][INFO] [Epoch:14/20, Batch:289/320]: loss1:0.0136 loss2:0.0537 loss3:0.0000 | AUC:0.7851 Anomaly AUC:0.9392
[2024-03-11 13:25:13,515][main.py][line:153][INFO] [Epoch:14/20]: lr:0.00010 | loss1:0.0143 loss2:0.0500 loss3:0.0000 | AUC:0.8125 Anomaly AUC:0.9421
[2024-03-11 13:25:28,251][main.py][line:124][INFO] [Epoch:15/20, Batch:9/320]: loss1:0.0071 loss2:0.0494 loss3:0.0000 | AUC:0.8198 Anomaly AUC:0.9417
[2024-03-11 13:25:42,965][main.py][line:124][INFO] [Epoch:15/20, Batch:19/320]: loss1:0.0053 loss2:0.0422 loss3:0.0000 | AUC:0.8044 Anomaly AUC:0.9396
[2024-03-11 13:25:57,134][main.py][line:124][INFO] [Epoch:15/20, Batch:29/320]: loss1:0.0051 loss2:0.0589 loss3:0.0000 | AUC:0.7967 Anomaly AUC:0.9382
[2024-03-11 13:26:11,420][main.py][line:124][INFO] [Epoch:15/20, Batch:39/320]: loss1:0.0077 loss2:0.0674 loss3:0.0000 | AUC:0.7682 Anomaly AUC:0.9320
[2024-03-11 13:26:25,652][main.py][line:124][INFO] [Epoch:15/20, Batch:49/320]: loss1:0.0063 loss2:0.0731 loss3:0.0000 | AUC:0.7540 Anomaly AUC:0.9297
[2024-03-11 13:26:40,073][main.py][line:124][INFO] [Epoch:15/20, Batch:59/320]: loss1:0.0060 loss2:0.0488 loss3:0.0000 | AUC:0.7680 Anomaly AUC:0.9325
[2024-03-11 13:26:54,422][main.py][line:124][INFO] [Epoch:15/20, Batch:69/320]: loss1:0.0091 loss2:0.0445 loss3:0.0000 | AUC:0.7553 Anomaly AUC:0.9291
[2024-03-11 13:27:08,596][main.py][line:124][INFO] [Epoch:15/20, Batch:79/320]: loss1:0.0072 loss2:0.0557 loss3:0.0000 | AUC:0.7502 Anomaly AUC:0.9296
[2024-03-11 13:27:22,796][main.py][line:124][INFO] [Epoch:15/20, Batch:89/320]: loss1:0.0766 loss2:0.1030 loss3:0.0000 | AUC:0.7661 Anomaly AUC:0.9333
[2024-03-11 13:27:37,156][main.py][line:124][INFO] [Epoch:15/20, Batch:99/320]: loss1:0.0083 loss2:0.0367 loss3:0.0000 | AUC:0.7641 Anomaly AUC:0.9314
[2024-03-11 13:27:51,311][main.py][line:124][INFO] [Epoch:15/20, Batch:109/320]: loss1:0.0074 loss2:0.0413 loss3:0.0000 | AUC:0.7649 Anomaly AUC:0.9309
[2024-03-11 13:28:05,582][main.py][line:124][INFO] [Epoch:15/20, Batch:119/320]: loss1:0.0072 loss2:0.0642 loss3:0.0000 | AUC:0.7720 Anomaly AUC:0.9316
[2024-03-11 13:28:20,031][main.py][line:124][INFO] [Epoch:15/20, Batch:129/320]: loss1:0.0086 loss2:0.0352 loss3:0.0000 | AUC:0.7660 Anomaly AUC:0.9298
[2024-03-11 13:28:34,333][main.py][line:124][INFO] [Epoch:15/20, Batch:139/320]: loss1:0.0051 loss2:0.0334 loss3:0.0000 | AUC:0.7828 Anomaly AUC:0.9346
[2024-03-11 13:28:48,603][main.py][line:124][INFO] [Epoch:15/20, Batch:149/320]: loss1:0.0068 loss2:0.0338 loss3:0.0000 | AUC:0.7827 Anomaly AUC:0.9350
[2024-03-11 13:29:02,827][main.py][line:124][INFO] [Epoch:15/20, Batch:159/320]: loss1:0.0067 loss2:0.0433 loss3:0.0000 | AUC:0.7827 Anomaly AUC:0.9351
[2024-03-11 13:29:17,057][main.py][line:124][INFO] [Epoch:15/20, Batch:169/320]: loss1:0.0198 loss2:0.0535 loss3:0.0000 | AUC:0.7917 Anomaly AUC:0.9362
[2024-03-11 13:29:31,361][main.py][line:124][INFO] [Epoch:15/20, Batch:179/320]: loss1:0.1098 loss2:0.1262 loss3:0.0000 | AUC:0.7471 Anomaly AUC:0.9282
[2024-03-11 13:29:45,450][main.py][line:124][INFO] [Epoch:15/20, Batch:189/320]: loss1:0.1213 loss2:0.1126 loss3:0.0000 | AUC:0.8014 Anomaly AUC:0.9385
[2024-03-11 13:29:59,713][main.py][line:124][INFO] [Epoch:15/20, Batch:199/320]: loss1:0.0345 loss2:0.0477 loss3:0.0000 | AUC:0.7811 Anomaly AUC:0.9337
[2024-03-11 13:30:14,096][main.py][line:124][INFO] [Epoch:15/20, Batch:209/320]: loss1:0.0217 loss2:0.0397 loss3:0.0000 | AUC:0.7645 Anomaly AUC:0.9288
[2024-03-11 13:30:28,557][main.py][line:124][INFO] [Epoch:15/20, Batch:219/320]: loss1:0.0060 loss2:0.0690 loss3:0.0000 | AUC:0.7705 Anomaly AUC:0.9291
[2024-03-11 13:30:42,789][main.py][line:124][INFO] [Epoch:15/20, Batch:229/320]: loss1:0.0114 loss2:0.0563 loss3:0.0000 | AUC:0.7381 Anomaly AUC:0.9190
[2024-03-11 13:30:56,986][main.py][line:124][INFO] [Epoch:15/20, Batch:239/320]: loss1:0.0073 loss2:0.0533 loss3:0.0000 | AUC:0.7782 Anomaly AUC:0.9340
[2024-03-11 13:31:11,350][main.py][line:124][INFO] [Epoch:15/20, Batch:249/320]: loss1:0.0264 loss2:0.0441 loss3:0.0000 | AUC:0.7592 Anomaly AUC:0.9310
[2024-03-11 13:31:25,593][main.py][line:124][INFO] [Epoch:15/20, Batch:259/320]: loss1:0.0165 loss2:0.0476 loss3:0.0000 | AUC:0.7811 Anomaly AUC:0.9347
[2024-03-11 13:31:40,447][main.py][line:124][INFO] [Epoch:15/20, Batch:269/320]: loss1:0.0114 loss2:0.0342 loss3:0.0000 | AUC:0.7844 Anomaly AUC:0.9348
[2024-03-11 13:31:54,965][main.py][line:124][INFO] [Epoch:15/20, Batch:279/320]: loss1:0.0053 loss2:0.0486 loss3:0.0000 | AUC:0.7813 Anomaly AUC:0.9319
[2024-03-11 13:32:09,154][main.py][line:124][INFO] [Epoch:15/20, Batch:289/320]: loss1:0.0082 loss2:0.0676 loss3:0.0000 | AUC:0.7851 Anomaly AUC:0.9322
[2024-03-11 13:32:23,060][main.py][line:153][INFO] [Epoch:15/20]: lr:0.00010 | loss1:0.1064 loss2:0.1218 loss3:0.0000 | AUC:0.7645 Anomaly AUC:0.9252
[2024-03-11 13:32:37,569][main.py][line:124][INFO] [Epoch:16/20, Batch:9/320]: loss1:0.1847 loss2:0.2049 loss3:0.0000 | AUC:0.7163 Anomaly AUC:0.9228
[2024-03-11 13:32:51,675][main.py][line:124][INFO] [Epoch:16/20, Batch:19/320]: loss1:0.0078 loss2:0.0747 loss3:0.0000 | AUC:0.6923 Anomaly AUC:0.9164
[2024-03-11 13:33:06,001][main.py][line:124][INFO] [Epoch:16/20, Batch:29/320]: loss1:0.1634 loss2:0.1717 loss3:0.0000 | AUC:0.7592 Anomaly AUC:0.9295
[2024-03-11 13:33:20,275][main.py][line:124][INFO] [Epoch:16/20, Batch:39/320]: loss1:0.0190 loss2:0.0628 loss3:0.0000 | AUC:0.7847 Anomaly AUC:0.9344
[2024-03-11 13:33:34,578][main.py][line:124][INFO] [Epoch:16/20, Batch:49/320]: loss1:0.0608 loss2:0.0808 loss3:0.0000 | AUC:0.7977 Anomaly AUC:0.9408
[2024-03-11 13:33:48,771][main.py][line:124][INFO] [Epoch:16/20, Batch:59/320]: loss1:0.0109 loss2:0.0581 loss3:0.0000 | AUC:0.7668 Anomaly AUC:0.9278
[2024-03-11 13:34:03,008][main.py][line:124][INFO] [Epoch:16/20, Batch:69/320]: loss1:0.0067 loss2:0.0331 loss3:0.0000 | AUC:0.7973 Anomaly AUC:0.9347
[2024-03-11 13:34:17,231][main.py][line:124][INFO] [Epoch:16/20, Batch:79/320]: loss1:0.0100 loss2:0.1096 loss3:0.0000 | AUC:0.8117 Anomaly AUC:0.9379
[2024-03-11 13:34:31,245][main.py][line:124][INFO] [Epoch:16/20, Batch:89/320]: loss1:0.0189 loss2:0.0504 loss3:0.0000 | AUC:0.7751 Anomaly AUC:0.9321
[2024-03-11 13:34:45,481][main.py][line:124][INFO] [Epoch:16/20, Batch:99/320]: loss1:0.0173 loss2:0.1047 loss3:0.0000 | AUC:0.7397 Anomaly AUC:0.9284
[2024-03-11 13:34:59,828][main.py][line:124][INFO] [Epoch:16/20, Batch:109/320]: loss1:0.0091 loss2:0.1465 loss3:0.0000 | AUC:0.8097 Anomaly AUC:0.9362
[2024-03-11 13:35:13,995][main.py][line:124][INFO] [Epoch:16/20, Batch:119/320]: loss1:0.0108 loss2:0.0978 loss3:0.0000 | AUC:0.7925 Anomaly AUC:0.9354
[2024-03-11 13:35:28,248][main.py][line:124][INFO] [Epoch:16/20, Batch:129/320]: loss1:0.0462 loss2:0.1688 loss3:0.0000 | AUC:0.7610 Anomaly AUC:0.9304
[2024-03-11 13:35:42,443][main.py][line:124][INFO] [Epoch:16/20, Batch:139/320]: loss1:0.0093 loss2:0.0787 loss3:0.0000 | AUC:0.7021 Anomaly AUC:0.9118
[2024-03-11 13:35:56,659][main.py][line:124][INFO] [Epoch:16/20, Batch:149/320]: loss1:0.0158 loss2:0.1010 loss3:0.0000 | AUC:0.7758 Anomaly AUC:0.9376
[2024-03-11 13:36:11,002][main.py][line:124][INFO] [Epoch:16/20, Batch:159/320]: loss1:0.0500 loss2:0.0996 loss3:0.0000 | AUC:0.7344 Anomaly AUC:0.9287
[2024-03-11 13:36:25,220][main.py][line:124][INFO] [Epoch:16/20, Batch:169/320]: loss1:0.0104 loss2:0.0530 loss3:0.0000 | AUC:0.7143 Anomaly AUC:0.9192
[2024-03-11 13:36:39,515][main.py][line:124][INFO] [Epoch:16/20, Batch:179/320]: loss1:0.0177 loss2:0.0828 loss3:0.0000 | AUC:0.6976 Anomaly AUC:0.9171
[2024-03-11 13:36:53,897][main.py][line:124][INFO] [Epoch:16/20, Batch:189/320]: loss1:0.0455 loss2:0.1903 loss3:0.0000 | AUC:0.6737 Anomaly AUC:0.9086
[2024-03-11 13:37:08,150][main.py][line:124][INFO] [Epoch:16/20, Batch:199/320]: loss1:0.0305 loss2:0.0805 loss3:0.0000 | AUC:0.6969 Anomaly AUC:0.9139
[2024-03-11 13:37:22,144][main.py][line:124][INFO] [Epoch:16/20, Batch:209/320]: loss1:0.0154 loss2:0.0844 loss3:0.0000 | AUC:0.7805 Anomaly AUC:0.9338
[2024-03-11 13:37:36,405][main.py][line:124][INFO] [Epoch:16/20, Batch:219/320]: loss1:0.0097 loss2:0.0532 loss3:0.0000 | AUC:0.7903 Anomaly AUC:0.9349
[2024-03-11 13:37:50,541][main.py][line:124][INFO] [Epoch:16/20, Batch:229/320]: loss1:0.0108 loss2:0.0471 loss3:0.0000 | AUC:0.7920 Anomaly AUC:0.9335
[2024-03-11 13:38:04,932][main.py][line:124][INFO] [Epoch:16/20, Batch:239/320]: loss1:0.0147 loss2:0.0532 loss3:0.0000 | AUC:0.7623 Anomaly AUC:0.9290
[2024-03-11 13:38:19,321][main.py][line:124][INFO] [Epoch:16/20, Batch:249/320]: loss1:0.0499 loss2:0.0847 loss3:0.0000 | AUC:0.6665 Anomaly AUC:0.9041
[2024-03-11 13:38:33,557][main.py][line:124][INFO] [Epoch:16/20, Batch:259/320]: loss1:0.1465 loss2:0.2331 loss3:0.0000 | AUC:0.7561 Anomaly AUC:0.9305
[2024-03-11 13:38:47,889][main.py][line:124][INFO] [Epoch:16/20, Batch:269/320]: loss1:0.0139 loss2:0.0707 loss3:0.0000 | AUC:0.7331 Anomaly AUC:0.9283
[2024-03-11 13:39:02,335][main.py][line:124][INFO] [Epoch:16/20, Batch:279/320]: loss1:0.1181 loss2:0.1621 loss3:0.0000 | AUC:0.7285 Anomaly AUC:0.9274
[2024-03-11 13:39:16,433][main.py][line:124][INFO] [Epoch:16/20, Batch:289/320]: loss1:0.0626 loss2:0.1364 loss3:0.0000 | AUC:0.7497 Anomaly AUC:0.9305
[2024-03-11 13:39:31,347][main.py][line:153][INFO] [Epoch:16/20]: lr:0.00010 | loss1:0.0124 loss2:0.0816 loss3:0.0000 | AUC:0.7603 Anomaly AUC:0.9322
[2024-03-11 13:39:46,037][main.py][line:124][INFO] [Epoch:17/20, Batch:9/320]: loss1:0.0261 loss2:0.0789 loss3:0.0000 | AUC:0.8268 Anomaly AUC:0.9458
[2024-03-11 13:40:00,412][main.py][line:124][INFO] [Epoch:17/20, Batch:19/320]: loss1:0.0110 loss2:0.0360 loss3:0.0000 | AUC:0.8199 Anomaly AUC:0.9448
[2024-03-11 13:40:15,192][main.py][line:124][INFO] [Epoch:17/20, Batch:29/320]: loss1:0.0084 loss2:0.0414 loss3:0.0000 | AUC:0.7844 Anomaly AUC:0.9371
[2024-03-11 13:40:30,214][main.py][line:124][INFO] [Epoch:17/20, Batch:39/320]: loss1:0.0093 loss2:0.0964 loss3:0.0000 | AUC:0.7717 Anomaly AUC:0.9333
[2024-03-11 13:40:44,527][main.py][line:124][INFO] [Epoch:17/20, Batch:49/320]: loss1:0.0081 loss2:0.0490 loss3:0.0000 | AUC:0.7700 Anomaly AUC:0.9322
[2024-03-11 13:40:58,524][main.py][line:124][INFO] [Epoch:17/20, Batch:59/320]: loss1:0.0097 loss2:0.0451 loss3:0.0000 | AUC:0.7558 Anomaly AUC:0.9284
[2024-03-11 13:41:12,779][main.py][line:124][INFO] [Epoch:17/20, Batch:69/320]: loss1:0.0065 loss2:0.0615 loss3:0.0000 | AUC:0.7743 Anomaly AUC:0.9321
[2024-03-11 13:41:27,117][main.py][line:124][INFO] [Epoch:17/20, Batch:79/320]: loss1:0.0086 loss2:0.0396 loss3:0.0000 | AUC:0.7808 Anomaly AUC:0.9325
[2024-03-11 13:41:41,444][main.py][line:124][INFO] [Epoch:17/20, Batch:89/320]: loss1:0.0039 loss2:0.0302 loss3:0.0000 | AUC:0.7808 Anomaly AUC:0.9317
[2024-03-11 13:41:55,663][main.py][line:124][INFO] [Epoch:17/20, Batch:99/320]: loss1:0.0105 loss2:0.0466 loss3:0.0000 | AUC:0.7621 Anomaly AUC:0.9294
[2024-03-11 13:42:09,977][main.py][line:124][INFO] [Epoch:17/20, Batch:109/320]: loss1:0.0098 loss2:0.0351 loss3:0.0000 | AUC:0.7774 Anomaly AUC:0.9338
[2024-03-11 13:42:24,277][main.py][line:124][INFO] [Epoch:17/20, Batch:119/320]: loss1:0.0058 loss2:0.0336 loss3:0.0000 | AUC:0.7855 Anomaly AUC:0.9353
[2024-03-11 13:42:38,564][main.py][line:124][INFO] [Epoch:17/20, Batch:129/320]: loss1:0.0086 loss2:0.0569 loss3:0.0000 | AUC:0.7992 Anomaly AUC:0.9381
[2024-03-11 13:42:53,301][main.py][line:124][INFO] [Epoch:17/20, Batch:139/320]: loss1:0.0105 loss2:0.0327 loss3:0.0000 | AUC:0.7889 Anomaly AUC:0.9349
[2024-03-11 13:43:07,954][main.py][line:124][INFO] [Epoch:17/20, Batch:149/320]: loss1:0.0105 loss2:0.0289 loss3:0.0000 | AUC:0.7855 Anomaly AUC:0.9325
[2024-03-11 13:43:22,466][main.py][line:124][INFO] [Epoch:17/20, Batch:159/320]: loss1:0.0078 loss2:0.0468 loss3:0.0000 | AUC:0.7801 Anomaly AUC:0.9319
[2024-03-11 13:43:36,745][main.py][line:124][INFO] [Epoch:17/20, Batch:169/320]: loss1:0.0079 loss2:0.0343 loss3:0.0000 | AUC:0.7633 Anomaly AUC:0.9277
[2024-03-11 13:43:51,666][main.py][line:124][INFO] [Epoch:17/20, Batch:179/320]: loss1:0.0072 loss2:0.0308 loss3:0.0000 | AUC:0.7548 Anomaly AUC:0.9263
[2024-03-11 13:44:06,477][main.py][line:124][INFO] [Epoch:17/20, Batch:189/320]: loss1:0.0088 loss2:0.0428 loss3:0.0000 | AUC:0.7620 Anomaly AUC:0.9280
[2024-03-11 13:44:21,010][main.py][line:124][INFO] [Epoch:17/20, Batch:199/320]: loss1:0.0055 loss2:0.0649 loss3:0.0000 | AUC:0.7680 Anomaly AUC:0.9277
[2024-03-11 13:44:35,343][main.py][line:124][INFO] [Epoch:17/20, Batch:209/320]: loss1:0.0091 loss2:0.0429 loss3:0.0000 | AUC:0.7813 Anomaly AUC:0.9318
[2024-03-11 13:44:49,757][main.py][line:124][INFO] [Epoch:17/20, Batch:219/320]: loss1:0.0184 loss2:0.0596 loss3:0.0000 | AUC:0.7669 Anomaly AUC:0.9319
[2024-03-11 13:45:04,075][main.py][line:124][INFO] [Epoch:17/20, Batch:229/320]: loss1:0.0196 loss2:0.0302 loss3:0.0000 | AUC:0.7597 Anomaly AUC:0.9300
[2024-03-11 13:45:18,644][main.py][line:124][INFO] [Epoch:17/20, Batch:239/320]: loss1:0.0071 loss2:0.0287 loss3:0.0000 | AUC:0.7607 Anomaly AUC:0.9273
[2024-03-11 13:45:33,321][main.py][line:124][INFO] [Epoch:17/20, Batch:249/320]: loss1:0.0102 loss2:0.0417 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9241
[2024-03-11 13:45:48,696][main.py][line:124][INFO] [Epoch:17/20, Batch:259/320]: loss1:0.0065 loss2:0.0457 loss3:0.0000 | AUC:0.7616 Anomaly AUC:0.9245
[2024-03-11 13:46:03,336][main.py][line:124][INFO] [Epoch:17/20, Batch:269/320]: loss1:0.0058 loss2:0.0425 loss3:0.0000 | AUC:0.7534 Anomaly AUC:0.9236
[2024-03-11 13:46:17,822][main.py][line:124][INFO] [Epoch:17/20, Batch:279/320]: loss1:0.0131 loss2:0.0438 loss3:0.0000 | AUC:0.7546 Anomaly AUC:0.9241
[2024-03-11 13:46:33,863][main.py][line:124][INFO] [Epoch:17/20, Batch:289/320]: loss1:0.0223 loss2:0.0544 loss3:0.0000 | AUC:0.7678 Anomaly AUC:0.9314
[2024-03-11 13:46:48,841][main.py][line:153][INFO] [Epoch:17/20]: lr:0.00010 | loss1:0.0126 loss2:0.0497 loss3:0.0000 | AUC:0.7868 Anomaly AUC:0.9359
[2024-03-11 13:47:04,237][main.py][line:124][INFO] [Epoch:18/20, Batch:9/320]: loss1:0.1496 loss2:0.0909 loss3:0.0000 | AUC:0.7909 Anomaly AUC:0.9323
[2024-03-11 13:47:19,623][main.py][line:124][INFO] [Epoch:18/20, Batch:19/320]: loss1:0.0126 loss2:0.0560 loss3:0.0000 | AUC:0.7786 Anomaly AUC:0.9341
[2024-03-11 13:47:34,058][main.py][line:124][INFO] [Epoch:18/20, Batch:29/320]: loss1:0.0119 loss2:0.0532 loss3:0.0000 | AUC:0.7662 Anomaly AUC:0.9315
[2024-03-11 13:47:48,385][main.py][line:124][INFO] [Epoch:18/20, Batch:39/320]: loss1:0.0235 loss2:0.0420 loss3:0.0000 | AUC:0.7538 Anomaly AUC:0.9272
[2024-03-11 13:48:02,772][main.py][line:124][INFO] [Epoch:18/20, Batch:49/320]: loss1:0.0123 loss2:0.0426 loss3:0.0000 | AUC:0.7414 Anomaly AUC:0.9259
[2024-03-11 13:48:17,096][main.py][line:124][INFO] [Epoch:18/20, Batch:59/320]: loss1:0.0135 loss2:0.0508 loss3:0.0000 | AUC:0.7356 Anomaly AUC:0.9234
[2024-03-11 13:48:31,580][main.py][line:124][INFO] [Epoch:18/20, Batch:69/320]: loss1:0.0093 loss2:0.0348 loss3:0.0000 | AUC:0.7558 Anomaly AUC:0.9272
[2024-03-11 13:48:46,770][main.py][line:124][INFO] [Epoch:18/20, Batch:79/320]: loss1:0.0058 loss2:0.0212 loss3:0.0000 | AUC:0.7691 Anomaly AUC:0.9294
[2024-03-11 13:49:00,993][main.py][line:124][INFO] [Epoch:18/20, Batch:89/320]: loss1:0.0066 loss2:0.0338 loss3:0.0000 | AUC:0.7792 Anomaly AUC:0.9292
[2024-03-11 13:49:15,708][main.py][line:124][INFO] [Epoch:18/20, Batch:99/320]: loss1:0.0092 loss2:0.0373 loss3:0.0000 | AUC:0.7717 Anomaly AUC:0.9275
[2024-03-11 13:49:30,227][main.py][line:124][INFO] [Epoch:18/20, Batch:109/320]: loss1:0.0064 loss2:0.0228 loss3:0.0000 | AUC:0.7618 Anomaly AUC:0.9240
[2024-03-11 13:49:45,042][main.py][line:124][INFO] [Epoch:18/20, Batch:119/320]: loss1:0.0076 loss2:0.0240 loss3:0.0000 | AUC:0.7407 Anomaly AUC:0.9226
[2024-03-11 13:49:59,228][main.py][line:124][INFO] [Epoch:18/20, Batch:129/320]: loss1:0.0107 loss2:0.0291 loss3:0.0000 | AUC:0.7478 Anomaly AUC:0.9251
[2024-03-11 13:50:13,464][main.py][line:124][INFO] [Epoch:18/20, Batch:139/320]: loss1:0.0070 loss2:0.0263 loss3:0.0000 | AUC:0.7594 Anomaly AUC:0.9276
[2024-03-11 13:50:28,278][main.py][line:124][INFO] [Epoch:18/20, Batch:149/320]: loss1:0.0083 loss2:0.0395 loss3:0.0000 | AUC:0.7662 Anomaly AUC:0.9278
[2024-03-11 13:50:42,809][main.py][line:124][INFO] [Epoch:18/20, Batch:159/320]: loss1:0.0069 loss2:0.0279 loss3:0.0000 | AUC:0.7995 Anomaly AUC:0.9366
[2024-03-11 13:50:57,879][main.py][line:124][INFO] [Epoch:18/20, Batch:169/320]: loss1:0.0101 loss2:0.0365 loss3:0.0000 | AUC:0.7997 Anomaly AUC:0.9371
[2024-03-11 13:51:12,472][main.py][line:124][INFO] [Epoch:18/20, Batch:179/320]: loss1:0.0076 loss2:0.0289 loss3:0.0000 | AUC:0.7832 Anomaly AUC:0.9318
[2024-03-11 13:51:27,536][main.py][line:124][INFO] [Epoch:18/20, Batch:189/320]: loss1:0.0106 loss2:0.0272 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9277
[2024-03-11 13:51:42,071][main.py][line:124][INFO] [Epoch:18/20, Batch:199/320]: loss1:0.0289 loss2:0.0262 loss3:0.0000 | AUC:0.7572 Anomaly AUC:0.9279
[2024-03-11 13:51:56,681][main.py][line:124][INFO] [Epoch:18/20, Batch:209/320]: loss1:0.0079 loss2:0.0305 loss3:0.0000 | AUC:0.7893 Anomaly AUC:0.9361
[2024-03-11 13:52:11,421][main.py][line:124][INFO] [Epoch:18/20, Batch:219/320]: loss1:0.0056 loss2:0.0261 loss3:0.0000 | AUC:0.7965 Anomaly AUC:0.9373
[2024-03-11 13:52:26,527][main.py][line:124][INFO] [Epoch:18/20, Batch:229/320]: loss1:0.0087 loss2:0.0303 loss3:0.0000 | AUC:0.7851 Anomaly AUC:0.9343
[2024-03-11 13:52:41,272][main.py][line:124][INFO] [Epoch:18/20, Batch:239/320]: loss1:0.0090 loss2:0.0324 loss3:0.0000 | AUC:0.7759 Anomaly AUC:0.9315
[2024-03-11 13:52:56,339][main.py][line:124][INFO] [Epoch:18/20, Batch:249/320]: loss1:0.0119 loss2:0.0463 loss3:0.0000 | AUC:0.7894 Anomaly AUC:0.9326
[2024-03-11 13:53:11,342][main.py][line:124][INFO] [Epoch:18/20, Batch:259/320]: loss1:0.0140 loss2:0.0355 loss3:0.0000 | AUC:0.7800 Anomaly AUC:0.9304
[2024-03-11 13:53:26,498][main.py][line:124][INFO] [Epoch:18/20, Batch:269/320]: loss1:0.0064 loss2:0.0460 loss3:0.0000 | AUC:0.7711 Anomaly AUC:0.9296
[2024-03-11 13:53:41,740][main.py][line:124][INFO] [Epoch:18/20, Batch:279/320]: loss1:0.0127 loss2:0.0424 loss3:0.0000 | AUC:0.7492 Anomaly AUC:0.9237
[2024-03-11 13:53:56,883][main.py][line:124][INFO] [Epoch:18/20, Batch:289/320]: loss1:0.0115 loss2:0.0648 loss3:0.0000 | AUC:0.7386 Anomaly AUC:0.9213
[2024-03-11 13:54:13,843][main.py][line:153][INFO] [Epoch:18/20]: lr:0.00010 | loss1:0.0065 loss2:0.0224 loss3:0.0000 | AUC:0.7462 Anomaly AUC:0.9226
[2024-03-11 13:54:29,569][main.py][line:124][INFO] [Epoch:19/20, Batch:9/320]: loss1:0.0109 loss2:0.0248 loss3:0.0000 | AUC:0.7508 Anomaly AUC:0.9237
[2024-03-11 13:54:43,923][main.py][line:124][INFO] [Epoch:19/20, Batch:19/320]: loss1:0.0128 loss2:0.0337 loss3:0.0000 | AUC:0.7518 Anomaly AUC:0.9222
[2024-03-11 13:54:58,572][main.py][line:124][INFO] [Epoch:19/20, Batch:29/320]: loss1:0.0061 loss2:0.0241 loss3:0.0000 | AUC:0.7578 Anomaly AUC:0.9240
[2024-03-11 13:55:12,910][main.py][line:124][INFO] [Epoch:19/20, Batch:39/320]: loss1:0.0092 loss2:0.0234 loss3:0.0000 | AUC:0.7589 Anomaly AUC:0.9267
[2024-03-11 13:55:27,340][main.py][line:124][INFO] [Epoch:19/20, Batch:49/320]: loss1:0.0086 loss2:0.0262 loss3:0.0000 | AUC:0.7590 Anomaly AUC:0.9278
[2024-03-11 13:55:42,071][main.py][line:124][INFO] [Epoch:19/20, Batch:59/320]: loss1:0.0066 loss2:0.0238 loss3:0.0000 | AUC:0.7602 Anomaly AUC:0.9278
[2024-03-11 13:55:56,310][main.py][line:124][INFO] [Epoch:19/20, Batch:69/320]: loss1:0.0066 loss2:0.0430 loss3:0.0000 | AUC:0.7586 Anomaly AUC:0.9272
[2024-03-11 13:56:10,982][main.py][line:124][INFO] [Epoch:19/20, Batch:79/320]: loss1:0.0086 loss2:0.0284 loss3:0.0000 | AUC:0.7518 Anomaly AUC:0.9252
[2024-03-11 13:56:25,060][main.py][line:124][INFO] [Epoch:19/20, Batch:89/320]: loss1:0.0066 loss2:0.0218 loss3:0.0000 | AUC:0.7513 Anomaly AUC:0.9237
[2024-03-11 13:56:39,623][main.py][line:124][INFO] [Epoch:19/20, Batch:99/320]: loss1:0.0080 loss2:0.0275 loss3:0.0000 | AUC:0.7507 Anomaly AUC:0.9239
[2024-03-11 13:56:53,922][main.py][line:124][INFO] [Epoch:19/20, Batch:109/320]: loss1:0.0075 loss2:0.0214 loss3:0.0000 | AUC:0.7298 Anomaly AUC:0.9199
[2024-03-11 13:57:08,310][main.py][line:124][INFO] [Epoch:19/20, Batch:119/320]: loss1:0.0094 loss2:0.0316 loss3:0.0000 | AUC:0.7399 Anomaly AUC:0.9230
[2024-03-11 13:57:22,916][main.py][line:124][INFO] [Epoch:19/20, Batch:129/320]: loss1:0.0088 loss2:0.0397 loss3:0.0000 | AUC:0.7336 Anomaly AUC:0.9232
[2024-03-11 13:57:37,378][main.py][line:124][INFO] [Epoch:19/20, Batch:139/320]: loss1:0.0061 loss2:0.0222 loss3:0.0000 | AUC:0.7571 Anomaly AUC:0.9280
[2024-03-11 13:57:52,118][main.py][line:124][INFO] [Epoch:19/20, Batch:149/320]: loss1:0.0069 loss2:0.0295 loss3:0.0000 | AUC:0.7429 Anomaly AUC:0.9227
[2024-03-11 13:58:06,300][main.py][line:124][INFO] [Epoch:19/20, Batch:159/320]: loss1:0.0048 loss2:0.0210 loss3:0.0000 | AUC:0.7374 Anomaly AUC:0.9209
[2024-03-11 13:58:20,589][main.py][line:124][INFO] [Epoch:19/20, Batch:169/320]: loss1:0.0056 loss2:0.0322 loss3:0.0000 | AUC:0.7411 Anomaly AUC:0.9197
[2024-03-11 13:58:35,965][main.py][line:124][INFO] [Epoch:19/20, Batch:179/320]: loss1:0.0057 loss2:0.0207 loss3:0.0000 | AUC:0.7548 Anomaly AUC:0.9242
[2024-03-11 13:58:50,171][main.py][line:124][INFO] [Epoch:19/20, Batch:189/320]: loss1:0.0074 loss2:0.0379 loss3:0.0000 | AUC:0.7665 Anomaly AUC:0.9294
[2024-03-11 13:59:04,751][main.py][line:124][INFO] [Epoch:19/20, Batch:199/320]: loss1:0.0072 loss2:0.0216 loss3:0.0000 | AUC:0.7641 Anomaly AUC:0.9266
[2024-03-11 13:59:19,245][main.py][line:124][INFO] [Epoch:19/20, Batch:209/320]: loss1:0.0542 loss2:0.0574 loss3:0.0000 | AUC:0.7573 Anomaly AUC:0.9233
[2024-03-11 13:59:33,660][main.py][line:124][INFO] [Epoch:19/20, Batch:219/320]: loss1:0.0111 loss2:0.0342 loss3:0.0000 | AUC:0.7416 Anomaly AUC:0.9198
[2024-03-11 13:59:48,859][main.py][line:124][INFO] [Epoch:19/20, Batch:229/320]: loss1:0.0070 loss2:0.0273 loss3:0.0000 | AUC:0.7435 Anomaly AUC:0.9195
[2024-03-11 14:00:03,238][main.py][line:124][INFO] [Epoch:19/20, Batch:239/320]: loss1:0.0055 loss2:0.0355 loss3:0.0000 | AUC:0.7198 Anomaly AUC:0.9145
[2024-03-11 14:00:17,596][main.py][line:124][INFO] [Epoch:19/20, Batch:249/320]: loss1:0.0071 loss2:0.0299 loss3:0.0000 | AUC:0.7453 Anomaly AUC:0.9188
[2024-03-11 14:00:32,994][main.py][line:124][INFO] [Epoch:19/20, Batch:259/320]: loss1:0.0054 loss2:0.0212 loss3:0.0000 | AUC:0.7416 Anomaly AUC:0.9181
[2024-03-11 14:00:47,290][main.py][line:124][INFO] [Epoch:19/20, Batch:269/320]: loss1:0.0101 loss2:0.0309 loss3:0.0000 | AUC:0.7417 Anomaly AUC:0.9184
[2024-03-11 14:01:02,172][main.py][line:124][INFO] [Epoch:19/20, Batch:279/320]: loss1:0.0083 loss2:0.0348 loss3:0.0000 | AUC:0.7374 Anomaly AUC:0.9184
[2024-03-11 14:01:16,514][main.py][line:124][INFO] [Epoch:19/20, Batch:289/320]: loss1:0.0111 loss2:0.0336 loss3:0.0000 | AUC:0.7594 Anomaly AUC:0.9281
[2024-03-11 14:01:30,376][main.py][line:153][INFO] [Epoch:19/20]: lr:0.00010 | loss1:0.0074 loss2:0.0211 loss3:0.0000 | AUC:0.7485 Anomaly AUC:0.9207
[2024-03-11 14:01:45,861][main.py][line:124][INFO] [Epoch:20/20, Batch:9/320]: loss1:0.0094 loss2:0.0247 loss3:0.0000 | AUC:0.7404 Anomaly AUC:0.9192
[2024-03-11 14:01:59,999][main.py][line:124][INFO] [Epoch:20/20, Batch:19/320]: loss1:0.0602 loss2:0.1214 loss3:0.0000 | AUC:0.7572 Anomaly AUC:0.9234
[2024-03-11 14:02:14,267][main.py][line:124][INFO] [Epoch:20/20, Batch:29/320]: loss1:0.0302 loss2:0.0563 loss3:0.0000 | AUC:0.7584 Anomaly AUC:0.9310
[2024-03-11 14:02:30,355][main.py][line:124][INFO] [Epoch:20/20, Batch:39/320]: loss1:0.0092 loss2:0.0312 loss3:0.0000 | AUC:0.7327 Anomaly AUC:0.9198
[2024-03-11 14:02:44,641][main.py][line:124][INFO] [Epoch:20/20, Batch:49/320]: loss1:0.0175 loss2:0.1414 loss3:0.0000 | AUC:0.7296 Anomaly AUC:0.9199
[2024-03-11 14:02:58,867][main.py][line:124][INFO] [Epoch:20/20, Batch:59/320]: loss1:0.0255 loss2:0.0437 loss3:0.0000 | AUC:0.7471 Anomaly AUC:0.9206
[2024-03-11 14:03:14,961][main.py][line:124][INFO] [Epoch:20/20, Batch:69/320]: loss1:0.0924 loss2:0.1411 loss3:0.0000 | AUC:0.7456 Anomaly AUC:0.9205
[2024-03-11 14:03:29,272][main.py][line:124][INFO] [Epoch:20/20, Batch:79/320]: loss1:0.0200 loss2:0.0931 loss3:0.0000 | AUC:0.8084 Anomaly AUC:0.9423
[2024-03-11 14:03:44,329][main.py][line:124][INFO] [Epoch:20/20, Batch:89/320]: loss1:0.0702 loss2:0.1086 loss3:0.0000 | AUC:0.7708 Anomaly AUC:0.9368
[2024-03-11 14:03:58,723][main.py][line:124][INFO] [Epoch:20/20, Batch:99/320]: loss1:0.0155 loss2:0.0859 loss3:0.0000 | AUC:0.8208 Anomaly AUC:0.9438
[2024-03-11 14:04:12,972][main.py][line:124][INFO] [Epoch:20/20, Batch:109/320]: loss1:0.0159 loss2:0.0514 loss3:0.0000 | AUC:0.7625 Anomaly AUC:0.9289
[2024-03-11 14:04:28,224][main.py][line:124][INFO] [Epoch:20/20, Batch:119/320]: loss1:0.0313 loss2:0.0700 loss3:0.0000 | AUC:0.7715 Anomaly AUC:0.9327
[2024-03-11 14:04:42,480][main.py][line:124][INFO] [Epoch:20/20, Batch:129/320]: loss1:0.0377 loss2:0.1048 loss3:0.0000 | AUC:0.8008 Anomaly AUC:0.9394
[2024-03-11 14:04:57,604][main.py][line:124][INFO] [Epoch:20/20, Batch:139/320]: loss1:0.0160 loss2:0.0323 loss3:0.0000 | AUC:0.7782 Anomaly AUC:0.9323
[2024-03-11 14:05:12,189][main.py][line:124][INFO] [Epoch:20/20, Batch:149/320]: loss1:0.0120 loss2:0.0426 loss3:0.0000 | AUC:0.7967 Anomaly AUC:0.9354
[2024-03-11 14:05:26,214][main.py][line:124][INFO] [Epoch:20/20, Batch:159/320]: loss1:0.0080 loss2:0.0418 loss3:0.0000 | AUC:0.8047 Anomaly AUC:0.9365
[2024-03-11 14:05:42,332][main.py][line:124][INFO] [Epoch:20/20, Batch:169/320]: loss1:0.0211 loss2:0.0397 loss3:0.0000 | AUC:0.8013 Anomaly AUC:0.9349
[2024-03-11 14:05:56,651][main.py][line:124][INFO] [Epoch:20/20, Batch:179/320]: loss1:0.0089 loss2:0.0719 loss3:0.0000 | AUC:0.8074 Anomaly AUC:0.9375
[2024-03-11 14:06:11,107][main.py][line:124][INFO] [Epoch:20/20, Batch:189/320]: loss1:0.0080 loss2:0.0243 loss3:0.0000 | AUC:0.8116 Anomaly AUC:0.9389
[2024-03-11 14:06:26,617][main.py][line:124][INFO] [Epoch:20/20, Batch:199/320]: loss1:0.0086 loss2:0.0645 loss3:0.0000 | AUC:0.7702 Anomaly AUC:0.9311
[2024-03-11 14:06:41,145][main.py][line:124][INFO] [Epoch:20/20, Batch:209/320]: loss1:0.0106 loss2:0.0806 loss3:0.0000 | AUC:0.7018 Anomaly AUC:0.9135
[2024-03-11 14:06:56,081][main.py][line:124][INFO] [Epoch:20/20, Batch:219/320]: loss1:0.0104 loss2:0.0362 loss3:0.0000 | AUC:0.7317 Anomaly AUC:0.9188
[2024-03-11 14:07:12,576][main.py][line:124][INFO] [Epoch:20/20, Batch:229/320]: loss1:0.0087 loss2:0.0366 loss3:0.0000 | AUC:0.7422 Anomaly AUC:0.9241
[2024-03-11 14:07:26,784][main.py][line:124][INFO] [Epoch:20/20, Batch:239/320]: loss1:0.0107 loss2:0.0301 loss3:0.0000 | AUC:0.7414 Anomaly AUC:0.9247
[2024-03-11 14:07:41,076][main.py][line:124][INFO] [Epoch:20/20, Batch:249/320]: loss1:0.0135 loss2:0.0404 loss3:0.0000 | AUC:0.7730 Anomaly AUC:0.9307
[2024-03-11 14:07:57,149][main.py][line:124][INFO] [Epoch:20/20, Batch:259/320]: loss1:0.0108 loss2:0.0301 loss3:0.0000 | AUC:0.7754 Anomaly AUC:0.9284
[2024-03-11 14:08:11,424][main.py][line:124][INFO] [Epoch:20/20, Batch:269/320]: loss1:0.0125 loss2:0.0706 loss3:0.0000 | AUC:0.7720 Anomaly AUC:0.9268
[2024-03-11 14:08:26,444][main.py][line:124][INFO] [Epoch:20/20, Batch:279/320]: loss1:0.0179 loss2:0.0385 loss3:0.0000 | AUC:0.8053 Anomaly AUC:0.9347
[2024-03-11 14:08:40,808][main.py][line:124][INFO] [Epoch:20/20, Batch:289/320]: loss1:0.0117 loss2:0.0824 loss3:0.0000 | AUC:0.7627 Anomaly AUC:0.9255
[2024-03-11 14:08:54,553][main.py][line:153][INFO] [Epoch:20/20]: lr:0.00010 | loss1:0.0102 loss2:0.1423 loss3:0.0000 | AUC:0.7687 Anomaly AUC:0.9312
[2024-03-11 14:08:54,585][main.py][line:171][INFO] Training completes in 139m 13s | best AUCAP:0.8368 Anomaly AUC:0.9484

