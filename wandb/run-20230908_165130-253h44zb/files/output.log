
8000 8100 2900
[2023-09-08 16:51:32,078][main.py][line:165][INFO] total params:7.5195M
[2023-09-08 16:51:32,078][main.py][line:168][INFO] Training Mode
[2023-09-08 16:51:32,078][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 16:51:32,078][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 16:51:40,082][main.py][line:82][INFO] Random initialize AUCAUC:0.5223 Anomaly AUC:0.49998
[2023-09-08 16:51:58,378][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3580 loss2:1.1621 loss3:0.3261 | AUC:0.8121 Anomaly AUC:0.6739
[2023-09-08 16:52:16,381][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0455 loss2:0.8325 loss3:0.2229 | AUC:0.8152 Anomaly AUC:0.6726
[2023-09-08 16:52:34,500][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0271 loss2:0.7438 loss3:0.1565 | AUC:0.8008 Anomaly AUC:0.6755
[2023-09-08 16:52:52,950][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0222 loss2:0.6590 loss3:0.1010 | AUC:0.8269 Anomaly AUC:0.6845
[2023-09-08 16:53:11,408][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0199 loss2:0.6125 loss3:0.0788 | AUC:0.8229 Anomaly AUC:0.6792
[2023-09-08 16:53:30,027][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0143 loss2:0.5575 loss3:0.0639 | AUC:0.8356 Anomaly AUC:0.6819
[2023-09-08 16:53:48,732][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0091 loss2:0.4882 loss3:0.0399 | AUC:0.8364 Anomaly AUC:0.6749
[2023-09-08 16:54:07,397][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0086 loss2:0.4448 loss3:0.0312 | AUC:0.8307 Anomaly AUC:0.6732
[2023-09-08 16:54:26,159][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0066 loss2:0.4008 loss3:0.0271 | AUC:0.8400 Anomaly AUC:0.6796
[2023-09-08 16:54:44,782][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0086 loss2:0.3931 loss3:0.0285 | AUC:0.8338 Anomaly AUC:0.6748
[2023-09-08 16:55:03,384][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0041 loss2:0.3282 loss3:0.0188 | AUC:0.8422 Anomaly AUC:0.6731
[2023-09-08 16:55:22,170][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0038 loss2:0.3039 loss3:0.0175 | AUC:0.8372 Anomaly AUC:0.6760
[2023-09-08 16:55:40,860][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0501 loss2:0.5177 loss3:0.0557 | AUC:0.8222 Anomaly AUC:0.6677
[2023-09-08 16:55:59,635][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0063 loss2:0.3703 loss3:0.0300 | AUC:0.8411 Anomaly AUC:0.6721
[2023-09-08 16:56:18,432][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0030 loss2:0.2882 loss3:0.0166 | AUC:0.8457 Anomaly AUC:0.6786
[2023-09-08 16:56:37,069][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0025 loss2:0.2614 loss3:0.0139 | AUC:0.8428 Anomaly AUC:0.6765
[2023-09-08 16:56:55,847][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0032 loss2:0.2480 loss3:0.0140 | AUC:0.8435 Anomaly AUC:0.6753
[2023-09-08 16:57:14,671][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0021 loss2:0.2301 loss3:0.0127 | AUC:0.8493 Anomaly AUC:0.6742
[2023-09-08 16:57:33,420][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0012 loss2:0.2165 loss3:0.0115 | AUC:0.8487 Anomaly AUC:0.6760
[2023-09-08 16:57:52,156][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0969 loss2:0.4701 loss3:0.0650 | AUC:0.8247 Anomaly AUC:0.6523
[2023-09-08 16:58:10,860][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0068 loss2:0.5549 loss3:0.0701 | AUC:0.8330 Anomaly AUC:0.6703
[2023-09-08 16:58:29,576][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0035 loss2:0.3117 loss3:0.0236 | AUC:0.8418 Anomaly AUC:0.6758
[2023-09-08 16:58:48,382][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0030 loss2:0.2557 loss3:0.0160 | AUC:0.8432 Anomaly AUC:0.6795
[2023-09-08 16:59:07,162][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0021 loss2:0.2328 loss3:0.0133 | AUC:0.8446 Anomaly AUC:0.6775
[2023-09-08 16:59:25,976][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0017 loss2:0.2173 loss3:0.0120 | AUC:0.8467 Anomaly AUC:0.6806
[2023-09-08 16:59:44,803][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0016 loss2:0.2067 loss3:0.0113 | AUC:0.8458 Anomaly AUC:0.6787
[2023-09-08 17:00:03,550][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0016 loss2:0.1992 loss3:0.0111 | AUC:0.8473 Anomaly AUC:0.6782
[2023-09-08 17:00:22,385][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0071 loss2:0.2170 loss3:0.0147 | AUC:0.8554 Anomaly AUC:0.6924
[2023-09-08 17:00:41,194][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0041 loss2:0.2075 loss3:0.0160 | AUC:0.8491 Anomaly AUC:0.6887
[2023-09-08 17:01:00,052][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0010 loss2:0.1804 loss3:0.0117 | AUC:0.8499 Anomaly AUC:0.6812
[2023-09-08 17:01:18,872][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0009 loss2:0.1682 loss3:0.0105 | AUC:0.8484 Anomaly AUC:0.6779
[2023-09-08 17:01:37,649][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0009 loss2:0.1630 loss3:0.0102 | AUC:0.8489 Anomaly AUC:0.6791
[2023-09-08 17:01:56,415][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0008 loss2:0.1557 loss3:0.0101 | AUC:0.8475 Anomaly AUC:0.6762
[2023-09-08 17:02:15,251][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0007 loss2:0.1503 loss3:0.0098 | AUC:0.8473 Anomaly AUC:0.6769
[2023-09-08 17:02:33,974][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0180 loss2:0.2840 loss3:0.0281 | AUC:0.8443 Anomaly AUC:0.6868
[2023-09-08 17:02:52,736][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0018 loss2:0.1618 loss3:0.0136 | AUC:0.8480 Anomaly AUC:0.6850
[2023-09-08 17:03:11,567][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0017 loss2:0.1470 loss3:0.0117 | AUC:0.8515 Anomaly AUC:0.6891
[2023-09-08 17:03:30,271][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0009 loss2:0.1358 loss3:0.0105 | AUC:0.8481 Anomaly AUC:0.6757
[2023-09-08 17:03:49,091][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0007 loss2:0.1291 loss3:0.0101 | AUC:0.8512 Anomaly AUC:0.6816
[2023-09-08 17:04:07,897][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0005 loss2:0.1227 loss3:0.0098 | AUC:0.8493 Anomaly AUC:0.6790
[2023-09-08 17:04:26,852][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0008 loss2:0.1191 loss3:0.0098 | AUC:0.8464 Anomaly AUC:0.6797
[2023-09-08 17:04:45,662][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0034 loss2:0.1457 loss3:0.0162 | AUC:0.8392 Anomaly AUC:0.6730
[2023-09-08 17:05:04,525][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0004 loss2:0.1129 loss3:0.0102 | AUC:0.8413 Anomaly AUC:0.6683
[2023-09-08 17:05:23,374][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0005 loss2:0.1062 loss3:0.0097 | AUC:0.8417 Anomaly AUC:0.6682
[2023-09-08 17:05:42,249][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0004 loss2:0.1015 loss3:0.0094 | AUC:0.8435 Anomaly AUC:0.6707
[2023-09-08 17:06:01,208][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0003 loss2:0.0973 loss3:0.0092 | AUC:0.8454 Anomaly AUC:0.6723
[2023-09-08 17:06:20,078][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0003 loss2:0.0936 loss3:0.0090 | AUC:0.8452 Anomaly AUC:0.6732
[2023-09-08 17:06:38,873][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0003 loss2:0.0900 loss3:0.0088 | AUC:0.8446 Anomaly AUC:0.6726
[2023-09-08 17:06:57,779][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0003 loss2:0.0868 loss3:0.0087 | AUC:0.8463 Anomaly AUC:0.6771
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 49, in train_func
    logger_wandb.log({"loss": loss1.item(), "loss2": loss2.item(), "loss3": UR_loss.item()})
AttributeError: 'int' object has no attribute 'item'