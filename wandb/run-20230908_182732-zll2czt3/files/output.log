
8000 8100 2900
[2023-09-08 18:27:34,078][main.py][line:165][INFO] total params:7.5195M
[2023-09-08 18:27:34,079][main.py][line:168][INFO] Training Mode
[2023-09-08 18:27:34,079][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 18:27:34,079][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 18:27:42,104][main.py][line:82][INFO] Random initialize AUCAUC:0.5223 Anomaly AUC:0.49998
[2023-09-08 18:28:00,494][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3561 loss2:1.1609 loss3:0.3260 | AUC:0.8097 Anomaly AUC:0.6761
[2023-09-08 18:28:18,528][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0456 loss2:0.8332 loss3:0.2223 | AUC:0.8185 Anomaly AUC:0.6773
[2023-09-08 18:28:36,772][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0263 loss2:0.7389 loss3:0.1530 | AUC:0.8138 Anomaly AUC:0.6741
[2023-09-08 18:28:55,395][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0255 loss2:0.6757 loss3:0.1059 | AUC:0.8116 Anomaly AUC:0.6731
[2023-09-08 18:29:13,968][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0200 loss2:0.6253 loss3:0.0797 | AUC:0.8245 Anomaly AUC:0.6716
[2023-09-08 18:29:32,608][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0109 loss2:0.5411 loss3:0.0501 | AUC:0.8335 Anomaly AUC:0.6742
[2023-09-08 18:29:51,289][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0577 loss2:0.6224 loss3:0.0706 | AUC:0.8163 Anomaly AUC:0.6734
[2023-09-08 18:30:10,007][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0082 loss2:0.4869 loss3:0.0398 | AUC:0.8261 Anomaly AUC:0.6733
[2023-09-08 18:30:28,875][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0077 loss2:0.4412 loss3:0.0309 | AUC:0.8261 Anomaly AUC:0.6699
[2023-09-08 18:30:47,676][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0064 loss2:0.4033 loss3:0.0257 | AUC:0.8287 Anomaly AUC:0.6662
[2023-09-08 18:31:06,581][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0068 loss2:0.3808 loss3:0.0243 | AUC:0.8360 Anomaly AUC:0.6798
[2023-09-08 18:31:25,412][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0052 loss2:0.3458 loss3:0.0204 | AUC:0.8404 Anomaly AUC:0.6748
[2023-09-08 18:31:44,207][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0058 loss2:0.3205 loss3:0.0177 | AUC:0.8437 Anomaly AUC:0.6793
[2023-09-08 18:32:03,051][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0034 loss2:0.3049 loss3:0.0179 | AUC:0.8381 Anomaly AUC:0.6680
[2023-09-08 18:32:22,023][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0031 loss2:0.2748 loss3:0.0148 | AUC:0.8446 Anomaly AUC:0.6801
[2023-09-08 18:32:40,758][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0025 loss2:0.2588 loss3:0.0138 | AUC:0.8440 Anomaly AUC:0.6767
[2023-09-08 18:32:59,558][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0822 loss2:0.4998 loss3:0.0692 | AUC:0.7905 Anomaly AUC:0.6538
[2023-09-08 18:33:18,372][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0055 loss2:0.4282 loss3:0.0352 | AUC:0.8379 Anomaly AUC:0.6802
[2023-09-08 18:33:37,162][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0036 loss2:0.2923 loss3:0.0188 | AUC:0.8443 Anomaly AUC:0.6807
[2023-09-08 18:33:56,063][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0027 loss2:0.2553 loss3:0.0147 | AUC:0.8496 Anomaly AUC:0.6850
[2023-09-08 18:34:14,815][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0018 loss2:0.2331 loss3:0.0127 | AUC:0.8486 Anomaly AUC:0.6798
[2023-09-08 18:34:33,641][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0064 loss2:0.2532 loss3:0.0151 | AUC:0.8410 Anomaly AUC:0.6704
[2023-09-08 18:34:52,572][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0016 loss2:0.2212 loss3:0.0130 | AUC:0.8495 Anomaly AUC:0.6811
[2023-09-08 18:35:11,321][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0011 loss2:0.2001 loss3:0.0114 | AUC:0.8458 Anomaly AUC:0.6739
[2023-09-08 18:35:30,213][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0025 loss2:0.1969 loss3:0.0124 | AUC:0.8519 Anomaly AUC:0.6799
[2023-09-08 18:35:48,995][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0016 loss2:0.1863 loss3:0.0115 | AUC:0.8479 Anomaly AUC:0.6754
[2023-09-08 18:36:07,863][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0046 loss2:0.1968 loss3:0.0139 | AUC:0.8285 Anomaly AUC:0.6554
[2023-09-08 18:36:26,647][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0024 loss2:0.1912 loss3:0.0140 | AUC:0.8563 Anomaly AUC:0.6919
[2023-09-08 18:36:45,531][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0009 loss2:0.1653 loss3:0.0108 | AUC:0.8526 Anomaly AUC:0.6833
[2023-09-08 18:37:04,393][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0007 loss2:0.1572 loss3:0.0104 | AUC:0.8585 Anomaly AUC:0.6891
[2023-09-08 18:37:23,299][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0006 loss2:0.1498 loss3:0.0100 | AUC:0.8590 Anomaly AUC:0.6901
[2023-09-08 18:37:42,101][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0005 loss2:0.1450 loss3:0.0099 | AUC:0.8570 Anomaly AUC:0.6874
[2023-09-08 18:38:00,991][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0451 loss2:0.4641 loss3:0.0566 | AUC:0.8098 Anomaly AUC:0.6611
[2023-09-08 18:38:19,936][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0030 loss2:0.2316 loss3:0.0182 | AUC:0.8452 Anomaly AUC:0.6770
[2023-09-08 18:38:38,866][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0012 loss2:0.1568 loss3:0.0119 | AUC:0.8426 Anomaly AUC:0.6696
[2023-09-08 18:38:57,819][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0008 loss2:0.1419 loss3:0.0106 | AUC:0.8482 Anomaly AUC:0.6748
[2023-09-08 18:39:16,614][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0007 loss2:0.1342 loss3:0.0103 | AUC:0.8432 Anomaly AUC:0.6640
[2023-09-08 18:39:35,570][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0006 loss2:0.1293 loss3:0.0100 | AUC:0.8481 Anomaly AUC:0.6748
[2023-09-08 18:39:54,502][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0006 loss2:0.1223 loss3:0.0098 | AUC:0.8460 Anomaly AUC:0.6705
[2023-09-08 18:40:13,387][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0103 loss2:0.2003 loss3:0.0223 | AUC:0.8380 Anomaly AUC:0.6655
[2023-09-08 18:40:32,321][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0007 loss2:0.1213 loss3:0.0113 | AUC:0.8475 Anomaly AUC:0.6711
[2023-09-08 18:40:51,186][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0005 loss2:0.1099 loss3:0.0101 | AUC:0.8463 Anomaly AUC:0.6702
[2023-09-08 18:41:10,207][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0051 loss2:0.1434 loss3:0.0152 | AUC:0.8433 Anomaly AUC:0.6814
[2023-09-08 18:41:29,049][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0007 loss2:0.1073 loss3:0.0113 | AUC:0.8479 Anomaly AUC:0.6796
[2023-09-08 18:41:47,999][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0004 loss2:0.0987 loss3:0.0100 | AUC:0.8473 Anomaly AUC:0.6758
[2023-09-08 18:42:07,136][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0017 loss2:0.0971 loss3:0.0107 | AUC:0.8464 Anomaly AUC:0.6774
[2023-09-08 18:42:26,046][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0004 loss2:0.0906 loss3:0.0096 | AUC:0.8432 Anomaly AUC:0.6756
[2023-09-08 18:42:44,996][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0004 loss2:0.0866 loss3:0.0094 | AUC:0.8458 Anomaly AUC:0.6761
[2023-09-08 18:43:03,926][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0003 loss2:0.0825 loss3:0.0092 | AUC:0.8456 Anomaly AUC:0.6775
[2023-09-08 18:43:22,869][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0003 loss2:0.0788 loss3:0.0090 | AUC:0.8442 Anomaly AUC:0.6731
[2023-09-08 18:43:41,940][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0003 loss2:0.0754 loss3:0.0089 | AUC:0.8450 Anomaly AUC:0.6770
[2023-09-08 18:44:00,877][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0002 loss2:0.0731 loss3:0.0088 | AUC:0.8450 Anomaly AUC:0.6769
[2023-09-08 18:44:19,836][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00010 | loss1:0.0002 loss2:0.0693 loss3:0.0086 | AUC:0.8446 Anomaly AUC:0.6763
[2023-09-08 18:44:38,863][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00010 | loss1:0.0119 loss2:0.1497 loss3:0.0186 | AUC:0.8347 Anomaly AUC:0.6564
[2023-09-08 18:44:57,855][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00010 | loss1:0.0020 loss2:0.1106 loss3:0.0156 | AUC:0.8450 Anomaly AUC:0.6687
[2023-09-08 18:45:16,785][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00010 | loss1:0.0007 loss2:0.0719 loss3:0.0106 | AUC:0.8426 Anomaly AUC:0.6636
[2023-09-08 18:45:35,736][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00010 | loss1:0.0002 loss2:0.0614 loss3:0.0091 | AUC:0.8438 Anomaly AUC:0.6633
[2023-09-08 18:45:54,639][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00010 | loss1:0.0039 loss2:0.0828 loss3:0.0142 | AUC:0.8446 Anomaly AUC:0.6677
[2023-09-08 18:46:13,656][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00010 | loss1:0.0006 loss2:0.0623 loss3:0.0103 | AUC:0.8440 Anomaly AUC:0.6671
[2023-09-08 18:46:32,586][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00010 | loss1:0.0003 loss2:0.0543 loss3:0.0090 | AUC:0.8431 Anomaly AUC:0.6606
[2023-09-08 18:46:51,589][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00010 | loss1:0.0002 loss2:0.0506 loss3:0.0086 | AUC:0.8420 Anomaly AUC:0.6612
[2023-09-08 18:47:10,579][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00010 | loss1:0.0082 loss2:0.1408 loss3:0.0283 | AUC:0.8411 Anomaly AUC:0.6722
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 36, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 39, in forward
    x_k_split = self.DR_DMU(x_split)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 64, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 76, in forward
    x = attn(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 14, in forward
    return self.fn(self.norm(x), **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 55, in forward
    tmp_ones = torch.ones(n).cuda()
KeyboardInterrupt