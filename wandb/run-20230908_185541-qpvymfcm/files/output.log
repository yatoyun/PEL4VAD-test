[2023-09-08 18:55:42,929][main.py][line:165][INFO] total params:7.0450M
[2023-09-08 18:55:42,929][main.py][line:168][INFO] Training Mode
[2023-09-08 18:55:42,930][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 18:55:42,930][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
8000 8100 2900
[2023-09-08 18:55:48,412][main.py][line:82][INFO] Random initialize AUCAUC:0.6061 Anomaly AUC:0.57717
[2023-09-08 18:56:02,673][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.2837 loss2:1.1899 loss3:0.3571 | AUC:0.8008 Anomaly AUC:0.6638
[2023-09-08 18:56:16,608][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0305 loss2:0.8369 loss3:0.2740 | AUC:0.7914 Anomaly AUC:0.6633
[2023-09-08 18:56:30,551][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0164 loss2:0.7428 loss3:0.2297 | AUC:0.8100 Anomaly AUC:0.6597
[2023-09-08 18:56:44,650][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0189 loss2:0.6583 loss3:0.1776 | AUC:0.8157 Anomaly AUC:0.6677
[2023-09-08 18:56:58,968][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0148 loss2:0.5898 loss3:0.1372 | AUC:0.8111 Anomaly AUC:0.6621
[2023-09-08 18:57:13,402][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0120 loss2:0.5341 loss3:0.1129 | AUC:0.8160 Anomaly AUC:0.6692
[2023-09-08 18:57:27,887][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0080 loss2:0.4760 loss3:0.0860 | AUC:0.8221 Anomaly AUC:0.6699
[2023-09-08 18:57:42,450][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0074 loss2:0.4278 loss3:0.0711 | AUC:0.8258 Anomaly AUC:0.6703
[2023-09-08 18:57:57,080][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0068 loss2:0.3931 loss3:0.0574 | AUC:0.8264 Anomaly AUC:0.6803
[2023-09-08 18:58:11,469][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0082 loss2:0.3844 loss3:0.0683 | AUC:0.8286 Anomaly AUC:0.6702
[2023-09-08 18:58:25,869][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0052 loss2:0.3376 loss3:0.0464 | AUC:0.8309 Anomaly AUC:0.6710
[2023-09-08 18:58:40,385][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0042 loss2:0.3078 loss3:0.0347 | AUC:0.8255 Anomaly AUC:0.6626
[2023-09-08 18:58:54,860][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0029 loss2:0.2817 loss3:0.0262 | AUC:0.8344 Anomaly AUC:0.6837
[2023-09-08 18:59:09,290][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0050 loss2:0.2901 loss3:0.0308 | AUC:0.8355 Anomaly AUC:0.6757
[2023-09-08 18:59:23,813][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0031 loss2:0.2679 loss3:0.0262 | AUC:0.8411 Anomaly AUC:0.6698
[2023-09-08 18:59:38,307][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0017 loss2:0.2367 loss3:0.0173 | AUC:0.8509 Anomaly AUC:0.6848
[2023-09-08 18:59:52,895][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0019 loss2:0.2314 loss3:0.0164 | AUC:0.8444 Anomaly AUC:0.6751
[2023-09-08 19:00:07,358][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0033 loss2:0.2353 loss3:0.0184 | AUC:0.8371 Anomaly AUC:0.6634
[2023-09-08 19:00:21,731][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.1846 loss2:0.4444 loss3:0.0963 | AUC:0.8166 Anomaly AUC:0.6716
[2023-09-08 19:00:36,198][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0110 loss2:0.6571 loss3:0.0723 | AUC:0.8036 Anomaly AUC:0.6635
[2023-09-08 19:00:50,632][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0048 loss2:0.4074 loss3:0.0395 | AUC:0.8193 Anomaly AUC:0.6699
[2023-09-08 19:01:05,176][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0060 loss2:0.3049 loss3:0.0322 | AUC:0.8276 Anomaly AUC:0.6683
[2023-09-08 19:01:19,652][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0031 loss2:0.2547 loss3:0.0235 | AUC:0.8447 Anomaly AUC:0.6780
[2023-09-08 19:01:34,068][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0015 loss2:0.2284 loss3:0.0158 | AUC:0.8466 Anomaly AUC:0.6817
[2023-09-08 19:01:48,582][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0013 loss2:0.2136 loss3:0.0129 | AUC:0.8458 Anomaly AUC:0.6800
[2023-09-08 19:02:03,075][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0016 loss2:0.2055 loss3:0.0120 | AUC:0.8391 Anomaly AUC:0.6703
[2023-09-08 19:02:17,527][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0054 loss2:0.2311 loss3:0.0198 | AUC:0.8426 Anomaly AUC:0.6762
[2023-09-08 19:02:31,995][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0012 loss2:0.1952 loss3:0.0121 | AUC:0.8562 Anomaly AUC:0.6945
[2023-09-08 19:02:46,490][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0010 loss2:0.1839 loss3:0.0097 | AUC:0.8490 Anomaly AUC:0.6818
[2023-09-08 19:03:00,975][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0007 loss2:0.1754 loss3:0.0082 | AUC:0.8458 Anomaly AUC:0.6758
[2023-09-08 19:03:15,474][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0503 loss2:0.3453 loss3:0.0642 | AUC:0.8176 Anomaly AUC:0.6629
[2023-09-08 19:03:30,115][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0051 loss2:0.2901 loss3:0.0350 | AUC:0.8301 Anomaly AUC:0.6656
[2023-09-08 19:03:44,645][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0022 loss2:0.1847 loss3:0.0157 | AUC:0.8363 Anomaly AUC:0.6673
[2023-09-08 19:03:59,120][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0028 loss2:0.1741 loss3:0.0127 | AUC:0.8282 Anomaly AUC:0.6597
[2023-09-08 19:04:13,649][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0025 loss2:0.1732 loss3:0.0122 | AUC:0.8380 Anomaly AUC:0.6675
[2023-09-08 19:04:28,153][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0012 loss2:0.1583 loss3:0.0082 | AUC:0.8383 Anomaly AUC:0.6683
[2023-09-08 19:04:42,722][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0008 loss2:0.1470 loss3:0.0058 | AUC:0.8374 Anomaly AUC:0.6667
[2023-09-08 19:04:57,280][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0011 loss2:0.1440 loss3:0.0052 | AUC:0.8343 Anomaly AUC:0.6640
[2023-09-08 19:05:11,808][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0027 loss2:0.1556 loss3:0.0070 | AUC:0.8369 Anomaly AUC:0.6660
[2023-09-08 19:05:26,344][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0009 loss2:0.1361 loss3:0.0046 | AUC:0.8402 Anomaly AUC:0.6665
[2023-09-08 19:05:40,881][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0005 loss2:0.1277 loss3:0.0024 | AUC:0.8391 Anomaly AUC:0.6653
[2023-09-08 19:05:55,427][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0197 loss2:0.2794 loss3:0.0349 | AUC:0.8305 Anomaly AUC:0.6530
[2023-09-08 19:06:09,946][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0013 loss2:0.1413 loss3:0.0072 | AUC:0.8394 Anomaly AUC:0.6566
[2023-09-08 19:06:24,475][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0011 loss2:0.1278 loss3:0.0047 | AUC:0.8365 Anomaly AUC:0.6555
[2023-09-08 19:06:39,031][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0009 loss2:0.1212 loss3:0.0039 | AUC:0.8408 Anomaly AUC:0.6617
[2023-09-08 19:06:53,646][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0101 loss2:0.1644 loss3:0.0117 | AUC:0.8317 Anomaly AUC:0.6653
[2023-09-08 19:07:08,222][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0015 loss2:0.1352 loss3:0.0081 | AUC:0.8416 Anomaly AUC:0.6632
[2023-09-08 19:07:22,777][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0013 loss2:0.1114 loss3:0.0047 | AUC:0.8383 Anomaly AUC:0.6646
[2023-09-08 19:07:37,322][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0004 loss2:0.1002 loss3:0.0020 | AUC:0.8431 Anomaly AUC:0.6692
[2023-09-08 19:07:51,903][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0003 loss2:0.0957 loss3:0.0016 | AUC:0.8409 Anomaly AUC:0.6679
[2023-09-08 19:08:06,474][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0004 loss2:0.0909 loss3:0.0014 | AUC:0.8395 Anomaly AUC:0.6650
[2023-09-08 19:08:21,014][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0003 loss2:0.0868 loss3:0.0013 | AUC:0.8405 Anomaly AUC:0.6637
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 52, in train_func
    loss.backward()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt