
8000 8100 2900
[2023-09-08 19:46:12,523][main.py][line:165][INFO] total params:7.0092M
[2023-09-08 19:46:12,524][main.py][line:168][INFO] Training Mode
[2023-09-08 19:46:12,524][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 19:46:12,524][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 19:46:17,993][main.py][line:82][INFO] Random initialize AUCAUC:0.4547 Anomaly AUC:0.50867
[2023-09-08 19:46:32,236][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3141 loss2:1.1729 loss3:0.3407 | AUC:0.8027 Anomaly AUC:0.6632
[2023-09-08 19:46:46,219][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0383 loss2:0.8388 loss3:0.2508 | AUC:0.7913 Anomaly AUC:0.6674
[2023-09-08 19:47:00,165][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0220 loss2:0.7371 loss3:0.1907 | AUC:0.8041 Anomaly AUC:0.6621
[2023-09-08 19:47:14,153][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0219 loss2:0.6520 loss3:0.1353 | AUC:0.8119 Anomaly AUC:0.6677
[2023-09-08 19:47:28,438][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0156 loss2:0.5796 loss3:0.0995 | AUC:0.8276 Anomaly AUC:0.6672
[2023-09-08 19:47:42,793][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0128 loss2:0.5223 loss3:0.0767 | AUC:0.8373 Anomaly AUC:0.6683
[2023-09-08 19:47:57,243][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0106 loss2:0.4686 loss3:0.0625 | AUC:0.8374 Anomaly AUC:0.6737
[2023-09-08 19:48:11,723][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0146 loss2:0.4505 loss3:0.0575 | AUC:0.7921 Anomaly AUC:0.6749
[2023-09-08 19:48:26,317][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0053 loss2:0.4033 loss3:0.0457 | AUC:0.8319 Anomaly AUC:0.6698
[2023-09-08 19:48:40,857][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0044 loss2:0.3480 loss3:0.0299 | AUC:0.8361 Anomaly AUC:0.6698
[2023-09-08 19:48:55,377][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0121 loss2:0.4212 loss3:0.0521 | AUC:0.8252 Anomaly AUC:0.6603
[2023-09-08 19:49:09,963][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0049 loss2:0.3330 loss3:0.0301 | AUC:0.8366 Anomaly AUC:0.6607
[2023-09-08 19:49:24,591][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0048 loss2:0.3157 loss3:0.0274 | AUC:0.8359 Anomaly AUC:0.6719
[2023-09-08 19:49:39,112][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0026 loss2:0.2729 loss3:0.0193 | AUC:0.8404 Anomaly AUC:0.6711
[2023-09-08 19:49:53,714][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0022 loss2:0.2562 loss3:0.0167 | AUC:0.8406 Anomaly AUC:0.6705
[2023-09-08 19:50:08,330][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0017 loss2:0.2409 loss3:0.0141 | AUC:0.8361 Anomaly AUC:0.6667
[2023-09-08 19:50:22,809][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0081 loss2:0.2852 loss3:0.0265 | AUC:0.8336 Anomaly AUC:0.6627
[2023-09-08 19:50:37,224][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0017 loss2:0.2284 loss3:0.0163 | AUC:0.8363 Anomaly AUC:0.6630
[2023-09-08 19:50:51,795][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0014 loss2:0.2130 loss3:0.0134 | AUC:0.8405 Anomaly AUC:0.6700
[2023-09-08 19:51:06,313][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.1298 loss2:0.3996 loss3:0.0934 | AUC:0.7134 Anomaly AUC:0.5683
[2023-09-08 19:51:20,868][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0094 loss2:0.6174 loss3:0.0787 | AUC:0.8167 Anomaly AUC:0.6667
[2023-09-08 19:51:35,456][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0048 loss2:0.3455 loss3:0.0341 | AUC:0.8361 Anomaly AUC:0.6782
[2023-09-08 19:51:50,077][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0032 loss2:0.2799 loss3:0.0220 | AUC:0.8379 Anomaly AUC:0.6761
[2023-09-08 19:52:04,690][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0023 loss2:0.2501 loss3:0.0169 | AUC:0.8385 Anomaly AUC:0.6739
[2023-09-08 19:52:19,291][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0018 loss2:0.2346 loss3:0.0142 | AUC:0.8383 Anomaly AUC:0.6697
[2023-09-08 19:52:33,909][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0019 loss2:0.2252 loss3:0.0129 | AUC:0.8363 Anomaly AUC:0.6686
[2023-09-08 19:52:48,559][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0014 loss2:0.2129 loss3:0.0117 | AUC:0.8337 Anomaly AUC:0.6608
[2023-09-08 19:53:03,166][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0086 loss2:0.2811 loss3:0.0282 | AUC:0.8345 Anomaly AUC:0.6635
[2023-09-08 19:53:17,759][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0019 loss2:0.2104 loss3:0.0139 | AUC:0.8361 Anomaly AUC:0.6685
[2023-09-08 19:53:32,430][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0012 loss2:0.1950 loss3:0.0112 | AUC:0.8348 Anomaly AUC:0.6613
[2023-09-08 19:53:47,086][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0013 loss2:0.1935 loss3:0.0109 | AUC:0.8380 Anomaly AUC:0.6668
[2023-09-08 19:54:01,759][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0010 loss2:0.1788 loss3:0.0101 | AUC:0.8330 Anomaly AUC:0.6649
[2023-09-08 19:54:16,370][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0008 loss2:0.1694 loss3:0.0093 | AUC:0.8333 Anomaly AUC:0.6634
[2023-09-08 19:54:31,003][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0167 loss2:0.4058 loss3:0.0448 | AUC:0.8326 Anomaly AUC:0.6675
[2023-09-08 19:54:45,645][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0041 loss2:0.2108 loss3:0.0220 | AUC:0.8339 Anomaly AUC:0.6660
[2023-09-08 19:55:00,307][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0018 loss2:0.1800 loss3:0.0143 | AUC:0.8351 Anomaly AUC:0.6601
[2023-09-08 19:55:14,988][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0052 loss2:0.2111 loss3:0.0221 | AUC:0.8313 Anomaly AUC:0.6627
[2023-09-08 19:55:29,624][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0015 loss2:0.1598 loss3:0.0123 | AUC:0.8348 Anomaly AUC:0.6662
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 36, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 39, in forward
    x_k_split = self.DR_DMU(x_split)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 64, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 77, in forward
    x = ff(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1494, in _call_impl
    def _call_impl(self, *args, **kwargs):
KeyboardInterrupt