
8000 8100 2900
[2023-09-08 19:56:36,826][main.py][line:165][INFO] total params:7.5349M
[2023-09-08 19:56:36,826][main.py][line:168][INFO] Training Mode
[2023-09-08 19:56:36,827][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 19:56:36,827][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 19:56:44,938][main.py][line:82][INFO] Random initialize AUCAUC:0.6086 Anomaly AUC:0.60989
[2023-09-08 19:57:03,141][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:0.5450 loss2:1.2442 loss3:0.2747 | AUC:0.7911 Anomaly AUC:0.6417
[2023-09-08 19:57:21,198][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:0.2978 loss2:0.9230 loss3:0.0732 | AUC:0.8174 Anomaly AUC:0.6572
[2023-09-08 19:57:39,535][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:0.2086 loss2:0.7948 loss3:0.0432 | AUC:0.8281 Anomaly AUC:0.6719
[2023-09-08 19:57:57,924][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:0.1382 loss2:0.7038 loss3:0.0291 | AUC:0.8350 Anomaly AUC:0.6632
[2023-09-08 19:58:16,611][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:0.0883 loss2:0.6191 loss3:0.0209 | AUC:0.8098 Anomaly AUC:0.6667
[2023-09-08 19:58:35,239][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:0.0710 loss2:0.5402 loss3:0.0167 | AUC:0.8275 Anomaly AUC:0.6663
[2023-09-08 19:58:53,849][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00050 | loss1:0.0354 loss2:0.4481 loss3:0.0098 | AUC:0.8277 Anomaly AUC:0.6618
[2023-09-08 19:59:12,618][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00050 | loss1:0.0279 loss2:0.3596 loss3:0.0067 | AUC:0.8118 Anomaly AUC:0.6322
[2023-09-08 19:59:31,386][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00050 | loss1:0.0168 loss2:0.2751 loss3:0.0039 | AUC:0.8102 Anomaly AUC:0.6553
[2023-09-08 19:59:50,090][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00050 | loss1:0.0228 loss2:0.2239 loss3:0.0044 | AUC:0.8350 Anomaly AUC:0.6470
[2023-09-08 20:00:08,757][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00050 | loss1:0.0107 loss2:0.1370 loss3:0.0020 | AUC:0.8340 Anomaly AUC:0.6435
[2023-09-08 20:00:27,454][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00050 | loss1:0.0225 loss2:0.1312 loss3:0.0044 | AUC:0.8346 Anomaly AUC:0.6499
[2023-09-08 20:00:46,227][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00050 | loss1:0.0168 loss2:0.0834 loss3:0.0029 | AUC:0.8433 Anomaly AUC:0.6621
[2023-09-08 20:01:04,956][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00050 | loss1:0.0081 loss2:0.0616 loss3:0.0018 | AUC:0.8420 Anomaly AUC:0.6574
[2023-09-08 20:01:23,695][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00050 | loss1:0.0114 loss2:0.0497 loss3:0.0019 | AUC:0.8318 Anomaly AUC:0.6544
[2023-09-08 20:01:42,599][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00050 | loss1:0.0005 loss2:0.0233 loss3:0.0004 | AUC:0.8546 Anomaly AUC:0.6734
[2023-09-08 20:02:01,319][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00050 | loss1:0.0002 loss2:0.0158 loss3:0.0003 | AUC:0.8497 Anomaly AUC:0.6672
[2023-09-08 20:02:20,174][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00050 | loss1:0.0002 loss2:0.0125 loss3:0.0002 | AUC:0.8474 Anomaly AUC:0.6593
[2023-09-08 20:02:38,964][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00050 | loss1:0.0002 loss2:0.0108 loss3:0.0003 | AUC:0.8470 Anomaly AUC:0.6621
[2023-09-08 20:02:57,901][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00050 | loss1:0.0001 loss2:0.0096 loss3:0.0003 | AUC:0.8464 Anomaly AUC:0.6606
[2023-09-08 20:03:16,689][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00050 | loss1:0.0001 loss2:0.0076 loss3:0.0003 | AUC:0.8500 Anomaly AUC:0.6683
[2023-09-08 20:03:35,514][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00050 | loss1:0.0001 loss2:0.0070 loss3:0.0003 | AUC:0.8482 Anomaly AUC:0.6633
[2023-09-08 20:03:54,320][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00050 | loss1:0.0001 loss2:0.0060 loss3:0.0003 | AUC:0.8488 Anomaly AUC:0.6653
[2023-09-08 20:04:13,068][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00050 | loss1:0.0001 loss2:0.0052 loss3:0.0003 | AUC:0.8503 Anomaly AUC:0.6686
[2023-09-08 20:04:31,953][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00050 | loss1:0.0001 loss2:0.0047 loss3:0.0003 | AUC:0.8502 Anomaly AUC:0.6685
[2023-09-08 20:04:50,772][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00050 | loss1:0.0001 loss2:0.0045 loss3:0.0003 | AUC:0.8513 Anomaly AUC:0.6854
[2023-09-08 20:05:09,732][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00050 | loss1:0.0001 loss2:0.0042 loss3:0.0003 | AUC:0.8455 Anomaly AUC:0.6596
[2023-09-08 20:05:28,499][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00050 | loss1:0.0001 loss2:0.0037 loss3:0.0003 | AUC:0.8517 Anomaly AUC:0.6782
[2023-09-08 20:05:47,498][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00050 | loss1:0.0001 loss2:0.0035 loss3:0.0003 | AUC:0.8478 Anomaly AUC:0.6693
[2023-09-08 20:06:06,300][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00050 | loss1:0.0001 loss2:0.0071 loss3:0.0004 | AUC:0.8073 Anomaly AUC:0.6596
[2023-09-08 20:06:25,093][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00050 | loss1:0.0920 loss2:0.2114 loss3:0.0176 | AUC:0.8439 Anomaly AUC:0.6695
[2023-09-08 20:06:43,994][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00050 | loss1:0.0168 loss2:0.0562 loss3:0.0033 | AUC:0.8168 Anomaly AUC:0.6596
[2023-09-08 20:07:02,757][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00050 | loss1:0.0154 loss2:0.0337 loss3:0.0029 | AUC:0.8238 Anomaly AUC:0.6646
[2023-09-08 20:07:21,609][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00050 | loss1:0.0109 loss2:0.0244 loss3:0.0022 | AUC:0.8233 Anomaly AUC:0.6726
[2023-09-08 20:07:40,499][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00050 | loss1:0.0096 loss2:0.0225 loss3:0.0018 | AUC:0.8442 Anomaly AUC:0.6686
[2023-09-08 20:07:59,322][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00050 | loss1:0.0097 loss2:0.0170 loss3:0.0019 | AUC:0.8320 Anomaly AUC:0.6647
[2023-09-08 20:08:18,155][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00050 | loss1:0.0031 loss2:0.0110 loss3:0.0009 | AUC:0.8358 Anomaly AUC:0.6806
[2023-09-08 20:08:37,046][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00050 | loss1:0.0021 loss2:0.0122 loss3:0.0007 | AUC:0.8285 Anomaly AUC:0.6719
[2023-09-08 20:08:55,863][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00050 | loss1:0.0089 loss2:0.0136 loss3:0.0019 | AUC:0.8279 Anomaly AUC:0.6783
[2023-09-08 20:09:14,655][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00050 | loss1:0.0085 loss2:0.0169 loss3:0.0019 | AUC:0.8306 Anomaly AUC:0.6564
[2023-09-08 20:09:33,503][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00050 | loss1:0.0164 loss2:0.0305 loss3:0.0028 | AUC:0.7788 Anomaly AUC:0.6641
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 36, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 39, in forward
    x_k_split = self.DR_DMU(x_split)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 64, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 76, in forward
    x = attn(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 14, in forward
    return self.fn(self.norm(x), **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 55, in forward
    tmp_ones = torch.ones(n).cuda()
KeyboardInterrupt