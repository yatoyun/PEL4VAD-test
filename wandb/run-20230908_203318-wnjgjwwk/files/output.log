
8000 8100 2900
[2023-09-08 20:33:20,734][main.py][line:165][INFO] total params:17.0166M
[2023-09-08 20:33:20,735][main.py][line:168][INFO] Training Mode
[2023-09-08 20:33:20,735][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=1024, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=1024, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=1024, out_features=1024, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=1024, out_features=1024, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 20:33:20,735][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 20:33:28,488][main.py][line:82][INFO] Random initialize AUCAUC:0.5283 Anomaly AUC:0.51616
[2023-09-08 20:33:50,341][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:0.0580 loss2:1.0715 loss3:0.1326 | AUC:0.7202 Anomaly AUC:0.6563
[2023-09-08 20:34:11,881][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:0.0068 loss2:0.8126 loss3:0.0576 | AUC:0.7708 Anomaly AUC:0.6662
[2023-09-08 20:34:33,788][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:0.0033 loss2:0.7263 loss3:0.0474 | AUC:0.7314 Anomaly AUC:0.6634
[2023-09-08 20:34:55,812][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:0.0026 loss2:0.6533 loss3:0.0401 | AUC:0.7380 Anomaly AUC:0.6664
[2023-09-08 20:35:17,853][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:0.0026 loss2:0.5435 loss3:0.0340 | AUC:0.7277 Anomaly AUC:0.6522
[2023-09-08 20:35:40,038][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:0.0026 loss2:0.4348 loss3:0.0326 | AUC:0.7893 Anomaly AUC:0.6759
[2023-09-08 20:36:02,174][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00050 | loss1:0.0041 loss2:0.3720 loss3:0.0381 | AUC:0.7293 Anomaly AUC:0.6778
[2023-09-08 20:36:24,421][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00050 | loss1:0.0024 loss2:0.3002 loss3:0.0320 | AUC:0.7415 Anomaly AUC:0.6627
[2023-09-08 20:36:46,707][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00050 | loss1:0.0013 loss2:0.2201 loss3:0.0219 | AUC:0.7344 Anomaly AUC:0.6602
[2023-09-08 20:37:08,913][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00050 | loss1:0.0012 loss2:0.1722 loss3:0.0207 | AUC:0.7552 Anomaly AUC:0.6524
[2023-09-08 20:37:31,183][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00050 | loss1:0.0013 loss2:0.1384 loss3:0.0188 | AUC:0.7451 Anomaly AUC:0.6437
[2023-09-08 20:37:53,450][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00050 | loss1:0.0007 loss2:0.0993 loss3:0.0163 | AUC:0.7379 Anomaly AUC:0.6276
[2023-09-08 20:38:15,761][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00050 | loss1:0.0011 loss2:0.0887 loss3:0.0178 | AUC:0.7642 Anomaly AUC:0.6525
[2023-09-08 20:38:38,153][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00050 | loss1:0.0011 loss2:0.0710 loss3:0.0176 | AUC:0.7689 Anomaly AUC:0.6543
[2023-09-08 20:39:00,405][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00050 | loss1:0.0010 loss2:0.0752 loss3:0.0165 | AUC:0.7650 Anomaly AUC:0.6555
[2023-09-08 20:39:22,688][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00050 | loss1:0.0008 loss2:0.0568 loss3:0.0161 | AUC:0.7810 Anomaly AUC:0.6737
[2023-09-08 20:39:45,091][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00050 | loss1:0.0006 loss2:0.0379 loss3:0.0146 | AUC:0.7674 Anomaly AUC:0.6539
[2023-09-08 20:40:07,444][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00050 | loss1:0.0004 loss2:0.0305 loss3:0.0130 | AUC:0.7635 Anomaly AUC:0.6572
[2023-09-08 20:40:29,683][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00050 | loss1:0.0005 loss2:0.0263 loss3:0.0134 | AUC:0.7634 Anomaly AUC:0.6477
[2023-09-08 20:40:51,984][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00050 | loss1:0.0003 loss2:0.0185 loss3:0.0120 | AUC:0.7691 Anomaly AUC:0.6544
[2023-09-08 20:41:14,319][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00050 | loss1:0.0031 loss2:0.0467 loss3:0.0184 | AUC:0.7741 Anomaly AUC:0.6619
[2023-09-08 20:41:36,677][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00050 | loss1:0.0016 loss2:0.0415 loss3:0.0211 | AUC:0.7053 Anomaly AUC:0.6439
[2023-09-08 20:41:59,012][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00050 | loss1:0.0013 loss2:0.0351 loss3:0.0195 | AUC:0.6557 Anomaly AUC:0.6560
[2023-09-08 20:42:21,315][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00050 | loss1:0.0047 loss2:0.0611 loss3:0.0337 | AUC:0.7483 Anomaly AUC:0.6892
[2023-09-08 20:42:43,611][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00050 | loss1:0.0008 loss2:0.0165 loss3:0.0164 | AUC:0.7842 Anomaly AUC:0.6923
[2023-09-08 20:43:06,044][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00050 | loss1:0.0003 loss2:0.0087 loss3:0.0132 | AUC:0.7443 Anomaly AUC:0.6694
[2023-09-08 20:43:28,540][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00050 | loss1:0.0002 loss2:0.0067 loss3:0.0119 | AUC:0.7817 Anomaly AUC:0.6759
[2023-09-08 20:43:51,058][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00050 | loss1:0.0002 loss2:0.0059 loss3:0.0113 | AUC:0.7719 Anomaly AUC:0.6658
[2023-09-08 20:44:13,423][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00050 | loss1:0.0002 loss2:0.0045 loss3:0.0110 | AUC:0.7707 Anomaly AUC:0.6662
[2023-09-08 20:44:35,711][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00050 | loss1:0.0002 loss2:0.0041 loss3:0.0109 | AUC:0.7626 Anomaly AUC:0.6593
[2023-09-08 20:44:58,029][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00050 | loss1:0.0004 loss2:0.0071 loss3:0.0119 | AUC:0.7739 Anomaly AUC:0.6666
[2023-09-08 20:45:20,407][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00050 | loss1:0.0011 loss2:0.0502 loss3:0.0202 | AUC:0.7666 Anomaly AUC:0.6652
[2023-09-08 20:45:42,619][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00050 | loss1:0.0038 loss2:0.0416 loss3:0.0252 | AUC:0.7427 Anomaly AUC:0.6667
[2023-09-08 20:46:05,008][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00050 | loss1:0.0006 loss2:0.0171 loss3:0.0174 | AUC:0.7287 Anomaly AUC:0.6694
[2023-09-08 20:46:27,457][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00050 | loss1:0.0002 loss2:0.0054 loss3:0.0130 | AUC:0.7321 Anomaly AUC:0.6512
[2023-09-08 20:46:49,684][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00050 | loss1:0.0003 loss2:0.0044 loss3:0.0127 | AUC:0.7281 Anomaly AUC:0.6416
[2023-09-08 20:47:12,057][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00050 | loss1:0.0056 loss2:0.0302 loss3:0.0218 | AUC:0.7441 Anomaly AUC:0.6491
[2023-09-08 20:47:34,538][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00050 | loss1:0.0007 loss2:0.0224 loss3:0.0178 | AUC:0.7423 Anomaly AUC:0.6630
[2023-09-08 20:47:56,878][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00050 | loss1:0.0006 loss2:0.0156 loss3:0.0164 | AUC:0.7471 Anomaly AUC:0.6532
[2023-09-08 20:48:19,255][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00050 | loss1:0.0004 loss2:0.0140 loss3:0.0151 | AUC:0.7091 Anomaly AUC:0.6602
[2023-09-08 20:48:41,721][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00050 | loss1:0.0004 loss2:0.0134 loss3:0.0149 | AUC:0.6988 Anomaly AUC:0.6614
[2023-09-08 20:49:04,144][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00050 | loss1:0.0012 loss2:0.0064 loss3:0.0130 | AUC:0.7375 Anomaly AUC:0.6765
[2023-09-08 20:49:26,522][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00050 | loss1:0.0003 loss2:0.0030 loss3:0.0130 | AUC:0.7078 Anomaly AUC:0.6642
[2023-09-08 20:49:48,899][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00050 | loss1:0.0001 loss2:0.0017 loss3:0.0111 | AUC:0.7023 Anomaly AUC:0.6590
[2023-09-08 20:50:11,285][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00050 | loss1:0.0001 loss2:0.0020 loss3:0.0110 | AUC:0.6777 Anomaly AUC:0.6521
[2023-09-08 20:50:33,788][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00050 | loss1:0.0004 loss2:0.0044 loss3:0.0121 | AUC:0.6660 Anomaly AUC:0.6362
[2023-09-08 20:50:56,368][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00050 | loss1:0.0006 loss2:0.0304 loss3:0.0187 | AUC:0.7358 Anomaly AUC:0.6363
[2023-09-08 20:51:18,708][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00050 | loss1:0.0008 loss2:0.0283 loss3:0.0190 | AUC:0.7071 Anomaly AUC:0.6471
[2023-09-08 20:51:41,099][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00050 | loss1:0.0003 loss2:0.0124 loss3:0.0147 | AUC:0.7391 Anomaly AUC:0.6377
[2023-09-08 20:52:03,592][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00050 | loss1:0.0002 loss2:0.0122 loss3:0.0139 | AUC:0.7014 Anomaly AUC:0.6370
[2023-09-08 20:52:26,105][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00050 | loss1:0.0004 loss2:0.0154 loss3:0.0154 | AUC:0.7456 Anomaly AUC:0.6433
[2023-09-08 20:52:48,682][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00050 | loss1:0.0002 loss2:0.0077 loss3:0.0127 | AUC:0.7604 Anomaly AUC:0.6641
[2023-09-08 20:53:11,070][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00050 | loss1:0.0002 loss2:0.0055 loss3:0.0127 | AUC:0.7430 Anomaly AUC:0.6422
[2023-09-08 20:53:33,626][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00050 | loss1:0.0005 loss2:0.0065 loss3:0.0133 | AUC:0.7415 Anomaly AUC:0.6594
[2023-09-08 20:53:56,136][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00050 | loss1:0.0003 loss2:0.0112 loss3:0.0140 | AUC:0.7520 Anomaly AUC:0.6575
[2023-09-08 20:54:18,696][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00050 | loss1:0.0002 loss2:0.0134 loss3:0.0140 | AUC:0.7320 Anomaly AUC:0.6575
[2023-09-08 20:54:41,093][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00050 | loss1:0.0007 loss2:0.0110 loss3:0.0152 | AUC:0.7659 Anomaly AUC:0.6707
[2023-09-08 20:55:03,621][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00050 | loss1:0.0003 loss2:0.0126 loss3:0.0186 | AUC:0.7482 Anomaly AUC:0.6588
[2023-09-08 20:55:26,172][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00050 | loss1:0.0001 loss2:0.0059 loss3:0.0133 | AUC:0.7383 Anomaly AUC:0.6645
[2023-09-08 20:55:48,579][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00050 | loss1:0.0005 loss2:0.0122 loss3:0.0152 | AUC:0.7098 Anomaly AUC:0.6471
[2023-09-08 20:56:10,928][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00050 | loss1:0.0008 loss2:0.0147 loss3:0.0180 | AUC:0.7148 Anomaly AUC:0.6820
[2023-09-08 20:56:33,480][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00050 | loss1:0.0002 loss2:0.0206 loss3:0.0163 | AUC:0.7511 Anomaly AUC:0.6588
[2023-09-08 20:56:55,988][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00050 | loss1:0.0002 loss2:0.0091 loss3:0.0133 | AUC:0.7228 Anomaly AUC:0.6504
[2023-09-08 20:57:18,403][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00050 | loss1:0.0001 loss2:0.0040 loss3:0.0125 | AUC:0.7385 Anomaly AUC:0.6585
[2023-09-08 20:57:41,216][main.py][line:106][INFO] [Epoch:65/100]: lr:0.00050 | loss1:0.0003 loss2:0.0044 loss3:0.0128 | AUC:0.7225 Anomaly AUC:0.6689
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 25, in train_func
    v_input = v_input.float().cuda(non_blocking=True)
KeyboardInterrupt