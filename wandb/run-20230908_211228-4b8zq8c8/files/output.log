
8000 8100 2900
[2023-09-08 21:12:30,791][main.py][line:165][INFO] total params:7.5247M
[2023-09-08 21:12:30,791][main.py][line:168][INFO] Training Mode
[2023-09-08 21:12:30,791][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 21:12:30,792][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 21:12:39,005][main.py][line:82][INFO] Random initialize AUCAUC:0.4433 Anomaly AUC:0.54786
[2023-09-08 21:12:57,412][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:0.5615 loss2:1.2079 loss3:0.2455 | AUC:0.7968 Anomaly AUC:0.5882
[2023-09-08 21:13:15,551][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:0.2868 loss2:0.9217 loss3:0.0672 | AUC:0.7977 Anomaly AUC:0.5915
[2023-09-08 21:13:34,028][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:0.2188 loss2:0.8124 loss3:0.0386 | AUC:0.8113 Anomaly AUC:0.5970
[2023-09-08 21:13:52,711][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:0.1726 loss2:0.7183 loss3:0.0287 | AUC:0.8013 Anomaly AUC:0.5803
[2023-09-08 21:14:11,446][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:0.1267 loss2:0.6461 loss3:0.0231 | AUC:0.7901 Anomaly AUC:0.5734
[2023-09-08 21:14:30,303][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:0.0803 loss2:0.5565 loss3:0.0154 | AUC:0.8072 Anomaly AUC:0.5509
[2023-09-08 21:14:49,012][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00050 | loss1:0.0503 loss2:0.4699 loss3:0.0102 | AUC:0.8017 Anomaly AUC:0.5496
[2023-09-08 21:15:07,815][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00050 | loss1:0.0380 loss2:0.3844 loss3:0.0066 | AUC:0.7932 Anomaly AUC:0.5316
[2023-09-08 21:15:26,712][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00050 | loss1:0.0330 loss2:0.3080 loss3:0.0053 | AUC:0.7716 Anomaly AUC:0.5247
[2023-09-08 21:15:45,567][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00050 | loss1:0.0165 loss2:0.2256 loss3:0.0033 | AUC:0.8059 Anomaly AUC:0.5563
[2023-09-08 21:16:04,435][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00050 | loss1:0.0204 loss2:0.1736 loss3:0.0036 | AUC:0.7806 Anomaly AUC:0.5205
[2023-09-08 21:16:23,285][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00050 | loss1:0.0175 loss2:0.1366 loss3:0.0030 | AUC:0.7850 Anomaly AUC:0.5250
[2023-09-08 21:16:42,164][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00050 | loss1:0.0079 loss2:0.0820 loss3:0.0018 | AUC:0.7767 Anomaly AUC:0.5314
[2023-09-08 21:17:01,129][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00050 | loss1:0.0078 loss2:0.0606 loss3:0.0016 | AUC:0.7991 Anomaly AUC:0.5593
[2023-09-08 21:17:20,089][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00050 | loss1:0.0151 loss2:0.0627 loss3:0.0028 | AUC:0.7636 Anomaly AUC:0.5518
[2023-09-08 21:17:38,990][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00050 | loss1:0.0160 loss2:0.0601 loss3:0.0029 | AUC:0.7552 Anomaly AUC:0.5150
[2023-09-08 21:17:57,866][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00050 | loss1:0.0078 loss2:0.0354 loss3:0.0015 | AUC:0.7824 Anomaly AUC:0.5357
[2023-09-08 21:18:16,797][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00050 | loss1:0.0136 loss2:0.0420 loss3:0.0023 | AUC:0.7475 Anomaly AUC:0.5117
[2023-09-08 21:18:35,747][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00050 | loss1:0.0084 loss2:0.0280 loss3:0.0015 | AUC:0.7242 Anomaly AUC:0.5216
[2023-09-08 21:18:54,697][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00050 | loss1:0.0102 loss2:0.0321 loss3:0.0019 | AUC:0.7210 Anomaly AUC:0.4950
[2023-09-08 21:19:13,530][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00050 | loss1:0.0037 loss2:0.0183 loss3:0.0009 | AUC:0.7273 Anomaly AUC:0.5428
[2023-09-08 21:19:32,445][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00050 | loss1:0.0075 loss2:0.0278 loss3:0.0014 | AUC:0.7664 Anomaly AUC:0.5462
[2023-09-08 21:19:51,390][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00050 | loss1:0.0115 loss2:0.0298 loss3:0.0020 | AUC:0.7724 Anomaly AUC:0.5559
[2023-09-08 21:20:10,334][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00050 | loss1:0.0053 loss2:0.0217 loss3:0.0011 | AUC:0.7748 Anomaly AUC:0.5504
[2023-09-08 21:20:29,286][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00050 | loss1:0.0105 loss2:0.0261 loss3:0.0019 | AUC:0.7450 Anomaly AUC:0.5167
[2023-09-08 21:20:48,227][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00050 | loss1:0.0063 loss2:0.0199 loss3:0.0012 | AUC:0.7457 Anomaly AUC:0.5239
[2023-09-08 21:21:07,159][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00050 | loss1:0.0066 loss2:0.0161 loss3:0.0013 | AUC:0.7713 Anomaly AUC:0.5428
[2023-09-08 21:21:26,159][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00050 | loss1:0.0087 loss2:0.0199 loss3:0.0015 | AUC:0.7507 Anomaly AUC:0.5203
[2023-09-08 21:21:45,056][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00050 | loss1:0.0094 loss2:0.0177 loss3:0.0014 | AUC:0.7529 Anomaly AUC:0.4958
[2023-09-08 21:22:04,025][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00050 | loss1:0.0133 loss2:0.0267 loss3:0.0024 | AUC:0.7773 Anomaly AUC:0.5679
[2023-09-08 21:22:22,913][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00050 | loss1:0.0088 loss2:0.0192 loss3:0.0016 | AUC:0.7113 Anomaly AUC:0.5236
[2023-09-08 21:22:41,813][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00050 | loss1:0.0055 loss2:0.0146 loss3:0.0011 | AUC:0.7139 Anomaly AUC:0.5048
[2023-09-08 21:23:00,670][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00050 | loss1:0.0021 loss2:0.0060 loss3:0.0006 | AUC:0.7075 Anomaly AUC:0.5275
[2023-09-08 21:23:19,645][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00050 | loss1:0.0027 loss2:0.0049 loss3:0.0006 | AUC:0.7153 Anomaly AUC:0.4906
[2023-09-08 21:23:38,626][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00050 | loss1:0.0002 loss2:0.0024 loss3:0.0003 | AUC:0.7362 Anomaly AUC:0.5053
[2023-09-08 21:23:57,569][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00050 | loss1:0.0000 loss2:0.0020 loss3:0.0002 | AUC:0.7508 Anomaly AUC:0.5107
[2023-09-08 21:24:16,477][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00050 | loss1:0.0000 loss2:0.0018 loss3:0.0002 | AUC:0.7689 Anomaly AUC:0.5223
[2023-09-08 21:24:35,486][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00050 | loss1:0.0000 loss2:0.0017 loss3:0.0002 | AUC:0.7758 Anomaly AUC:0.5270
[2023-09-08 21:24:54,608][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00050 | loss1:0.0000 loss2:0.0015 loss3:0.0002 | AUC:0.7845 Anomaly AUC:0.5382
[2023-09-08 21:25:13,648][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00050 | loss1:0.0000 loss2:0.0017 loss3:0.0002 | AUC:0.7887 Anomaly AUC:0.5441
[2023-09-08 21:25:32,682][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00050 | loss1:0.0000 loss2:0.0016 loss3:0.0002 | AUC:0.7937 Anomaly AUC:0.5505
[2023-09-08 21:25:51,816][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00050 | loss1:0.0000 loss2:0.0016 loss3:0.0002 | AUC:0.7966 Anomaly AUC:0.5575
[2023-09-08 21:26:10,833][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00050 | loss1:0.0000 loss2:0.0014 loss3:0.0002 | AUC:0.8012 Anomaly AUC:0.5649
[2023-09-08 21:26:29,842][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00050 | loss1:0.0000 loss2:0.0015 loss3:0.0002 | AUC:0.8034 Anomaly AUC:0.5659
[2023-09-08 21:26:48,833][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00050 | loss1:0.0000 loss2:0.0014 loss3:0.0002 | AUC:0.8049 Anomaly AUC:0.5618
[2023-09-08 21:27:07,916][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00050 | loss1:0.0000 loss2:0.0015 loss3:0.0002 | AUC:0.8072 Anomaly AUC:0.5638
[2023-09-08 21:27:26,976][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00050 | loss1:0.0000 loss2:0.0013 loss3:0.0002 | AUC:0.8078 Anomaly AUC:0.5644
[2023-09-08 21:27:46,135][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00050 | loss1:0.0000 loss2:0.0013 loss3:0.0002 | AUC:0.8069 Anomaly AUC:0.5615
[2023-09-08 21:28:05,296][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00050 | loss1:0.0000 loss2:0.0013 loss3:0.0002 | AUC:0.8110 Anomaly AUC:0.5661
[2023-09-08 21:28:24,346][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00050 | loss1:0.0000 loss2:0.0012 loss3:0.0002 | AUC:0.8116 Anomaly AUC:0.5658
[2023-09-08 21:28:43,465][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00050 | loss1:0.0000 loss2:0.0015 loss3:0.0003 | AUC:0.8129 Anomaly AUC:0.5711
[2023-09-08 21:29:02,537][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00050 | loss1:0.0449 loss2:0.1110 loss3:0.0081 | AUC:0.7526 Anomaly AUC:0.5289
[2023-09-08 21:29:21,760][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00050 | loss1:0.0294 loss2:0.0470 loss3:0.0046 | AUC:0.7681 Anomaly AUC:0.5581
[2023-09-08 21:29:40,916][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00050 | loss1:0.0111 loss2:0.0201 loss3:0.0019 | AUC:0.8130 Anomaly AUC:0.5686
[2023-09-08 21:29:59,900][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00050 | loss1:0.0079 loss2:0.0127 loss3:0.0013 | AUC:0.7636 Anomaly AUC:0.5255
[2023-09-08 21:30:18,923][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00050 | loss1:0.0140 loss2:0.0143 loss3:0.0022 | AUC:0.7996 Anomaly AUC:0.5733
[2023-09-08 21:30:37,901][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00050 | loss1:0.0032 loss2:0.0067 loss3:0.0007 | AUC:0.7857 Anomaly AUC:0.5413
[2023-09-08 21:30:56,948][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00050 | loss1:0.0026 loss2:0.0039 loss3:0.0005 | AUC:0.7839 Anomaly AUC:0.5491
[2023-09-08 21:31:15,910][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00050 | loss1:0.0010 loss2:0.0019 loss3:0.0004 | AUC:0.7935 Anomaly AUC:0.5602
[2023-09-08 21:31:34,875][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00050 | loss1:0.0010 loss2:0.0029 loss3:0.0004 | AUC:0.7986 Anomaly AUC:0.5818
[2023-09-08 21:31:54,022][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00050 | loss1:0.0126 loss2:0.0141 loss3:0.0020 | AUC:0.7853 Anomaly AUC:0.5453
[2023-09-08 21:32:13,082][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00050 | loss1:0.0065 loss2:0.0200 loss3:0.0015 | AUC:0.7529 Anomaly AUC:0.5227
[2023-09-08 21:32:32,104][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00050 | loss1:0.0037 loss2:0.0087 loss3:0.0008 | AUC:0.7937 Anomaly AUC:0.5664
[2023-09-08 21:32:51,022][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00050 | loss1:0.0008 loss2:0.0037 loss3:0.0003 | AUC:0.7964 Anomaly AUC:0.5644
[2023-09-08 21:33:10,362][main.py][line:106][INFO] [Epoch:65/100]: lr:0.00050 | loss1:0.0000 loss2:0.0018 loss3:0.0002 | AUC:0.8003 Anomaly AUC:0.5691
[2023-09-08 21:33:29,474][main.py][line:106][INFO] [Epoch:66/100]: lr:0.00050 | loss1:0.0000 loss2:0.0011 loss3:0.0002 | AUC:0.8043 Anomaly AUC:0.5775
[2023-09-08 21:33:48,560][main.py][line:106][INFO] [Epoch:67/100]: lr:0.00050 | loss1:0.0000 loss2:0.0011 loss3:0.0002 | AUC:0.8058 Anomaly AUC:0.5802
[2023-09-08 21:34:07,636][main.py][line:106][INFO] [Epoch:68/100]: lr:0.00050 | loss1:0.0000 loss2:0.0010 loss3:0.0002 | AUC:0.8058 Anomaly AUC:0.5828
[2023-09-08 21:34:26,665][main.py][line:106][INFO] [Epoch:69/100]: lr:0.00050 | loss1:0.0000 loss2:0.0010 loss3:0.0002 | AUC:0.8026 Anomaly AUC:0.5790
[2023-09-08 21:34:45,733][main.py][line:106][INFO] [Epoch:70/100]: lr:0.00050 | loss1:0.0000 loss2:0.0009 loss3:0.0002 | AUC:0.7997 Anomaly AUC:0.5729
[2023-09-08 21:35:04,896][main.py][line:106][INFO] [Epoch:71/100]: lr:0.00050 | loss1:0.0000 loss2:0.0009 loss3:0.0002 | AUC:0.7970 Anomaly AUC:0.5701
[2023-09-08 21:35:24,077][main.py][line:106][INFO] [Epoch:72/100]: lr:0.00050 | loss1:0.0000 loss2:0.0011 loss3:0.0002 | AUC:0.7885 Anomaly AUC:0.5702
[2023-09-08 21:35:43,044][main.py][line:106][INFO] [Epoch:73/100]: lr:0.00050 | loss1:0.0000 loss2:0.0010 loss3:0.0002 | AUC:0.7822 Anomaly AUC:0.5705
Exception ignored in: <function _releaseLock at 0x7fc353511870>
Traceback (most recent call last):
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/logging/__init__.py", line 228, in _releaseLock
    def _releaseLock():
KeyboardInterrupt:
Traceback (most recent call last):
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/queue.py", line 179, in get
    raise Empty
_queue.Empty
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 13, in train_func
    for i, ((v_ninput, t_ninput, nlabel, multi_nlabel), (v_ainput, t_ainput, alabel, multi_alabel)) \
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1284, in _get_data
    success, data = self._try_get_data()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1145, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 472278, 472279, 472280, 472281) exited unexpectedly