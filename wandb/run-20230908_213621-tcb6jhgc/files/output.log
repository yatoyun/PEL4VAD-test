
8000 8100 2900
[2023-09-08 21:36:23,485][main.py][line:165][INFO] total params:7.5349M
[2023-09-08 21:36:23,485][main.py][line:168][INFO] Training Mode
[2023-09-08 21:36:23,486][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 21:36:23,486][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-08 21:36:31,789][main.py][line:82][INFO] Random initialize AUCAUC:0.6086 Anomaly AUC:0.60989
[2023-09-08 21:36:50,147][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.5169 loss2:1.2753 loss3:0.3613 | AUC:0.8294 Anomaly AUC:0.6350
[2023-09-08 21:37:08,253][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.2677 loss2:0.9382 loss3:0.2428 | AUC:0.8360 Anomaly AUC:0.6620
[2023-09-08 21:37:26,869][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.1559 loss2:0.8212 loss3:0.1661 | AUC:0.8388 Anomaly AUC:0.6692
[2023-09-08 21:37:45,391][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0850 loss2:0.7455 loss3:0.1071 | AUC:0.8461 Anomaly AUC:0.6629
[2023-09-08 21:38:04,031][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0643 loss2:0.6950 loss3:0.0695 | AUC:0.8453 Anomaly AUC:0.6720
[2023-09-08 21:38:22,744][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0341 loss2:0.6373 loss3:0.0434 | AUC:0.8496 Anomaly AUC:0.6742
[2023-09-08 21:38:41,411][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0381 loss2:0.6010 loss3:0.0337 | AUC:0.8434 Anomaly AUC:0.6838
[2023-09-08 21:39:00,127][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0134 loss2:0.5511 loss3:0.0235 | AUC:0.8499 Anomaly AUC:0.6724
[2023-09-08 21:39:18,925][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0063 loss2:0.5001 loss3:0.0180 | AUC:0.8497 Anomaly AUC:0.6666
[2023-09-08 21:39:37,743][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0032 loss2:0.4539 loss3:0.0144 | AUC:0.8562 Anomaly AUC:0.6796
[2023-09-08 21:39:56,571][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0027 loss2:0.4152 loss3:0.0127 | AUC:0.8527 Anomaly AUC:0.6746
[2023-09-08 21:40:15,399][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0022 loss2:0.3768 loss3:0.0114 | AUC:0.8538 Anomaly AUC:0.6750
[2023-09-08 21:40:34,188][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0332 loss2:0.3860 loss3:0.0157 | AUC:0.8505 Anomaly AUC:0.6643
[2023-09-08 21:40:52,962][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0377 loss2:0.3987 loss3:0.0166 | AUC:0.8541 Anomaly AUC:0.6683
[2023-09-08 21:41:11,844][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0028 loss2:0.3101 loss3:0.0103 | AUC:0.8564 Anomaly AUC:0.6670
[2023-09-08 21:41:30,552][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0015 loss2:0.2786 loss3:0.0088 | AUC:0.8553 Anomaly AUC:0.6629
[2023-09-08 21:41:49,539][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0012 loss2:0.2550 loss3:0.0079 | AUC:0.8565 Anomaly AUC:0.6656
[2023-09-08 21:42:08,293][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0010 loss2:0.2362 loss3:0.0071 | AUC:0.8547 Anomaly AUC:0.6619
[2023-09-08 21:42:27,193][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0009 loss2:0.2196 loss3:0.0061 | AUC:0.8568 Anomaly AUC:0.6699
[2023-09-08 21:42:46,073][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0008 loss2:0.2032 loss3:0.0052 | AUC:0.8558 Anomaly AUC:0.6635
[2023-09-08 21:43:05,010][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0008 loss2:0.1901 loss3:0.0041 | AUC:0.8575 Anomaly AUC:0.6661
[2023-09-08 21:43:23,839][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0009 loss2:0.1802 loss3:0.0029 | AUC:0.8565 Anomaly AUC:0.6649
[2023-09-08 21:43:42,695][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0010 loss2:0.1674 loss3:0.0014 | AUC:0.8493 Anomaly AUC:0.6449
[2023-09-08 21:44:01,544][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0698 loss2:0.2879 loss3:0.0135 | AUC:0.8553 Anomaly AUC:0.6804
[2023-09-08 21:44:20,298][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0093 loss2:0.1836 loss3:0.0034 | AUC:0.8589 Anomaly AUC:0.6715
[2023-09-08 21:44:39,077][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0011 loss2:0.1504 loss3:0.0013 | AUC:0.8529 Anomaly AUC:0.6598
[2023-09-08 21:44:58,042][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0008 loss2:0.1405 loss3:0.0010 | AUC:0.8529 Anomaly AUC:0.6621
[2023-09-08 21:45:16,895][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0009 loss2:0.1325 loss3:0.0009 | AUC:0.8490 Anomaly AUC:0.6527
[2023-09-08 21:45:35,795][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0006 loss2:0.1251 loss3:0.0008 | AUC:0.8502 Anomaly AUC:0.6572
[2023-09-08 21:45:54,724][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0005 loss2:0.1197 loss3:0.0007 | AUC:0.8491 Anomaly AUC:0.6541
[2023-09-08 21:46:13,674][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0004 loss2:0.1136 loss3:0.0007 | AUC:0.8449 Anomaly AUC:0.6437
[2023-09-08 21:46:32,571][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0004 loss2:0.1078 loss3:0.0007 | AUC:0.8435 Anomaly AUC:0.6404
[2023-09-08 21:46:51,477][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0004 loss2:0.1041 loss3:0.0007 | AUC:0.8442 Anomaly AUC:0.6452
[2023-09-08 21:47:10,399][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0734 loss2:0.2668 loss3:0.0148 | AUC:0.8365 Anomaly AUC:0.6368
[2023-09-08 21:47:29,348][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0053 loss2:0.1340 loss3:0.0023 | AUC:0.8450 Anomaly AUC:0.6482
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 25, in train_func
    v_input = v_input.float().cuda(non_blocking=True)
KeyboardInterrupt