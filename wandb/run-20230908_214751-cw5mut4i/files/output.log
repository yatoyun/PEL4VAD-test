[2023-09-08 21:47:53,147][main.py][line:165][INFO] total params:7.5400M
[2023-09-08 21:47:53,147][main.py][line:168][INFO] Training Mode
[2023-09-08 21:47:53,147][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
)
[2023-09-08 21:47:53,148][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
8000 8100 2900
[2023-09-08 21:48:01,207][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-08 21:48:19,499][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.4374 loss2:1.1496 loss3:0.3402 | AUC:0.8219 Anomaly AUC:0.6583
[2023-09-08 21:48:37,810][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.1801 loss2:0.8474 loss3:0.2034 | AUC:0.8387 Anomaly AUC:0.6840
[2023-09-08 21:48:56,440][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0788 loss2:0.7539 loss3:0.1038 | AUC:0.8408 Anomaly AUC:0.6751
[2023-09-08 21:49:15,007][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0392 loss2:0.6852 loss3:0.0481 | AUC:0.8430 Anomaly AUC:0.6660
[2023-09-08 21:49:33,692][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0323 loss2:0.6325 loss3:0.0285 | AUC:0.8380 Anomaly AUC:0.6804
[2023-09-08 21:49:52,421][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0134 loss2:0.5665 loss3:0.0188 | AUC:0.8472 Anomaly AUC:0.6919
[2023-09-08 21:50:11,247][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0067 loss2:0.5143 loss3:0.0143 | AUC:0.8504 Anomaly AUC:0.6878
[2023-09-08 21:50:30,126][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0032 loss2:0.4610 loss3:0.0110 | AUC:0.8522 Anomaly AUC:0.6929
[2023-09-08 21:50:49,100][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0021 loss2:0.4126 loss3:0.0086 | AUC:0.8575 Anomaly AUC:0.7036
[2023-09-08 21:51:07,902][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0019 loss2:0.3726 loss3:0.0062 | AUC:0.8525 Anomaly AUC:0.6970
[2023-09-08 21:51:26,656][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0025 loss2:0.3387 loss3:0.0039 | AUC:0.8514 Anomaly AUC:0.6938
[2023-09-08 21:51:45,442][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0604 loss2:0.3826 loss3:0.0119 | AUC:0.8473 Anomaly AUC:0.6722
[2023-09-08 21:52:04,292][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0066 loss2:0.3137 loss3:0.0043 | AUC:0.8536 Anomaly AUC:0.6915
[2023-09-08 21:52:23,266][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0015 loss2:0.2795 loss3:0.0026 | AUC:0.8506 Anomaly AUC:0.6858
[2023-09-08 21:52:42,135][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0010 loss2:0.2579 loss3:0.0021 | AUC:0.8522 Anomaly AUC:0.6924
[2023-09-08 21:53:01,005][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0009 loss2:0.2398 loss3:0.0018 | AUC:0.8465 Anomaly AUC:0.6881
[2023-09-08 21:53:19,840][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0009 loss2:0.2237 loss3:0.0019 | AUC:0.8491 Anomaly AUC:0.6820
[2023-09-08 21:53:38,610][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0008 loss2:0.2077 loss3:0.0017 | AUC:0.8407 Anomaly AUC:0.6805
[2023-09-08 21:53:57,484][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0012 loss2:0.1967 loss3:0.0020 | AUC:0.8501 Anomaly AUC:0.6868
[2023-09-08 21:54:16,265][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0355 loss2:0.2288 loss3:0.0064 | AUC:0.8480 Anomaly AUC:0.6948
[2023-09-08 21:54:35,183][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0168 loss2:0.2142 loss3:0.0046 | AUC:0.8579 Anomaly AUC:0.6983
[2023-09-08 21:54:54,175][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0018 loss2:0.1790 loss3:0.0022 | AUC:0.8597 Anomaly AUC:0.7000
[2023-09-08 21:55:13,066][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0012 loss2:0.1658 loss3:0.0018 | AUC:0.8503 Anomaly AUC:0.6883
[2023-09-08 21:55:31,993][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0026 loss2:0.1609 loss3:0.0023 | AUC:0.8556 Anomaly AUC:0.6946
[2023-09-08 21:55:50,934][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0007 loss2:0.1496 loss3:0.0016 | AUC:0.8538 Anomaly AUC:0.6894
[2023-09-08 21:56:09,943][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0006 loss2:0.1421 loss3:0.0014 | AUC:0.8569 Anomaly AUC:0.6965
[2023-09-08 21:56:28,877][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0023 loss2:0.1396 loss3:0.0017 | AUC:0.8512 Anomaly AUC:0.6870
[2023-09-08 21:56:47,668][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0013 loss2:0.1326 loss3:0.0018 | AUC:0.8521 Anomaly AUC:0.6917
[2023-09-08 21:57:06,589][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0171 loss2:0.1508 loss3:0.0037 | AUC:0.8629 Anomaly AUC:0.6990
[2023-09-08 21:57:25,536][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0068 loss2:0.1363 loss3:0.0029 | AUC:0.8504 Anomaly AUC:0.6771
[2023-09-08 21:57:44,478][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0026 loss2:0.1205 loss3:0.0021 | AUC:0.8542 Anomaly AUC:0.6851
[2023-09-08 21:58:03,288][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0004 loss2:0.1123 loss3:0.0013 | AUC:0.8583 Anomaly AUC:0.6896
[2023-09-08 21:58:22,254][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0003 loss2:0.1055 loss3:0.0011 | AUC:0.8547 Anomaly AUC:0.6849
[2023-09-08 21:58:41,163][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0003 loss2:0.1018 loss3:0.0009 | AUC:0.8560 Anomaly AUC:0.6872
[2023-09-08 21:59:00,173][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0003 loss2:0.0972 loss3:0.0008 | AUC:0.8526 Anomaly AUC:0.6839
[2023-09-08 21:59:19,144][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0003 loss2:0.0930 loss3:0.0007 | AUC:0.8541 Anomaly AUC:0.6867
[2023-09-08 21:59:38,017][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0003 loss2:0.0896 loss3:0.0007 | AUC:0.8566 Anomaly AUC:0.6902
[2023-09-08 21:59:56,917][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0002 loss2:0.0863 loss3:0.0006 | AUC:0.8547 Anomaly AUC:0.6888
[2023-09-08 22:00:15,837][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0002 loss2:0.0825 loss3:0.0006 | AUC:0.8574 Anomaly AUC:0.6918
[2023-09-08 22:00:34,878][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0002 loss2:0.0792 loss3:0.0006 | AUC:0.8530 Anomaly AUC:0.6846
[2023-09-08 22:00:53,815][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0002 loss2:0.0765 loss3:0.0007 | AUC:0.8524 Anomaly AUC:0.6887
[2023-09-08 22:01:12,721][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0378 loss2:0.1324 loss3:0.0076 | AUC:0.8238 Anomaly AUC:0.6668
[2023-09-08 22:01:31,743][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0116 loss2:0.1019 loss3:0.0039 | AUC:0.8448 Anomaly AUC:0.6859
[2023-09-08 22:01:50,701][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0030 loss2:0.0755 loss3:0.0017 | AUC:0.8426 Anomaly AUC:0.6844
[2023-09-08 22:02:09,668][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0005 loss2:0.0678 loss3:0.0008 | AUC:0.8442 Anomaly AUC:0.6849
[2023-09-08 22:02:28,796][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0003 loss2:0.0640 loss3:0.0007 | AUC:0.8449 Anomaly AUC:0.6864
[2023-09-08 22:02:47,837][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0003 loss2:0.0606 loss3:0.0006 | AUC:0.8445 Anomaly AUC:0.6848
[2023-09-08 22:03:06,810][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0002 loss2:0.0588 loss3:0.0005 | AUC:0.8449 Anomaly AUC:0.6842
[2023-09-08 22:03:25,823][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0002 loss2:0.0555 loss3:0.0005 | AUC:0.8454 Anomaly AUC:0.6851
[2023-09-08 22:03:44,898][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0002 loss2:0.0526 loss3:0.0005 | AUC:0.8460 Anomaly AUC:0.6852
[2023-09-08 22:04:03,987][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0002 loss2:0.0511 loss3:0.0005 | AUC:0.8463 Anomaly AUC:0.6849
[2023-09-08 22:04:22,966][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0002 loss2:0.0487 loss3:0.0005 | AUC:0.8469 Anomaly AUC:0.6856
[2023-09-08 22:04:41,926][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00010 | loss1:0.0002 loss2:0.0469 loss3:0.0005 | AUC:0.8470 Anomaly AUC:0.6858
[2023-09-08 22:05:00,873][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00010 | loss1:0.0001 loss2:0.0452 loss3:0.0005 | AUC:0.8476 Anomaly AUC:0.6863
[2023-09-08 22:05:19,883][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00010 | loss1:0.0001 loss2:0.0427 loss3:0.0004 | AUC:0.8472 Anomaly AUC:0.6855
[2023-09-08 22:05:38,889][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00010 | loss1:0.0001 loss2:0.0415 loss3:0.0004 | AUC:0.8477 Anomaly AUC:0.6859
[2023-09-08 22:05:57,978][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00010 | loss1:0.0001 loss2:0.0395 loss3:0.0004 | AUC:0.8481 Anomaly AUC:0.6854
[2023-09-08 22:06:17,046][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00010 | loss1:0.0001 loss2:0.0380 loss3:0.0004 | AUC:0.8481 Anomaly AUC:0.6856
[2023-09-08 22:06:36,187][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00010 | loss1:0.0001 loss2:0.0363 loss3:0.0004 | AUC:0.8481 Anomaly AUC:0.6845
[2023-09-08 22:06:55,239][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00010 | loss1:0.0001 loss2:0.0351 loss3:0.0004 | AUC:0.8496 Anomaly AUC:0.6872
[2023-09-08 22:07:14,330][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00010 | loss1:0.0001 loss2:0.0333 loss3:0.0004 | AUC:0.8500 Anomaly AUC:0.6878
[2023-09-08 22:07:33,397][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00010 | loss1:0.0001 loss2:0.0320 loss3:0.0004 | AUC:0.8497 Anomaly AUC:0.6885
[2023-09-08 22:07:52,483][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00010 | loss1:0.0001 loss2:0.0308 loss3:0.0004 | AUC:0.8463 Anomaly AUC:0.6814
[2023-09-08 22:08:11,478][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00010 | loss1:0.0001 loss2:0.0290 loss3:0.0004 | AUC:0.8474 Anomaly AUC:0.6847
[2023-09-08 22:08:30,444][main.py][line:106][INFO] [Epoch:65/100]: lr:0.00010 | loss1:0.0001 loss2:0.0282 loss3:0.0005 | AUC:0.8491 Anomaly AUC:0.6860
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 36, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 39, in forward
    x_k_split = self.DR_DMU(x_split)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 64, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 76, in forward
    x = attn(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 14, in forward
    return self.fn(self.norm(x), **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 63, in forward
    return self.to_out(out)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt