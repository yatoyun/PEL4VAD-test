
8000 8100 2900
[2023-09-09 13:19:51,010][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 13:19:51,011][main.py][line:168][INFO] Training Mode
[2023-09-09 13:19:51,011][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 13:19:51,011][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0005
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 13:19:59,013][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 13:20:16,512][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:0.0070 loss2:0.8904 loss3:0.1400 | AUC:0.6717 Anomaly AUC:0.6542
[2023-09-09 13:20:33,966][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:0.0015 loss2:0.5649 loss3:0.0286 | AUC:0.8309 Anomaly AUC:0.6677
[2023-09-09 13:20:51,595][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:0.0006 loss2:0.3935 loss3:0.0160 | AUC:0.6373 Anomaly AUC:0.6302
[2023-09-09 13:21:09,194][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:0.0004 loss2:0.2722 loss3:0.0082 | AUC:0.8298 Anomaly AUC:0.6533
[2023-09-09 13:21:26,908][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:0.0002 loss2:0.1953 loss3:0.0028 | AUC:0.8245 Anomaly AUC:0.6558
[2023-09-09 13:21:44,554][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:0.0002 loss2:0.1498 loss3:0.0017 | AUC:0.8336 Anomaly AUC:0.6548
[2023-09-09 13:22:02,209][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00049 | loss1:0.0002 loss2:0.1389 loss3:0.0028 | AUC:0.8344 Anomaly AUC:0.6563
[2023-09-09 13:22:19,891][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00049 | loss1:0.0003 loss2:0.1255 loss3:0.0031 | AUC:0.8299 Anomaly AUC:0.6538
[2023-09-09 13:22:37,670][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00049 | loss1:0.0002 loss2:0.0945 loss3:0.0019 | AUC:0.8342 Anomaly AUC:0.6553
[2023-09-09 13:22:55,460][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00049 | loss1:0.0001 loss2:0.0658 loss3:0.0006 | AUC:0.8354 Anomaly AUC:0.6577
[2023-09-09 13:23:13,156][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00049 | loss1:0.0001 loss2:0.0633 loss3:0.0019 | AUC:0.8294 Anomaly AUC:0.6420
[2023-09-09 13:23:30,823][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00048 | loss1:0.0002 loss2:0.0597 loss3:0.0019 | AUC:0.8470 Anomaly AUC:0.6638
[2023-09-09 13:23:48,492][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00048 | loss1:0.0002 loss2:0.0706 loss3:0.0033 | AUC:0.8339 Anomaly AUC:0.6584
[2023-09-09 13:24:06,085][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00048 | loss1:0.0006 loss2:0.0971 loss3:0.0084 | AUC:0.8202 Anomaly AUC:0.6533
[2023-09-09 13:24:23,807][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00047 | loss1:0.0001 loss2:0.0506 loss3:0.0028 | AUC:0.8272 Anomaly AUC:0.6519
[2023-09-09 13:24:41,570][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00047 | loss1:0.0001 loss2:0.0349 loss3:0.0011 | AUC:0.8348 Anomaly AUC:0.6484
[2023-09-09 13:24:59,346][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00047 | loss1:0.0001 loss2:0.0281 loss3:0.0013 | AUC:0.8224 Anomaly AUC:0.6451
[2023-09-09 13:25:17,232][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00046 | loss1:0.0000 loss2:0.0218 loss3:0.0009 | AUC:0.8082 Anomaly AUC:0.6339
[2023-09-09 13:25:34,892][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00046 | loss1:0.0004 loss2:0.0552 loss3:0.0074 | AUC:0.7810 Anomaly AUC:0.5984
[2023-09-09 13:25:52,833][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00045 | loss1:0.0002 loss2:0.0317 loss3:0.0030 | AUC:0.7115 Anomaly AUC:0.6221
[2023-09-09 13:26:10,501][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00045 | loss1:0.0001 loss2:0.0179 loss3:0.0011 | AUC:0.6550 Anomaly AUC:0.6117
[2023-09-09 13:26:28,202][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00044 | loss1:0.0001 loss2:0.0114 loss3:0.0003 | AUC:0.6454 Anomaly AUC:0.6053
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 25, in train_func
    v_input = v_input.float().cuda(non_blocking=True)
KeyboardInterrupt