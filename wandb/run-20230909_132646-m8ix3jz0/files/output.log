
8000 8100 2900
[2023-09-09 13:26:48,237][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 13:26:48,238][main.py][line:168][INFO] Training Mode
[2023-09-09 13:26:48,238][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 13:26:48,238][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 13:26:56,285][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 13:27:13,781][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.7033 loss2:1.3637 loss3:0.3816 | AUC:0.8138 Anomaly AUC:0.6006
[2023-09-09 13:27:31,129][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.4346 loss2:1.2466 loss3:0.3590 | AUC:0.8197 Anomaly AUC:0.6231
[2023-09-09 13:27:48,716][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.4043 loss2:1.0682 loss3:0.3059 | AUC:0.8296 Anomaly AUC:0.6433
[2023-09-09 13:28:06,342][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.3078 loss2:0.9267 loss3:0.2407 | AUC:0.8385 Anomaly AUC:0.6455
[2023-09-09 13:28:24,047][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.2001 loss2:0.8303 loss3:0.1743 | AUC:0.8440 Anomaly AUC:0.6524
[2023-09-09 13:28:41,697][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.1407 loss2:0.7715 loss3:0.1235 | AUC:0.8354 Anomaly AUC:0.6392
[2023-09-09 13:28:59,354][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0881 loss2:0.7154 loss3:0.0819 | AUC:0.8299 Anomaly AUC:0.6384
[2023-09-09 13:29:16,997][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0767 loss2:0.6711 loss3:0.0559 | AUC:0.8226 Anomaly AUC:0.6700
[2023-09-09 13:29:34,730][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0551 loss2:0.6222 loss3:0.0379 | AUC:0.8410 Anomaly AUC:0.6744
[2023-09-09 13:29:52,446][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0448 loss2:0.5770 loss3:0.0268 | AUC:0.8329 Anomaly AUC:0.6680
[2023-09-09 13:30:10,086][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0422 loss2:0.5350 loss3:0.0204 | AUC:0.8389 Anomaly AUC:0.6657
[2023-09-09 13:30:27,769][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0412 loss2:0.4971 loss3:0.0163 | AUC:0.8400 Anomaly AUC:0.6693
[2023-09-09 13:30:45,709][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0374 loss2:0.4570 loss3:0.0128 | AUC:0.8453 Anomaly AUC:0.6773
[2023-09-09 13:31:03,353][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0369 loss2:0.4198 loss3:0.0100 | AUC:0.8394 Anomaly AUC:0.6676
[2023-09-09 13:31:21,032][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00009 | loss1:0.0339 loss2:0.3855 loss3:0.0079 | AUC:0.8427 Anomaly AUC:0.6699
[2023-09-09 13:31:38,710][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00009 | loss1:0.0315 loss2:0.3539 loss3:0.0065 | AUC:0.8493 Anomaly AUC:0.6745
[2023-09-09 13:31:56,324][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00009 | loss1:0.0290 loss2:0.3273 loss3:0.0056 | AUC:0.8427 Anomaly AUC:0.6664
[2023-09-09 13:32:13,990][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00009 | loss1:0.0724 loss2:0.3466 loss3:0.0158 | AUC:0.8392 Anomaly AUC:0.6447
[2023-09-09 13:32:31,648][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00009 | loss1:0.0966 loss2:0.3712 loss3:0.0175 | AUC:0.8326 Anomaly AUC:0.6394
[2023-09-09 13:32:49,549][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00009 | loss1:0.0316 loss2:0.2926 loss3:0.0072 | AUC:0.8430 Anomaly AUC:0.6538
[2023-09-09 13:33:07,238][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00009 | loss1:0.0259 loss2:0.2663 loss3:0.0054 | AUC:0.8525 Anomaly AUC:0.6631
[2023-09-09 13:33:24,987][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00009 | loss1:0.0234 loss2:0.2485 loss3:0.0043 | AUC:0.8544 Anomaly AUC:0.6676
[2023-09-09 13:33:42,730][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00009 | loss1:0.0216 loss2:0.2325 loss3:0.0037 | AUC:0.8558 Anomaly AUC:0.6694
[2023-09-09 13:34:00,359][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00009 | loss1:0.0202 loss2:0.2204 loss3:0.0033 | AUC:0.8543 Anomaly AUC:0.6648
[2023-09-09 13:34:17,970][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00009 | loss1:0.0188 loss2:0.2082 loss3:0.0030 | AUC:0.8549 Anomaly AUC:0.6647
[2023-09-09 13:34:35,596][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00008 | loss1:0.0175 loss2:0.1980 loss3:0.0027 | AUC:0.8563 Anomaly AUC:0.6677
[2023-09-09 13:34:53,339][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00008 | loss1:0.0167 loss2:0.1880 loss3:0.0025 | AUC:0.8563 Anomaly AUC:0.6682
[2023-09-09 13:35:10,994][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00008 | loss1:0.0157 loss2:0.1786 loss3:0.0023 | AUC:0.8571 Anomaly AUC:0.6667
[2023-09-09 13:35:28,701][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00008 | loss1:0.0147 loss2:0.1708 loss3:0.0021 | AUC:0.8564 Anomaly AUC:0.6675
[2023-09-09 13:35:46,381][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00008 | loss1:0.0139 loss2:0.1636 loss3:0.0019 | AUC:0.8567 Anomaly AUC:0.6657
[2023-09-09 13:36:04,064][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00008 | loss1:0.0127 loss2:0.1569 loss3:0.0018 | AUC:0.8560 Anomaly AUC:0.6660
[2023-09-09 13:36:21,888][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00008 | loss1:0.0125 loss2:0.1517 loss3:0.0017 | AUC:0.8570 Anomaly AUC:0.6688
[2023-09-09 13:36:39,509][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00008 | loss1:0.0115 loss2:0.1441 loss3:0.0016 | AUC:0.8566 Anomaly AUC:0.6681
[2023-09-09 13:36:57,254][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00007 | loss1:0.0112 loss2:0.1402 loss3:0.0016 | AUC:0.8581 Anomaly AUC:0.6715
[2023-09-09 13:37:14,939][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00007 | loss1:0.0108 loss2:0.1343 loss3:0.0015 | AUC:0.8554 Anomaly AUC:0.6669
[2023-09-09 13:37:32,794][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00007 | loss1:0.0105 loss2:0.1291 loss3:0.0014 | AUC:0.8570 Anomaly AUC:0.6701
[2023-09-09 13:37:50,589][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00007 | loss1:0.0101 loss2:0.1254 loss3:0.0014 | AUC:0.8553 Anomaly AUC:0.6679
[2023-09-09 13:38:08,334][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00007 | loss1:0.0101 loss2:0.1216 loss3:0.0013 | AUC:0.8554 Anomaly AUC:0.6699
[2023-09-09 13:38:26,104][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00007 | loss1:0.5119 loss2:0.9194 loss3:0.3733 | AUC:0.8120 Anomaly AUC:0.6391
[2023-09-09 13:38:43,784][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00007 | loss1:0.2438 loss2:0.8395 loss3:0.1446 | AUC:0.8244 Anomaly AUC:0.6465
[2023-09-09 13:39:01,533][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00006 | loss1:0.1425 loss2:0.7199 loss3:0.0948 | AUC:0.8281 Anomaly AUC:0.6483
[2023-09-09 13:39:19,329][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00006 | loss1:0.0885 loss2:0.6373 loss3:0.0696 | AUC:0.8367 Anomaly AUC:0.6658
[2023-09-09 13:39:37,244][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00006 | loss1:0.0634 loss2:0.5747 loss3:0.0526 | AUC:0.8393 Anomaly AUC:0.6660
[2023-09-09 13:39:54,908][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00006 | loss1:0.0510 loss2:0.5173 loss3:0.0411 | AUC:0.8416 Anomaly AUC:0.6660
[2023-09-09 13:40:12,757][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00006 | loss1:0.0409 loss2:0.4684 loss3:0.0330 | AUC:0.8442 Anomaly AUC:0.6716
[2023-09-09 13:40:30,534][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00006 | loss1:0.0380 loss2:0.4254 loss3:0.0281 | AUC:0.8437 Anomaly AUC:0.6684
[2023-09-09 13:40:48,406][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00005 | loss1:0.0345 loss2:0.3870 loss3:0.0242 | AUC:0.8453 Anomaly AUC:0.6703
[2023-09-09 13:41:06,129][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00005 | loss1:0.0307 loss2:0.3571 loss3:0.0213 | AUC:0.8449 Anomaly AUC:0.6685
[2023-09-09 13:41:23,881][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00005 | loss1:0.0293 loss2:0.3268 loss3:0.0194 | AUC:0.8483 Anomaly AUC:0.6734
[2023-09-09 13:41:41,727][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00005 | loss1:0.0269 loss2:0.3028 loss3:0.0175 | AUC:0.8457 Anomaly AUC:0.6687
[2023-09-09 13:41:59,525][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00005 | loss1:0.0259 loss2:0.2843 loss3:0.0161 | AUC:0.8471 Anomaly AUC:0.6712
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 30, in train_func
    logits, x_k, output_MSNSD = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 26, in forward
    mask = self.get_mask(self.win_size, x.shape[1], seq_len)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 63, in get_mask
    m = m.repeat(self.n_heads, len(seq_len), 1, 1).cuda()
KeyboardInterrupt