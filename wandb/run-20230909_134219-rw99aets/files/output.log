
8000 8100 2900
[2023-09-09 13:42:21,023][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 13:42:21,023][main.py][line:168][INFO] Training Mode
[2023-09-09 13:42:21,024][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 13:42:21,024][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 13:42:29,057][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 13:42:46,629][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3654 loss2:1.1552 loss3:0.3401 | AUC:0.7761 Anomaly AUC:0.6183
[2023-09-09 13:43:04,037][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.2157 loss2:0.8709 loss3:0.1945 | AUC:0.7867 Anomaly AUC:0.5966
[2023-09-09 13:43:21,543][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.1248 loss2:0.7741 loss3:0.0932 | AUC:0.7995 Anomaly AUC:0.5982
[2023-09-09 13:43:39,101][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0817 loss2:0.7122 loss3:0.0458 | AUC:0.8303 Anomaly AUC:0.6516
[2023-09-09 13:43:56,713][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0326 loss2:0.6363 loss3:0.0253 | AUC:0.8451 Anomaly AUC:0.6698
[2023-09-09 13:44:14,318][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0341 loss2:0.5836 loss3:0.0190 | AUC:0.8251 Anomaly AUC:0.6350
[2023-09-09 13:44:32,008][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0223 loss2:0.5317 loss3:0.0141 | AUC:0.8467 Anomaly AUC:0.6559
[2023-09-09 13:44:49,655][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0119 loss2:0.4714 loss3:0.0098 | AUC:0.8451 Anomaly AUC:0.6576
[2023-09-09 13:45:07,245][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0091 loss2:0.4197 loss3:0.0071 | AUC:0.8468 Anomaly AUC:0.6642
[2023-09-09 13:45:24,893][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0079 loss2:0.3811 loss3:0.0045 | AUC:0.8412 Anomaly AUC:0.6560
[2023-09-09 13:45:42,604][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0066 loss2:0.3426 loss3:0.0022 | AUC:0.8424 Anomaly AUC:0.6549
[2023-09-09 13:46:00,335][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0065 loss2:0.3125 loss3:0.0017 | AUC:0.8416 Anomaly AUC:0.6519
[2023-09-09 13:46:17,937][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0063 loss2:0.2851 loss3:0.0016 | AUC:0.8335 Anomaly AUC:0.6460
[2023-09-09 13:46:35,587][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0060 loss2:0.2640 loss3:0.0015 | AUC:0.8317 Anomaly AUC:0.6380
[2023-09-09 13:46:53,148][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0054 loss2:0.2450 loss3:0.0014 | AUC:0.8237 Anomaly AUC:0.6348
[2023-09-09 13:47:10,859][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0051 loss2:0.2311 loss3:0.0016 | AUC:0.8145 Anomaly AUC:0.6284
[2023-09-09 13:47:28,629][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0230 loss2:0.2323 loss3:0.0045 | AUC:0.8284 Anomaly AUC:0.6492
[2023-09-09 13:47:46,356][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0642 loss2:0.2972 loss3:0.0131 | AUC:0.8151 Anomaly AUC:0.6064
[2023-09-09 13:48:04,027][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0111 loss2:0.2194 loss3:0.0029 | AUC:0.8362 Anomaly AUC:0.6447
[2023-09-09 13:48:21,753][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0036 loss2:0.1916 loss3:0.0014 | AUC:0.8348 Anomaly AUC:0.6494
[2023-09-09 13:48:39,439][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0029 loss2:0.1798 loss3:0.0012 | AUC:0.8336 Anomaly AUC:0.6450
[2023-09-09 13:48:57,149][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0025 loss2:0.1710 loss3:0.0011 | AUC:0.8299 Anomaly AUC:0.6414
[2023-09-09 13:49:14,756][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0023 loss2:0.1627 loss3:0.0010 | AUC:0.8280 Anomaly AUC:0.6431
[2023-09-09 13:49:32,439][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0019 loss2:0.1552 loss3:0.0009 | AUC:0.8232 Anomaly AUC:0.6351
[2023-09-09 13:49:50,168][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0018 loss2:0.1487 loss3:0.0009 | AUC:0.8239 Anomaly AUC:0.6422
[2023-09-09 13:50:07,872][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0015 loss2:0.1420 loss3:0.0008 | AUC:0.8234 Anomaly AUC:0.6384
[2023-09-09 13:50:25,649][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0014 loss2:0.1363 loss3:0.0008 | AUC:0.8228 Anomaly AUC:0.6349
[2023-09-09 13:50:43,478][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0013 loss2:0.1299 loss3:0.0008 | AUC:0.8195 Anomaly AUC:0.6341
[2023-09-09 13:51:01,092][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0011 loss2:0.1251 loss3:0.0007 | AUC:0.8185 Anomaly AUC:0.6322
[2023-09-09 13:51:18,775][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0010 loss2:0.1199 loss3:0.0007 | AUC:0.8216 Anomaly AUC:0.6382
[2023-09-09 13:51:36,448][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0010 loss2:0.1155 loss3:0.0007 | AUC:0.8207 Anomaly AUC:0.6393
[2023-09-09 13:51:54,251][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0009 loss2:0.1117 loss3:0.0007 | AUC:0.8178 Anomaly AUC:0.6338
[2023-09-09 13:52:11,911][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0008 loss2:0.1054 loss3:0.0007 | AUC:0.8160 Anomaly AUC:0.6269
[2023-09-09 13:52:29,657][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0008 loss2:0.1027 loss3:0.0006 | AUC:0.8207 Anomaly AUC:0.6340
[2023-09-09 13:52:47,280][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0006 loss2:0.0979 loss3:0.0006 | AUC:0.8191 Anomaly AUC:0.6304
[2023-09-09 13:53:05,055][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0007 loss2:0.0938 loss3:0.0006 | AUC:0.8165 Anomaly AUC:0.6242
[2023-09-09 13:53:22,718][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0006 loss2:0.0909 loss3:0.0006 | AUC:0.8123 Anomaly AUC:0.6246
[2023-09-09 13:53:40,359][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0005 loss2:0.0871 loss3:0.0005 | AUC:0.8140 Anomaly AUC:0.6244
[2023-09-09 13:53:58,056][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0005 loss2:0.0833 loss3:0.0005 | AUC:0.8160 Anomaly AUC:0.6256
[2023-09-09 13:54:15,814][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0005 loss2:0.0803 loss3:0.0005 | AUC:0.8046 Anomaly AUC:0.6180
[2023-09-09 13:54:33,498][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0005 loss2:0.0787 loss3:0.0007 | AUC:0.8122 Anomaly AUC:0.6237
[2023-09-09 13:54:51,307][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0006 loss2:0.0758 loss3:0.0007 | AUC:0.8153 Anomaly AUC:0.6300
[2023-09-09 13:55:09,241][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0004 loss2:0.0719 loss3:0.0006 | AUC:0.8174 Anomaly AUC:0.6271
[2023-09-09 13:55:26,920][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0003 loss2:0.0681 loss3:0.0005 | AUC:0.8171 Anomaly AUC:0.6262
[2023-09-09 13:55:44,705][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0003 loss2:0.0657 loss3:0.0005 | AUC:0.8116 Anomaly AUC:0.6217
[2023-09-09 13:56:02,520][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0003 loss2:0.0631 loss3:0.0005 | AUC:0.8141 Anomaly AUC:0.6241
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 37, in train_func
    video_feat, token_feat, video_labels = get_cas(v_feat, t_input, logits, multi_label)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/utils.py", line 119, in get_cas
    video_feat = torch.cat((video_feat, fg), dim=0)
KeyboardInterrupt