
8000 8100 2900
[2023-09-09 13:56:51,349][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 13:56:51,349][main.py][line:168][INFO] Training Mode
[2023-09-09 13:56:51,350][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 13:56:51,350][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 13:56:59,344][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 13:57:17,778][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.4349 loss2:1.1467 loss3:0.3419 | AUC:0.8093 Anomaly AUC:0.6481
[2023-09-09 13:57:35,835][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.1786 loss2:0.8462 loss3:0.2046 | AUC:0.8261 Anomaly AUC:0.6898
[2023-09-09 13:57:54,451][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0732 loss2:0.7514 loss3:0.1039 | AUC:0.8298 Anomaly AUC:0.6661
[2023-09-09 13:58:13,067][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0391 loss2:0.6865 loss3:0.0475 | AUC:0.8449 Anomaly AUC:0.6880
[2023-09-09 13:58:31,677][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0193 loss2:0.6301 loss3:0.0275 | AUC:0.8419 Anomaly AUC:0.6792
[2023-09-09 13:58:50,436][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0269 loss2:0.5821 loss3:0.0205 | AUC:0.8352 Anomaly AUC:0.6688
[2023-09-09 13:59:09,289][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0098 loss2:0.5277 loss3:0.0150 | AUC:0.8418 Anomaly AUC:0.6704
[2023-09-09 13:59:28,328][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0076 loss2:0.4815 loss3:0.0121 | AUC:0.8279 Anomaly AUC:0.6621
[2023-09-09 13:59:47,357][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0024 loss2:0.4334 loss3:0.0093 | AUC:0.8410 Anomaly AUC:0.6761
[2023-09-09 14:00:06,248][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0018 loss2:0.3934 loss3:0.0071 | AUC:0.8407 Anomaly AUC:0.6786
[2023-09-09 14:00:25,185][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0036 loss2:0.3608 loss3:0.0051 | AUC:0.8414 Anomaly AUC:0.6770
[2023-09-09 14:00:44,033][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0019 loss2:0.3325 loss3:0.0027 | AUC:0.8428 Anomaly AUC:0.6754
[2023-09-09 14:01:02,980][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0098 loss2:0.3154 loss3:0.0035 | AUC:0.8301 Anomaly AUC:0.6615
[2023-09-09 14:01:21,859][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0304 loss2:0.3308 loss3:0.0064 | AUC:0.8462 Anomaly AUC:0.6733
[2023-09-09 14:01:40,785][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0075 loss2:0.2926 loss3:0.0040 | AUC:0.8496 Anomaly AUC:0.6799
[2023-09-09 14:01:59,800][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0013 loss2:0.2549 loss3:0.0024 | AUC:0.8525 Anomaly AUC:0.6855
[2023-09-09 14:02:18,684][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0007 loss2:0.2354 loss3:0.0021 | AUC:0.8514 Anomaly AUC:0.6844
[2023-09-09 14:02:37,647][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0007 loss2:0.2193 loss3:0.0019 | AUC:0.8490 Anomaly AUC:0.6803
[2023-09-09 14:02:56,664][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0006 loss2:0.2049 loss3:0.0018 | AUC:0.8532 Anomaly AUC:0.6856
[2023-09-09 14:03:15,568][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0005 loss2:0.1917 loss3:0.0017 | AUC:0.8517 Anomaly AUC:0.6825
[2023-09-09 14:03:34,517][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0005 loss2:0.1819 loss3:0.0016 | AUC:0.8492 Anomaly AUC:0.6813
[2023-09-09 14:03:53,453][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0005 loss2:0.1721 loss3:0.0015 | AUC:0.8518 Anomaly AUC:0.6863
[2023-09-09 14:04:12,394][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0004 loss2:0.1632 loss3:0.0013 | AUC:0.8525 Anomaly AUC:0.6856
[2023-09-09 14:04:31,338][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0004 loss2:0.1553 loss3:0.0012 | AUC:0.8521 Anomaly AUC:0.6871
[2023-09-09 14:04:50,347][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0003 loss2:0.1481 loss3:0.0012 | AUC:0.8541 Anomaly AUC:0.6893
[2023-09-09 14:05:09,328][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0003 loss2:0.1414 loss3:0.0011 | AUC:0.8539 Anomaly AUC:0.6888
[2023-09-09 14:05:28,335][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0003 loss2:0.1362 loss3:0.0010 | AUC:0.8547 Anomaly AUC:0.6875
[2023-09-09 14:05:47,303][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0468 loss2:0.2022 loss3:0.0108 | AUC:0.8425 Anomaly AUC:0.6805
[2023-09-09 14:06:06,306][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0020 loss2:0.1369 loss3:0.0023 | AUC:0.8427 Anomaly AUC:0.6802
[2023-09-09 14:06:25,250][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0005 loss2:0.1242 loss3:0.0014 | AUC:0.8475 Anomaly AUC:0.6836
[2023-09-09 14:06:44,286][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0004 loss2:0.1176 loss3:0.0012 | AUC:0.8503 Anomaly AUC:0.6863
[2023-09-09 14:07:03,264][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0004 loss2:0.1132 loss3:0.0011 | AUC:0.8492 Anomaly AUC:0.6852
[2023-09-09 14:07:22,359][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0003 loss2:0.1069 loss3:0.0010 | AUC:0.8505 Anomaly AUC:0.6879
[2023-09-09 14:07:41,416][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0003 loss2:0.1035 loss3:0.0009 | AUC:0.8522 Anomaly AUC:0.6908
[2023-09-09 14:08:00,377][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0003 loss2:0.0990 loss3:0.0009 | AUC:0.8515 Anomaly AUC:0.6883
[2023-09-09 14:08:19,362][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0002 loss2:0.0946 loss3:0.0008 | AUC:0.8510 Anomaly AUC:0.6878
[2023-09-09 14:08:38,328][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0002 loss2:0.0912 loss3:0.0007 | AUC:0.8518 Anomaly AUC:0.6893
[2023-09-09 14:08:57,382][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0002 loss2:0.0879 loss3:0.0007 | AUC:0.8527 Anomaly AUC:0.6918
[2023-09-09 14:09:16,420][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0002 loss2:0.0839 loss3:0.0007 | AUC:0.8508 Anomaly AUC:0.6916
[2023-09-09 14:09:35,461][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0002 loss2:0.0808 loss3:0.0006 | AUC:0.8489 Anomaly AUC:0.6849
[2023-09-09 14:09:54,633][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0525 loss2:0.1380 loss3:0.0088 | AUC:0.8399 Anomaly AUC:0.6794
[2023-09-09 14:10:13,640][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0178 loss2:0.1184 loss3:0.0053 | AUC:0.8483 Anomaly AUC:0.6928
[2023-09-09 14:10:32,649][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0020 loss2:0.0801 loss3:0.0019 | AUC:0.8529 Anomaly AUC:0.6968
[2023-09-09 14:10:51,739][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0005 loss2:0.0730 loss3:0.0012 | AUC:0.8534 Anomaly AUC:0.6978
[2023-09-09 14:11:10,806][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0004 loss2:0.0687 loss3:0.0010 | AUC:0.8514 Anomaly AUC:0.6966
[2023-09-09 14:11:30,000][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0003 loss2:0.0656 loss3:0.0008 | AUC:0.8520 Anomaly AUC:0.6953
[2023-09-09 14:11:49,191][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0003 loss2:0.0627 loss3:0.0008 | AUC:0.8531 Anomaly AUC:0.6934
[2023-09-09 14:12:08,381][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0003 loss2:0.0606 loss3:0.0007 | AUC:0.8541 Anomaly AUC:0.6942
[2023-09-09 14:12:27,445][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0002 loss2:0.0572 loss3:0.0006 | AUC:0.8533 Anomaly AUC:0.6941
[2023-09-09 14:12:46,787][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0002 loss2:0.0542 loss3:0.0006 | AUC:0.8528 Anomaly AUC:0.6928
[2023-09-09 14:13:05,981][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0002 loss2:0.0526 loss3:0.0006 | AUC:0.8533 Anomaly AUC:0.6936
[2023-09-09 14:13:25,094][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0002 loss2:0.0501 loss3:0.0005 | AUC:0.8541 Anomaly AUC:0.6945
[2023-09-09 14:13:44,234][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00010 | loss1:0.0002 loss2:0.0483 loss3:0.0005 | AUC:0.8541 Anomaly AUC:0.6943
[2023-09-09 14:14:03,430][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00010 | loss1:0.0002 loss2:0.0466 loss3:0.0005 | AUC:0.8530 Anomaly AUC:0.6921
[2023-09-09 14:14:22,494][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00010 | loss1:0.0001 loss2:0.0440 loss3:0.0005 | AUC:0.8523 Anomaly AUC:0.6913
[2023-09-09 14:14:41,653][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00010 | loss1:0.0001 loss2:0.0427 loss3:0.0005 | AUC:0.8535 Anomaly AUC:0.6932
[2023-09-09 14:15:00,802][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00010 | loss1:0.0001 loss2:0.0407 loss3:0.0005 | AUC:0.8531 Anomaly AUC:0.6917
[2023-09-09 14:15:19,952][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00010 | loss1:0.0001 loss2:0.0391 loss3:0.0005 | AUC:0.8541 Anomaly AUC:0.6929
[2023-09-09 14:15:39,038][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00010 | loss1:0.0001 loss2:0.0373 loss3:0.0005 | AUC:0.8542 Anomaly AUC:0.6931
[2023-09-09 14:15:58,125][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00010 | loss1:0.0001 loss2:0.0362 loss3:0.0005 | AUC:0.8536 Anomaly AUC:0.6920
[2023-09-09 14:16:17,157][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00010 | loss1:0.0001 loss2:0.0343 loss3:0.0005 | AUC:0.8537 Anomaly AUC:0.6930
[2023-09-09 14:16:36,219][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00010 | loss1:0.0001 loss2:0.0329 loss3:0.0005 | AUC:0.8535 Anomaly AUC:0.6930
[2023-09-09 14:16:55,493][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00010 | loss1:0.0001 loss2:0.0317 loss3:0.0005 | AUC:0.8531 Anomaly AUC:0.6908
[2023-09-09 14:17:14,667][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00010 | loss1:0.0001 loss2:0.0299 loss3:0.0005 | AUC:0.8538 Anomaly AUC:0.6910
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 39, in forward
    x_k_split = self.DR_DMU(x_split)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 64, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 76, in forward
    x = attn(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 14, in forward
    return self.fn(self.norm(x), **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 55, in forward
    tmp_ones = torch.ones(n).cuda()
KeyboardInterrupt