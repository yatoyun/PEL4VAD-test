
8000 8100 2900
[2023-09-09 14:27:24,514][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 14:27:24,515][main.py][line:168][INFO] Training Mode
[2023-09-09 14:27:24,515][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 14:27:24,515][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 14:27:32,565][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 14:27:51,061][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.4523 loss2:1.1476 loss3:0.2664 | AUC:0.8081 Anomaly AUC:0.6499
[2023-09-09 14:28:09,221][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.1908 loss2:0.8474 loss3:0.1376 | AUC:0.8139 Anomaly AUC:0.6760
[2023-09-09 14:28:27,698][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0768 loss2:0.7486 loss3:0.0629 | AUC:0.8248 Anomaly AUC:0.6739
[2023-09-09 14:28:46,349][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0366 loss2:0.6794 loss3:0.0307 | AUC:0.8376 Anomaly AUC:0.6799
[2023-09-09 14:29:05,050][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0262 loss2:0.6256 loss3:0.0205 | AUC:0.8414 Anomaly AUC:0.6791
[2023-09-09 14:29:23,799][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0105 loss2:0.5629 loss3:0.0149 | AUC:0.8395 Anomaly AUC:0.6763
[2023-09-09 14:29:42,756][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0083 loss2:0.5136 loss3:0.0117 | AUC:0.8383 Anomaly AUC:0.6768
[2023-09-09 14:30:01,789][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0046 loss2:0.4616 loss3:0.0088 | AUC:0.8381 Anomaly AUC:0.6731
[2023-09-09 14:30:20,684][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0210 loss2:0.4350 loss3:0.0083 | AUC:0.8330 Anomaly AUC:0.6648
[2023-09-09 14:30:39,591][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0181 loss2:0.4095 loss3:0.0054 | AUC:0.8339 Anomaly AUC:0.6784
[2023-09-09 14:30:58,564][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0026 loss2:0.3506 loss3:0.0033 | AUC:0.8404 Anomaly AUC:0.6839
[2023-09-09 14:31:17,540][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0048 loss2:0.3280 loss3:0.0037 | AUC:0.8325 Anomaly AUC:0.6700
[2023-09-09 14:31:36,418][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0024 loss2:0.2959 loss3:0.0032 | AUC:0.8366 Anomaly AUC:0.6765
[2023-09-09 14:31:55,382][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0013 loss2:0.2705 loss3:0.0029 | AUC:0.8389 Anomaly AUC:0.6820
[2023-09-09 14:32:14,426][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0011 loss2:0.2496 loss3:0.0028 | AUC:0.8392 Anomaly AUC:0.6823
[2023-09-09 14:32:33,360][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0010 loss2:0.2319 loss3:0.0027 | AUC:0.8396 Anomaly AUC:0.6844
[2023-09-09 14:32:52,290][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0008 loss2:0.2155 loss3:0.0024 | AUC:0.8356 Anomaly AUC:0.6797
[2023-09-09 14:33:11,246][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0008 loss2:0.2014 loss3:0.0023 | AUC:0.8317 Anomaly AUC:0.6786
[2023-09-09 14:33:30,204][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0291 loss2:0.2246 loss3:0.0068 | AUC:0.8249 Anomaly AUC:0.6595
[2023-09-09 14:33:49,097][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0140 loss2:0.2063 loss3:0.0051 | AUC:0.8230 Anomaly AUC:0.6541
[2023-09-09 14:34:08,042][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0047 loss2:0.1852 loss3:0.0038 | AUC:0.8352 Anomaly AUC:0.6763
[2023-09-09 14:34:27,005][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0015 loss2:0.1705 loss3:0.0030 | AUC:0.8305 Anomaly AUC:0.6717
[2023-09-09 14:34:45,971][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0036 loss2:0.1673 loss3:0.0034 | AUC:0.8234 Anomaly AUC:0.6775
[2023-09-09 14:35:04,946][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0128 loss2:0.1728 loss3:0.0046 | AUC:0.8359 Anomaly AUC:0.6862
[2023-09-09 14:35:24,119][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0061 loss2:0.1578 loss3:0.0038 | AUC:0.8216 Anomaly AUC:0.6647
[2023-09-09 14:35:43,138][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0023 loss2:0.1457 loss3:0.0026 | AUC:0.8312 Anomaly AUC:0.6688
[2023-09-09 14:36:02,145][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0005 loss2:0.1353 loss3:0.0021 | AUC:0.8383 Anomaly AUC:0.6764
[2023-09-09 14:36:21,133][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0004 loss2:0.1281 loss3:0.0019 | AUC:0.8386 Anomaly AUC:0.6776
[2023-09-09 14:36:40,108][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0004 loss2:0.1230 loss3:0.0017 | AUC:0.8408 Anomaly AUC:0.6790
[2023-09-09 14:36:59,155][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0004 loss2:0.1182 loss3:0.0016 | AUC:0.8410 Anomaly AUC:0.6794
[2023-09-09 14:37:18,129][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0003 loss2:0.1134 loss3:0.0014 | AUC:0.8406 Anomaly AUC:0.6788
[2023-09-09 14:37:37,143][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0003 loss2:0.1099 loss3:0.0013 | AUC:0.8415 Anomaly AUC:0.6802
[2023-09-09 14:37:56,156][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0003 loss2:0.1043 loss3:0.0012 | AUC:0.8391 Anomaly AUC:0.6768
[2023-09-09 14:38:15,264][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0003 loss2:0.1011 loss3:0.0011 | AUC:0.8407 Anomaly AUC:0.6787
[2023-09-09 14:38:34,374][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0003 loss2:0.0968 loss3:0.0010 | AUC:0.8409 Anomaly AUC:0.6787
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 40, in forward
    x_k = torch.cat((x_k, x_k_split["x"]), 0)
KeyboardInterrupt