
8000 8100 2900
[2023-09-09 14:39:01,311][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 14:39:01,311][main.py][line:168][INFO] Training Mode
[2023-09-09 14:39:01,312][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 14:39:01,312][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 14:39:09,375][main.py][line:82][INFO] Random initialize AUCAUC:0.6179 Anomaly AUC:0.52117
[2023-09-09 14:39:28,006][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.4509 loss2:1.1490 loss3:0.2908 | AUC:0.8134 Anomaly AUC:0.6486
[2023-09-09 14:39:46,404][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.1869 loss2:0.8464 loss3:0.2333 | AUC:0.8388 Anomaly AUC:0.6900
[2023-09-09 14:40:05,090][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0662 loss2:0.7416 loss3:0.1819 | AUC:0.8391 Anomaly AUC:0.6851
[2023-09-09 14:40:23,980][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0276 loss2:0.6707 loss3:0.1490 | AUC:0.8463 Anomaly AUC:0.6728
[2023-09-09 14:40:42,781][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0257 loss2:0.6202 loss3:0.1288 | AUC:0.8335 Anomaly AUC:0.6757
[2023-09-09 14:41:01,695][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0171 loss2:0.5642 loss3:0.1187 | AUC:0.8387 Anomaly AUC:0.6741
[2023-09-09 14:41:20,576][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0200 loss2:0.5242 loss3:0.1143 | AUC:0.8441 Anomaly AUC:0.6748
[2023-09-09 14:41:39,509][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0082 loss2:0.4738 loss3:0.1111 | AUC:0.8433 Anomaly AUC:0.6732
[2023-09-09 14:41:58,553][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0040 loss2:0.4296 loss3:0.1087 | AUC:0.8483 Anomaly AUC:0.6786
[2023-09-09 14:42:17,437][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0032 loss2:0.3929 loss3:0.1054 | AUC:0.8459 Anomaly AUC:0.6709
[2023-09-09 14:42:36,349][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0026 loss2:0.3577 loss3:0.0971 | AUC:0.8471 Anomaly AUC:0.6716
[2023-09-09 14:42:55,407][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0078 loss2:0.3451 loss3:0.0386 | AUC:0.8372 Anomaly AUC:0.6657
[2023-09-09 14:43:14,435][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0114 loss2:0.3302 loss3:0.0154 | AUC:0.8364 Anomaly AUC:0.6659
[2023-09-09 14:43:33,402][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0088 loss2:0.3055 loss3:0.0122 | AUC:0.8275 Anomaly AUC:0.6658
[2023-09-09 14:43:52,519][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0270 loss2:0.3104 loss3:0.0117 | AUC:0.7912 Anomaly AUC:0.6352
[2023-09-09 14:44:11,514][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0057 loss2:0.2753 loss3:0.0087 | AUC:0.8223 Anomaly AUC:0.6649
[2023-09-09 14:44:30,668][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0010 loss2:0.2460 loss3:0.0067 | AUC:0.8213 Anomaly AUC:0.6577
[2023-09-09 14:44:49,706][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0008 loss2:0.2283 loss3:0.0050 | AUC:0.8159 Anomaly AUC:0.6573
[2023-09-09 14:45:08,737][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0007 loss2:0.2131 loss3:0.0031 | AUC:0.8152 Anomaly AUC:0.6502
[2023-09-09 14:45:27,897][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0006 loss2:0.2000 loss3:0.0010 | AUC:0.8188 Anomaly AUC:0.6510
[2023-09-09 14:45:46,965][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0005 loss2:0.1895 loss3:0.0007 | AUC:0.8211 Anomaly AUC:0.6595
[2023-09-09 14:46:06,081][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0005 loss2:0.1799 loss3:0.0006 | AUC:0.8167 Anomaly AUC:0.6505
[2023-09-09 14:46:25,080][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0005 loss2:0.1708 loss3:0.0005 | AUC:0.8097 Anomaly AUC:0.6433
[2023-09-09 14:46:44,130][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0004 loss2:0.1628 loss3:0.0004 | AUC:0.8178 Anomaly AUC:0.6511
[2023-09-09 14:47:03,179][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0004 loss2:0.1550 loss3:0.0004 | AUC:0.8129 Anomaly AUC:0.6421
[2023-09-09 14:47:22,237][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0003 loss2:0.1480 loss3:0.0005 | AUC:0.8162 Anomaly AUC:0.6471
[2023-09-09 14:47:41,187][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0003 loss2:0.1417 loss3:0.0005 | AUC:0.8099 Anomaly AUC:0.6455
[2023-09-09 14:48:00,195][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0003 loss2:0.1345 loss3:0.0005 | AUC:0.8166 Anomaly AUC:0.6495
[2023-09-09 14:48:19,159][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0003 loss2:0.1287 loss3:0.0004 | AUC:0.8115 Anomaly AUC:0.6406
[2023-09-09 14:48:38,226][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0003 loss2:0.1231 loss3:0.0005 | AUC:0.8147 Anomaly AUC:0.6392
[2023-09-09 14:48:57,231][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0003 loss2:0.1175 loss3:0.0005 | AUC:0.8171 Anomaly AUC:0.6443
[2023-09-09 14:49:16,298][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0108 loss2:0.1257 loss3:0.0012 | AUC:0.7809 Anomaly AUC:0.6265
[2023-09-09 14:49:35,406][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0444 loss2:0.1882 loss3:0.0062 | AUC:0.8157 Anomaly AUC:0.6577
[2023-09-09 14:49:54,573][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0081 loss2:0.1280 loss3:0.0018 | AUC:0.8268 Anomaly AUC:0.6434
[2023-09-09 14:50:13,735][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0035 loss2:0.1127 loss3:0.0012 | AUC:0.8217 Anomaly AUC:0.6342
[2023-09-09 14:50:32,873][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0099 loss2:0.1181 loss3:0.0015 | AUC:0.8266 Anomaly AUC:0.6345
[2023-09-09 14:50:51,989][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0023 loss2:0.1020 loss3:0.0009 | AUC:0.8227 Anomaly AUC:0.6422
[2023-09-09 14:51:11,158][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0011 loss2:0.0953 loss3:0.0006 | AUC:0.8228 Anomaly AUC:0.6407
[2023-09-09 14:51:30,268][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0003 loss2:0.0879 loss3:0.0005 | AUC:0.8193 Anomaly AUC:0.6388
[2023-09-09 14:51:49,396][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0003 loss2:0.0836 loss3:0.0005 | AUC:0.8199 Anomaly AUC:0.6408
[2023-09-09 14:52:08,491][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0004 loss2:0.0801 loss3:0.0005 | AUC:0.8135 Anomaly AUC:0.6319
[2023-09-09 14:52:27,554][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0003 loss2:0.0764 loss3:0.0005 | AUC:0.8126 Anomaly AUC:0.6323
[2023-09-09 14:52:46,728][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0002 loss2:0.0736 loss3:0.0004 | AUC:0.8122 Anomaly AUC:0.6264
[2023-09-09 14:53:06,021][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0002 loss2:0.0699 loss3:0.0004 | AUC:0.8092 Anomaly AUC:0.6251
[2023-09-09 14:53:25,178][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0002 loss2:0.0671 loss3:0.0004 | AUC:0.8111 Anomaly AUC:0.6277
[2023-09-09 14:53:44,373][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0002 loss2:0.0646 loss3:0.0004 | AUC:0.8121 Anomaly AUC:0.6280
[2023-09-09 14:54:03,559][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0002 loss2:0.0614 loss3:0.0004 | AUC:0.8114 Anomaly AUC:0.6296
[2023-09-09 14:54:22,645][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0002 loss2:0.0601 loss3:0.0004 | AUC:0.8166 Anomaly AUC:0.6330
[2023-09-09 14:54:41,820][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0002 loss2:0.0567 loss3:0.0004 | AUC:0.8139 Anomaly AUC:0.6318
[2023-09-09 14:55:00,984][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0001 loss2:0.0540 loss3:0.0004 | AUC:0.8141 Anomaly AUC:0.6288
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 23, in train_func
    seq_len = torch.sum(torch.max(torch.abs(v_input), dim=2)[0] > 0, 1)
KeyboardInterrupt