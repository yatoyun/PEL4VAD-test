
8000 8100 2900
[2023-09-09 14:56:39,206][main.py][line:165][INFO] total params:7.5605M
[2023-09-09 14:56:39,206][main.py][line:168][INFO] Training Mode
[2023-09-09 14:56:39,206][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 14:56:39,207][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 14:56:47,382][main.py][line:82][INFO] Random initialize AUCAUC:0.4858 Anomaly AUC:0.49013
[2023-09-09 14:57:05,931][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.4901 loss2:1.1818 loss3:0.3479 | AUC:0.8332 Anomaly AUC:0.6601
[2023-09-09 14:57:24,137][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.2145 loss2:0.8773 loss3:0.2274 | AUC:0.8487 Anomaly AUC:0.6853
[2023-09-09 14:57:42,665][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0845 loss2:0.7646 loss3:0.1348 | AUC:0.8491 Anomaly AUC:0.6756
[2023-09-09 14:58:01,388][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0404 loss2:0.6936 loss3:0.0661 | AUC:0.8454 Anomaly AUC:0.6825
[2023-09-09 14:58:20,215][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0268 loss2:0.6379 loss3:0.0350 | AUC:0.8212 Anomaly AUC:0.6568
[2023-09-09 14:58:39,089][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0150 loss2:0.5801 loss3:0.0217 | AUC:0.8311 Anomaly AUC:0.6698
[2023-09-09 14:58:58,040][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0066 loss2:0.5258 loss3:0.0149 | AUC:0.8458 Anomaly AUC:0.6779
[2023-09-09 14:59:17,274][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0037 loss2:0.4733 loss3:0.0104 | AUC:0.8532 Anomaly AUC:0.6921
[2023-09-09 14:59:36,342][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0036 loss2:0.4281 loss3:0.0073 | AUC:0.8532 Anomaly AUC:0.6842
[2023-09-09 14:59:55,291][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0047 loss2:0.3911 loss3:0.0048 | AUC:0.8339 Anomaly AUC:0.6737
[2023-09-09 15:00:14,419][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0243 loss2:0.3830 loss3:0.0067 | AUC:0.8433 Anomaly AUC:0.6899
[2023-09-09 15:00:33,584][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0150 loss2:0.3503 loss3:0.0056 | AUC:0.8551 Anomaly AUC:0.6981
[2023-09-09 15:00:52,693][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0022 loss2:0.3089 loss3:0.0030 | AUC:0.8553 Anomaly AUC:0.6971
[2023-09-09 15:01:11,719][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0011 loss2:0.2839 loss3:0.0025 | AUC:0.8509 Anomaly AUC:0.6919
[2023-09-09 15:01:30,808][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0010 loss2:0.2616 loss3:0.0022 | AUC:0.8515 Anomaly AUC:0.6916
[2023-09-09 15:01:49,816][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0008 loss2:0.2441 loss3:0.0020 | AUC:0.8563 Anomaly AUC:0.6956
[2023-09-09 15:02:08,825][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0007 loss2:0.2278 loss3:0.0019 | AUC:0.8542 Anomaly AUC:0.6942
[2023-09-09 15:02:27,867][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0007 loss2:0.2128 loss3:0.0018 | AUC:0.8545 Anomaly AUC:0.6905
[2023-09-09 15:02:46,918][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0006 loss2:0.2013 loss3:0.0017 | AUC:0.8530 Anomaly AUC:0.6896
[2023-09-09 15:03:05,935][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0006 loss2:0.1896 loss3:0.0016 | AUC:0.8559 Anomaly AUC:0.6934
[2023-09-09 15:03:25,047][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0005 loss2:0.1791 loss3:0.0015 | AUC:0.8535 Anomaly AUC:0.6903
[2023-09-09 15:03:44,158][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0005 loss2:0.1716 loss3:0.0014 | AUC:0.8578 Anomaly AUC:0.6904
[2023-09-09 15:04:03,224][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0005 loss2:0.1645 loss3:0.0013 | AUC:0.8491 Anomaly AUC:0.6803
[2023-09-09 15:04:22,299][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0019 loss2:0.1615 loss3:0.0017 | AUC:0.8318 Anomaly AUC:0.6883
[2023-09-09 15:04:41,462][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0851 loss2:0.2784 loss3:0.0145 | AUC:0.8346 Anomaly AUC:0.6650
[2023-09-09 15:05:00,552][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0023 loss2:0.1632 loss3:0.0024 | AUC:0.8443 Anomaly AUC:0.6767
[2023-09-09 15:05:19,655][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0008 loss2:0.1468 loss3:0.0016 | AUC:0.8448 Anomaly AUC:0.6745
[2023-09-09 15:05:38,765][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0007 loss2:0.1387 loss3:0.0013 | AUC:0.8454 Anomaly AUC:0.6754
[2023-09-09 15:05:57,899][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0006 loss2:0.1324 loss3:0.0011 | AUC:0.8453 Anomaly AUC:0.6749
[2023-09-09 15:06:17,101][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0005 loss2:0.1274 loss3:0.0010 | AUC:0.8466 Anomaly AUC:0.6769
[2023-09-09 15:06:36,205][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0004 loss2:0.1215 loss3:0.0010 | AUC:0.8456 Anomaly AUC:0.6752
[2023-09-09 15:06:55,355][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0004 loss2:0.1167 loss3:0.0009 | AUC:0.8482 Anomaly AUC:0.6794
[2023-09-09 15:07:14,481][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0004 loss2:0.1117 loss3:0.0008 | AUC:0.8487 Anomaly AUC:0.6783
[2023-09-09 15:07:33,676][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0003 loss2:0.1080 loss3:0.0008 | AUC:0.8465 Anomaly AUC:0.6763
[2023-09-09 15:07:52,760][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0003 loss2:0.1041 loss3:0.0008 | AUC:0.8483 Anomaly AUC:0.6776
[2023-09-09 15:08:12,000][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0003 loss2:0.0992 loss3:0.0008 | AUC:0.8491 Anomaly AUC:0.6798
[2023-09-09 15:08:31,266][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0003 loss2:0.0963 loss3:0.0007 | AUC:0.8483 Anomaly AUC:0.6777
[2023-09-09 15:08:50,505][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0002 loss2:0.0928 loss3:0.0007 | AUC:0.8505 Anomaly AUC:0.6782
[2023-09-09 15:09:09,619][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0002 loss2:0.0885 loss3:0.0007 | AUC:0.8502 Anomaly AUC:0.6777
[2023-09-09 15:09:28,785][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0002 loss2:0.0854 loss3:0.0007 | AUC:0.8513 Anomaly AUC:0.6798
[2023-09-09 15:09:48,029][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0002 loss2:0.0819 loss3:0.0007 | AUC:0.8511 Anomaly AUC:0.6796
[2023-09-09 15:10:07,270][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0002 loss2:0.0789 loss3:0.0007 | AUC:0.8522 Anomaly AUC:0.6790
[2023-09-09 15:10:26,367][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0002 loss2:0.0761 loss3:0.0006 | AUC:0.8501 Anomaly AUC:0.6764
[2023-09-09 15:10:45,496][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0701 loss2:0.1483 loss3:0.0104 | AUC:0.8164 Anomaly AUC:0.6406
[2023-09-09 15:11:04,682][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0135 loss2:0.1119 loss3:0.0051 | AUC:0.8319 Anomaly AUC:0.6738
[2023-09-09 15:11:24,024][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0036 loss2:0.0796 loss3:0.0022 | AUC:0.8425 Anomaly AUC:0.6846
[2023-09-09 15:11:43,287][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0005 loss2:0.0691 loss3:0.0011 | AUC:0.8402 Anomaly AUC:0.6815
[2023-09-09 15:12:02,621][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0004 loss2:0.0653 loss3:0.0009 | AUC:0.8388 Anomaly AUC:0.6784
[2023-09-09 15:12:21,855][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0004 loss2:0.0619 loss3:0.0008 | AUC:0.8407 Anomaly AUC:0.6807
[2023-09-09 15:12:41,048][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0003 loss2:0.0591 loss3:0.0007 | AUC:0.8418 Anomaly AUC:0.6817
[2023-09-09 15:13:00,268][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0003 loss2:0.0567 loss3:0.0007 | AUC:0.8434 Anomaly AUC:0.6815
[2023-09-09 15:13:19,467][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0003 loss2:0.0540 loss3:0.0007 | AUC:0.8427 Anomaly AUC:0.6804
[2023-09-09 15:13:38,620][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00010 | loss1:0.0002 loss2:0.0519 loss3:0.0006 | AUC:0.8439 Anomaly AUC:0.6817
[2023-09-09 15:13:57,916][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00010 | loss1:0.0002 loss2:0.0494 loss3:0.0006 | AUC:0.8442 Anomaly AUC:0.6816
[2023-09-09 15:14:17,179][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00010 | loss1:0.0002 loss2:0.0475 loss3:0.0006 | AUC:0.8457 Anomaly AUC:0.6818
[2023-09-09 15:14:36,402][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00010 | loss1:0.0002 loss2:0.0460 loss3:0.0006 | AUC:0.8452 Anomaly AUC:0.6792
[2023-09-09 15:14:55,599][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00010 | loss1:0.0002 loss2:0.0435 loss3:0.0006 | AUC:0.8470 Anomaly AUC:0.6813
[2023-09-09 15:15:14,756][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00010 | loss1:0.0002 loss2:0.0419 loss3:0.0006 | AUC:0.8476 Anomaly AUC:0.6825
[2023-09-09 15:15:34,032][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00010 | loss1:0.0001 loss2:0.0401 loss3:0.0006 | AUC:0.8472 Anomaly AUC:0.6797
[2023-09-09 15:15:53,209][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00010 | loss1:0.0001 loss2:0.0384 loss3:0.0006 | AUC:0.8490 Anomaly AUC:0.6830
[2023-09-09 15:16:12,474][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00010 | loss1:0.0001 loss2:0.0368 loss3:0.0006 | AUC:0.8489 Anomaly AUC:0.6817
[2023-09-09 15:16:31,678][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00010 | loss1:0.0001 loss2:0.0351 loss3:0.0005 | AUC:0.8493 Anomaly AUC:0.6817
[2023-09-09 15:16:50,944][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00010 | loss1:0.0001 loss2:0.0338 loss3:0.0005 | AUC:0.8490 Anomaly AUC:0.6801
[2023-09-09 15:17:10,149][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00010 | loss1:0.0001 loss2:0.0322 loss3:0.0005 | AUC:0.8496 Anomaly AUC:0.6812
[2023-09-09 15:17:29,497][main.py][line:106][INFO] [Epoch:65/100]: lr:0.00010 | loss1:0.0001 loss2:0.0311 loss3:0.0005 | AUC:0.8490 Anomaly AUC:0.6796
[2023-09-09 15:17:48,698][main.py][line:106][INFO] [Epoch:66/100]: lr:0.00010 | loss1:0.0001 loss2:0.0295 loss3:0.0005 | AUC:0.8494 Anomaly AUC:0.6803
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 43, in train_func
    loss1 = CLAS2(logits, label, seq_len, criterion)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/loss.py", line 13, in CLAS2
    tmp, _ = torch.topk(logits[i][:seq_len[i]], k=1, largest=True)
KeyboardInterrupt