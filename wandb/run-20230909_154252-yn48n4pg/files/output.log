[2023-09-09 15:42:54,751][main.py][line:165][INFO] total params:7.5605M
[2023-09-09 15:42:54,751][main.py][line:168][INFO] Training Mode
[2023-09-09 15:42:54,751][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 15:42:54,751][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
8000 8100 2900
[2023-09-09 15:43:02,885][main.py][line:82][INFO] Random initialize AUCAUC:0.5550 Anomaly AUC:0.47816
[2023-09-09 15:43:21,351][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.5489 loss2:1.2543 loss3:0.3635 | AUC:0.8383 Anomaly AUC:0.6516
[2023-09-09 15:43:39,487][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.2966 loss2:0.9399 loss3:0.2611 | AUC:0.8530 Anomaly AUC:0.6858
[2023-09-09 15:43:57,956][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.1850 loss2:0.8289 loss3:0.1958 | AUC:0.8512 Anomaly AUC:0.6861
[2023-09-09 15:44:16,717][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.1093 loss2:0.7560 loss3:0.1375 | AUC:0.8543 Anomaly AUC:0.6844
[2023-09-09 15:44:35,456][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0740 loss2:0.7014 loss3:0.0879 | AUC:0.8471 Anomaly AUC:0.6760
[2023-09-09 15:44:54,294][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0347 loss2:0.6405 loss3:0.0530 | AUC:0.8545 Anomaly AUC:0.6873
[2023-09-09 15:45:13,054][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0342 loss2:0.6033 loss3:0.0355 | AUC:0.8435 Anomaly AUC:0.6744
[2023-09-09 15:45:32,067][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0226 loss2:0.5617 loss3:0.0258 | AUC:0.8500 Anomaly AUC:0.6727
[2023-09-09 15:45:51,036][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0075 loss2:0.5079 loss3:0.0175 | AUC:0.8569 Anomaly AUC:0.6888
[2023-09-09 15:46:09,966][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0040 loss2:0.4624 loss3:0.0132 | AUC:0.8526 Anomaly AUC:0.6792
[2023-09-09 15:46:28,741][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0027 loss2:0.4210 loss3:0.0104 | AUC:0.8565 Anomaly AUC:0.6786
[2023-09-09 15:46:47,712][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0023 loss2:0.3803 loss3:0.0081 | AUC:0.8571 Anomaly AUC:0.6778
[2023-09-09 15:47:06,563][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0020 loss2:0.3447 loss3:0.0056 | AUC:0.8574 Anomaly AUC:0.6757
[2023-09-09 15:47:25,496][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0015 loss2:0.3148 loss3:0.0034 | AUC:0.8580 Anomaly AUC:0.6748
[2023-09-09 15:47:44,355][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0014 loss2:0.2861 loss3:0.0028 | AUC:0.8495 Anomaly AUC:0.6601
[2023-09-09 15:48:03,302][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0480 loss2:0.3365 loss3:0.0089 | AUC:0.8443 Anomaly AUC:0.6627
[2023-09-09 15:48:22,228][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0508 loss2:0.3536 loss3:0.0102 | AUC:0.8418 Anomaly AUC:0.6648
[2023-09-09 15:48:41,185][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0064 loss2:0.2598 loss3:0.0035 | AUC:0.8376 Anomaly AUC:0.6583
[2023-09-09 15:49:00,103][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0154 loss2:0.2631 loss3:0.0048 | AUC:0.8439 Anomaly AUC:0.6465
[2023-09-09 15:49:18,897][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0108 loss2:0.2361 loss3:0.0037 | AUC:0.8407 Anomaly AUC:0.6471
[2023-09-09 15:49:37,790][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0011 loss2:0.1988 loss3:0.0021 | AUC:0.8419 Anomaly AUC:0.6483
[2023-09-09 15:49:56,610][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0009 loss2:0.1850 loss3:0.0019 | AUC:0.8406 Anomaly AUC:0.6446
[2023-09-09 15:50:15,446][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0008 loss2:0.1732 loss3:0.0017 | AUC:0.8421 Anomaly AUC:0.6475
[2023-09-09 15:50:34,359][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0007 loss2:0.1626 loss3:0.0015 | AUC:0.8397 Anomaly AUC:0.6436
[2023-09-09 15:50:53,224][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0007 loss2:0.1525 loss3:0.0014 | AUC:0.8411 Anomaly AUC:0.6442
[2023-09-09 15:51:12,098][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0006 loss2:0.1439 loss3:0.0013 | AUC:0.8398 Anomaly AUC:0.6459
[2023-09-09 15:51:31,031][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0005 loss2:0.1368 loss3:0.0013 | AUC:0.8389 Anomaly AUC:0.6430
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 30, in train_func
    logits, x_k, output_MSNSD = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 33, in forward
    x_k = self.DR_DMU(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 96, in forward
    A_Naug = self.encoder_mu(A_Naug)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt