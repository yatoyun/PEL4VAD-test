
8000 8100 2900
[2023-09-09 15:51:57,222][main.py][line:165][INFO] total params:7.5605M
[2023-09-09 15:51:57,223][main.py][line:168][INFO] Training Mode
[2023-09-09 15:51:57,223][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.5, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)
[2023-09-09 15:51:57,223][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 15:52:05,425][main.py][line:82][INFO] Random initialize AUCAUC:0.5550 Anomaly AUC:0.47816
[2023-09-09 15:52:23,906][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.7732 loss2:1.3563 loss3:0.3815 | AUC:0.8165 Anomaly AUC:0.6031
[2023-09-09 15:52:42,302][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.3803 loss2:1.0982 loss3:0.3043 | AUC:0.8443 Anomaly AUC:0.6654
[2023-09-09 15:53:01,096][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.2694 loss2:0.9170 loss3:0.2145 | AUC:0.8511 Anomaly AUC:0.6743
[2023-09-09 15:53:19,704][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.1790 loss2:0.8334 loss3:0.1487 | AUC:0.8476 Anomaly AUC:0.6709
[2023-09-09 15:53:38,565][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.1088 loss2:0.7673 loss3:0.0947 | AUC:0.8465 Anomaly AUC:0.6652
[2023-09-09 15:53:57,496][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0542 loss2:0.7069 loss3:0.0571 | AUC:0.8517 Anomaly AUC:0.6715
[2023-09-09 15:54:16,360][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0687 loss2:0.6852 loss3:0.0409 | AUC:0.8487 Anomaly AUC:0.6653
[2023-09-09 15:54:35,330][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0156 loss2:0.6221 loss3:0.0240 | AUC:0.8482 Anomaly AUC:0.6706
[2023-09-09 15:54:54,452][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0167 loss2:0.5853 loss3:0.0186 | AUC:0.8475 Anomaly AUC:0.6669
[2023-09-09 15:55:13,370][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0347 loss2:0.5689 loss3:0.0173 | AUC:0.8430 Anomaly AUC:0.6634
[2023-09-09 15:55:32,519][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0146 loss2:0.5229 loss3:0.0115 | AUC:0.8407 Anomaly AUC:0.6747
[2023-09-09 15:55:51,643][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0241 loss2:0.5026 loss3:0.0094 | AUC:0.8546 Anomaly AUC:0.6660
[2023-09-09 15:56:10,706][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0217 loss2:0.4793 loss3:0.0074 | AUC:0.8537 Anomaly AUC:0.6614
[2023-09-09 15:56:29,882][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0026 loss2:0.4175 loss3:0.0041 | AUC:0.8528 Anomaly AUC:0.6633
[2023-09-09 15:56:48,923][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0026 loss2:0.3829 loss3:0.0039 | AUC:0.8497 Anomaly AUC:0.6624
[2023-09-09 15:57:07,917][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0020 loss2:0.3518 loss3:0.0035 | AUC:0.8515 Anomaly AUC:0.6615
[2023-09-09 15:57:27,063][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0019 loss2:0.3188 loss3:0.0033 | AUC:0.8502 Anomaly AUC:0.6574
[2023-09-09 15:57:46,062][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0017 loss2:0.2909 loss3:0.0032 | AUC:0.8502 Anomaly AUC:0.6547
[2023-09-09 15:58:05,136][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0016 loss2:0.2681 loss3:0.0030 | AUC:0.8487 Anomaly AUC:0.6548
[2023-09-09 15:58:24,159][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0015 loss2:0.2420 loss3:0.0028 | AUC:0.8476 Anomaly AUC:0.6549
[2023-09-09 15:58:43,139][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0014 loss2:0.2217 loss3:0.0027 | AUC:0.8491 Anomaly AUC:0.6593
[2023-09-09 15:59:02,174][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0033 loss2:0.2126 loss3:0.0028 | AUC:0.8420 Anomaly AUC:0.6386
[2023-09-09 15:59:21,207][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0659 loss2:0.3155 loss3:0.0119 | AUC:0.8540 Anomaly AUC:0.6637
[2023-09-09 15:59:40,166][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0018 loss2:0.1979 loss3:0.0028 | AUC:0.8432 Anomaly AUC:0.6481
[2023-09-09 15:59:59,257][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0014 loss2:0.1779 loss3:0.0022 | AUC:0.8444 Anomaly AUC:0.6508
[2023-09-09 16:00:18,407][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0012 loss2:0.1655 loss3:0.0020 | AUC:0.8402 Anomaly AUC:0.6466
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 37, in train_func
    video_feat, token_feat, video_labels = get_cas(v_feat, t_input, logits, multi_label)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/utils.py", line 105, in get_cas
    bg_label = torch.tensor([0]).cuda()
KeyboardInterrupt