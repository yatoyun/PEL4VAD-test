[2023-09-09 16:09:33,805][main.py][line:165][INFO] total params:7.5605M
[2023-09-09 16:09:33,805][main.py][line:168][INFO] Training Mode
[2023-09-09 16:09:33,805][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.5, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)
[2023-09-09 16:09:33,806][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0005
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
8000 8100 2900
[2023-09-09 16:09:41,886][main.py][line:82][INFO] Random initialize AUCAUC:0.5550 Anomaly AUC:0.47816
[2023-09-09 16:10:00,202][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:24.9849 loss2:1.4053 loss3:0.4150 | AUC:0.5764 Anomaly AUC:0.5424
[2023-09-09 16:10:18,548][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:15.2751 loss2:1.3992 loss3:0.3825 | AUC:0.6492 Anomaly AUC:0.5948
[2023-09-09 16:10:37,179][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:12.5146 loss2:1.3983 loss3:0.3590 | AUC:0.7420 Anomaly AUC:0.5570
[2023-09-09 16:10:56,002][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:12.0236 loss2:1.3989 loss3:0.3308 | AUC:0.5944 Anomaly AUC:0.5838
[2023-09-09 16:11:14,784][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:13.9748 loss2:1.4016 loss3:0.3079 | AUC:0.6682 Anomaly AUC:0.5674
[2023-09-09 16:11:33,569][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:10.5040 loss2:1.4027 loss3:0.2776 | AUC:0.7060 Anomaly AUC:0.5642
[2023-09-09 16:11:52,461][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00050 | loss1:9.9657 loss2:1.4029 loss3:0.2457 | AUC:0.7214 Anomaly AUC:0.5720
[2023-09-09 16:12:11,470][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00050 | loss1:10.3849 loss2:1.4004 loss3:0.2546 | AUC:0.7058 Anomaly AUC:0.5488
[2023-09-09 16:12:30,306][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00050 | loss1:8.4261 loss2:1.3954 loss3:0.2257 | AUC:0.7508 Anomaly AUC:0.5537
[2023-09-09 16:12:49,350][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00050 | loss1:7.1663 loss2:1.3887 loss3:0.2178 | AUC:0.7183 Anomaly AUC:0.5338
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 58, in train_func
    loss.backward()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt