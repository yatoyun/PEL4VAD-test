[2023-09-09 16:13:07,780][main.py][line:165][INFO] total params:7.5605M
[2023-09-09 16:13:07,781][main.py][line:168][INFO] Training Mode
[2023-09-09 16:13:07,781][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.5, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.5, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.5, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.5, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
)
[2023-09-09 16:13:07,781][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0005
    lr: 0.0005
    maximize: False
    weight_decay: 5e-05
)
8000 8100 2900
[2023-09-09 16:13:15,769][main.py][line:82][INFO] Random initialize AUCAUC:0.5550 Anomaly AUC:0.47816
[2023-09-09 16:13:34,115][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00050 | loss1:1.4510 loss2:1.3836 loss3:0.3743 | AUC:0.7751 Anomaly AUC:0.5804
[2023-09-09 16:13:52,372][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00050 | loss1:0.9677 loss2:1.2492 loss3:0.2886 | AUC:0.7956 Anomaly AUC:0.5871
[2023-09-09 16:14:11,060][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00050 | loss1:0.6573 loss2:1.1661 loss3:0.1415 | AUC:0.8167 Anomaly AUC:0.6102
[2023-09-09 16:14:29,640][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00050 | loss1:0.5304 loss2:1.1294 loss3:0.0805 | AUC:0.8002 Anomaly AUC:0.5759
[2023-09-09 16:14:48,375][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00050 | loss1:0.4119 loss2:1.1034 loss3:0.0413 | AUC:0.8050 Anomaly AUC:0.5669
[2023-09-09 16:15:07,197][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00050 | loss1:0.3137 loss2:1.0764 loss3:0.0257 | AUC:0.7713 Anomaly AUC:0.5661
[2023-09-09 16:15:26,078][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00050 | loss1:0.2533 loss2:1.0567 loss3:0.0197 | AUC:0.7833 Anomaly AUC:0.5578
[2023-09-09 16:15:44,987][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00050 | loss1:0.1839 loss2:1.0379 loss3:0.0135 | AUC:0.7892 Anomaly AUC:0.5655
[2023-09-09 16:16:04,082][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00050 | loss1:0.1378 loss2:1.0203 loss3:0.0096 | AUC:0.8168 Anomaly AUC:0.5799
[2023-09-09 16:16:22,859][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00050 | loss1:0.1116 loss2:1.0076 loss3:0.0072 | AUC:0.8013 Anomaly AUC:0.5610
[2023-09-09 16:16:41,810][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00050 | loss1:0.1098 loss2:1.0013 loss3:0.0073 | AUC:0.7791 Anomaly AUC:0.5356
[2023-09-09 16:17:00,714][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00050 | loss1:0.0665 loss2:0.9868 loss3:0.0041 | AUC:0.7600 Anomaly AUC:0.5304
[2023-09-09 16:17:19,567][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00050 | loss1:0.0750 loss2:0.9825 loss3:0.0049 | AUC:0.7537 Anomaly AUC:0.5456
[2023-09-09 16:17:38,491][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00050 | loss1:0.0901 loss2:0.9803 loss3:0.0056 | AUC:0.8029 Anomaly AUC:0.5523
[2023-09-09 16:17:57,396][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00050 | loss1:0.0518 loss2:0.9620 loss3:0.0030 | AUC:0.7954 Anomaly AUC:0.5413
[2023-09-09 16:18:16,332][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00050 | loss1:0.0699 loss2:0.9640 loss3:0.0041 | AUC:0.7786 Anomaly AUC:0.5468
[2023-09-09 16:18:35,182][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00050 | loss1:0.0452 loss2:0.9527 loss3:0.0027 | AUC:0.7816 Anomaly AUC:0.5348
[2023-09-09 16:18:54,110][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00050 | loss1:0.1266 loss2:0.9630 loss3:0.0090 | AUC:0.8066 Anomaly AUC:0.5648
[2023-09-09 16:19:13,031][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00050 | loss1:0.0894 loss2:0.9568 loss3:0.0059 | AUC:0.7899 Anomaly AUC:0.5497
[2023-09-09 16:19:32,085][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00050 | loss1:0.0431 loss2:0.9435 loss3:0.0030 | AUC:0.7886 Anomaly AUC:0.5635
[2023-09-09 16:19:51,066][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00050 | loss1:0.1176 loss2:0.9555 loss3:0.0088 | AUC:0.7766 Anomaly AUC:0.5563
[2023-09-09 16:20:10,040][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00050 | loss1:0.0898 loss2:0.9507 loss3:0.0061 | AUC:0.8178 Anomaly AUC:0.5864
[2023-09-09 16:20:28,991][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00050 | loss1:0.0754 loss2:0.9390 loss3:0.0044 | AUC:0.7942 Anomaly AUC:0.5567
[2023-09-09 16:20:47,947][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00050 | loss1:0.0296 loss2:0.9318 loss3:0.0017 | AUC:0.8040 Anomaly AUC:0.5422
[2023-09-09 16:21:07,066][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00050 | loss1:0.0279 loss2:0.9345 loss3:0.0015 | AUC:0.7896 Anomaly AUC:0.5583
[2023-09-09 16:21:25,997][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00050 | loss1:0.0179 loss2:0.9267 loss3:0.0009 | AUC:0.7906 Anomaly AUC:0.5558
[2023-09-09 16:21:44,834][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00050 | loss1:0.1226 loss2:0.9270 loss3:0.0096 | AUC:0.6850 Anomaly AUC:0.5060
[2023-09-09 16:22:03,735][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00050 | loss1:0.1166 loss2:0.8758 loss3:0.0131 | AUC:0.7694 Anomaly AUC:0.5707
[2023-09-09 16:22:22,688][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00050 | loss1:0.0237 loss2:0.8038 loss3:0.0015 | AUC:0.8036 Anomaly AUC:0.5739
[2023-09-09 16:22:41,624][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00050 | loss1:0.1025 loss2:0.7838 loss3:0.0094 | AUC:0.7610 Anomaly AUC:0.5444
[2023-09-09 16:23:00,641][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00050 | loss1:0.0297 loss2:0.6748 loss3:0.0013 | AUC:0.7966 Anomaly AUC:0.5841
[2023-09-09 16:23:19,682][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00050 | loss1:0.0202 loss2:0.5512 loss3:0.0008 | AUC:0.7775 Anomaly AUC:0.5527
[2023-09-09 16:23:38,655][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00050 | loss1:0.0328 loss2:0.4756 loss3:0.0016 | AUC:0.7932 Anomaly AUC:0.5680
[2023-09-09 16:23:57,665][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00050 | loss1:0.0238 loss2:0.3574 loss3:0.0012 | AUC:0.7814 Anomaly AUC:0.5606
[2023-09-09 16:24:16,628][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00050 | loss1:0.0278 loss2:0.2544 loss3:0.0017 | AUC:0.7798 Anomaly AUC:0.5509
[2023-09-09 16:24:35,768][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00050 | loss1:0.0392 loss2:0.2019 loss3:0.0020 | AUC:0.7740 Anomaly AUC:0.5552
[2023-09-09 16:24:54,690][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00050 | loss1:0.0403 loss2:0.1446 loss3:0.0020 | AUC:0.7207 Anomaly AUC:0.5186
[2023-09-09 16:25:13,728][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00050 | loss1:0.0330 loss2:0.0833 loss3:0.0014 | AUC:0.7450 Anomaly AUC:0.5488
[2023-09-09 16:25:32,864][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00050 | loss1:0.0283 loss2:0.0717 loss3:0.0018 | AUC:0.7528 Anomaly AUC:0.5536
[2023-09-09 16:25:51,982][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00050 | loss1:0.0086 loss2:0.0204 loss3:0.0005 | AUC:0.7522 Anomaly AUC:0.5590
[2023-09-09 16:26:11,236][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00050 | loss1:0.0058 loss2:0.0109 loss3:0.0004 | AUC:0.7468 Anomaly AUC:0.5481
[2023-09-09 16:26:30,309][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00050 | loss1:0.0046 loss2:0.0082 loss3:0.0003 | AUC:0.7487 Anomaly AUC:0.5571
[2023-09-09 16:26:49,430][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00050 | loss1:0.0040 loss2:0.0058 loss3:0.0003 | AUC:0.7495 Anomaly AUC:0.5573
[2023-09-09 16:27:08,517][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00050 | loss1:0.0034 loss2:0.0045 loss3:0.0003 | AUC:0.7489 Anomaly AUC:0.5529
[2023-09-09 16:27:27,750][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00050 | loss1:0.0031 loss2:0.0041 loss3:0.0003 | AUC:0.7559 Anomaly AUC:0.5595
[2023-09-09 16:27:46,919][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00050 | loss1:0.0031 loss2:0.0040 loss3:0.0003 | AUC:0.7488 Anomaly AUC:0.5558
[2023-09-09 16:28:06,015][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00050 | loss1:0.0028 loss2:0.0036 loss3:0.0003 | AUC:0.7452 Anomaly AUC:0.5397
[2023-09-09 16:28:25,225][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00050 | loss1:0.0036 loss2:0.0108 loss3:0.0004 | AUC:0.7419 Anomaly AUC:0.5465
[2023-09-09 16:28:44,248][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00050 | loss1:0.1076 loss2:0.1394 loss3:0.0086 | AUC:0.7472 Anomaly AUC:0.5377
[2023-09-09 16:29:03,360][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00050 | loss1:0.1332 loss2:0.1853 loss3:0.0078 | AUC:0.7976 Anomaly AUC:0.5542
[2023-09-09 16:29:22,422][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00050 | loss1:0.0684 loss2:0.0739 loss3:0.0033 | AUC:0.7907 Anomaly AUC:0.5323
[2023-09-09 16:29:41,572][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00050 | loss1:0.0169 loss2:0.0176 loss3:0.0010 | AUC:0.7616 Anomaly AUC:0.5369
[2023-09-09 16:30:00,613][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00050 | loss1:0.0257 loss2:0.0188 loss3:0.0013 | AUC:0.7638 Anomaly AUC:0.5141
[2023-09-09 16:30:19,680][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00050 | loss1:0.0053 loss2:0.0069 loss3:0.0005 | AUC:0.7795 Anomaly AUC:0.5247
[2023-09-09 16:30:38,803][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00050 | loss1:0.0031 loss2:0.0043 loss3:0.0003 | AUC:0.7770 Anomaly AUC:0.5236
[2023-09-09 16:30:57,892][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00050 | loss1:0.0024 loss2:0.0032 loss3:0.0002 | AUC:0.7736 Anomaly AUC:0.5231
[2023-09-09 16:31:16,930][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00050 | loss1:0.0019 loss2:0.0026 loss3:0.0002 | AUC:0.7728 Anomaly AUC:0.5219
[2023-09-09 16:31:35,993][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00050 | loss1:0.0015 loss2:0.0025 loss3:0.0002 | AUC:0.7714 Anomaly AUC:0.5213
[2023-09-09 16:31:55,002][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00050 | loss1:0.0015 loss2:0.0025 loss3:0.0002 | AUC:0.7722 Anomaly AUC:0.5216
[2023-09-09 16:32:14,128][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00050 | loss1:0.0012 loss2:0.0025 loss3:0.0002 | AUC:0.7726 Anomaly AUC:0.5242
[2023-09-09 16:32:33,182][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00050 | loss1:0.0010 loss2:0.0021 loss3:0.0002 | AUC:0.7721 Anomaly AUC:0.5233
[2023-09-09 16:32:52,287][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00050 | loss1:0.0009 loss2:0.0017 loss3:0.0002 | AUC:0.7734 Anomaly AUC:0.5261
[2023-09-09 16:33:11,532][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00050 | loss1:0.0010 loss2:0.0019 loss3:0.0002 | AUC:0.7750 Anomaly AUC:0.5292
[2023-09-09 16:33:30,863][main.py][line:106][INFO] [Epoch:64/100]: lr:0.00050 | loss1:0.0008 loss2:0.0018 loss3:0.0002 | AUC:0.7729 Anomaly AUC:0.5271
[2023-09-09 16:33:50,000][main.py][line:106][INFO] [Epoch:65/100]: lr:0.00050 | loss1:0.0008 loss2:0.0017 loss3:0.0002 | AUC:0.7754 Anomaly AUC:0.5322
[2023-09-09 16:34:09,090][main.py][line:106][INFO] [Epoch:66/100]: lr:0.00050 | loss1:0.0008 loss2:0.0016 loss3:0.0002 | AUC:0.7779 Anomaly AUC:0.5308
[2023-09-09 16:34:28,348][main.py][line:106][INFO] [Epoch:67/100]: lr:0.00050 | loss1:0.0007 loss2:0.0018 loss3:0.0002 | AUC:0.7792 Anomaly AUC:0.5311
[2023-09-09 16:34:47,433][main.py][line:106][INFO] [Epoch:68/100]: lr:0.00050 | loss1:0.0007 loss2:0.0016 loss3:0.0002 | AUC:0.7748 Anomaly AUC:0.5317
[2023-09-09 16:35:06,551][main.py][line:106][INFO] [Epoch:69/100]: lr:0.00050 | loss1:0.0006 loss2:0.0016 loss3:0.0002 | AUC:0.7777 Anomaly AUC:0.5344
[2023-09-09 16:35:25,713][main.py][line:106][INFO] [Epoch:70/100]: lr:0.00050 | loss1:0.0007 loss2:0.0018 loss3:0.0002 | AUC:0.7738 Anomaly AUC:0.5275
[2023-09-09 16:35:44,830][main.py][line:106][INFO] [Epoch:71/100]: lr:0.00050 | loss1:0.0009 loss2:0.0044 loss3:0.0002 | AUC:0.7754 Anomaly AUC:0.5276
[2023-09-09 16:36:04,012][main.py][line:106][INFO] [Epoch:72/100]: lr:0.00050 | loss1:0.1927 loss2:0.2322 loss3:0.0157 | AUC:0.7282 Anomaly AUC:0.4985
[2023-09-09 16:36:23,226][main.py][line:106][INFO] [Epoch:73/100]: lr:0.00050 | loss1:0.0678 loss2:0.0931 loss3:0.0041 | AUC:0.6988 Anomaly AUC:0.4762
[2023-09-09 16:36:42,450][main.py][line:106][INFO] [Epoch:74/100]: lr:0.00050 | loss1:0.0278 loss2:0.0321 loss3:0.0017 | AUC:0.7005 Anomaly AUC:0.4862
[2023-09-09 16:37:01,565][main.py][line:106][INFO] [Epoch:75/100]: lr:0.00050 | loss1:0.0296 loss2:0.0269 loss3:0.0016 | AUC:0.7226 Anomaly AUC:0.5129
[2023-09-09 16:37:20,722][main.py][line:106][INFO] [Epoch:76/100]: lr:0.00050 | loss1:0.0211 loss2:0.0200 loss3:0.0013 | AUC:0.7545 Anomaly AUC:0.4956
[2023-09-09 16:37:39,978][main.py][line:106][INFO] [Epoch:77/100]: lr:0.00050 | loss1:0.0222 loss2:0.0199 loss3:0.0015 | AUC:0.7627 Anomaly AUC:0.5121
[2023-09-09 16:37:59,278][main.py][line:106][INFO] [Epoch:78/100]: lr:0.00050 | loss1:0.0080 loss2:0.0086 loss3:0.0007 | AUC:0.7631 Anomaly AUC:0.5099
[2023-09-09 16:38:18,351][main.py][line:106][INFO] [Epoch:79/100]: lr:0.00050 | loss1:0.0030 loss2:0.0044 loss3:0.0004 | AUC:0.7636 Anomaly AUC:0.5085
[2023-09-09 16:38:37,578][main.py][line:106][INFO] [Epoch:80/100]: lr:0.00050 | loss1:0.0454 loss2:0.0256 loss3:0.0029 | AUC:0.7879 Anomaly AUC:0.5444
[2023-09-09 16:38:56,751][main.py][line:106][INFO] [Epoch:81/100]: lr:0.00050 | loss1:0.0404 loss2:0.0375 loss3:0.0022 | AUC:0.7785 Anomaly AUC:0.5397
[2023-09-09 16:39:15,946][main.py][line:106][INFO] [Epoch:82/100]: lr:0.00050 | loss1:0.0236 loss2:0.0197 loss3:0.0015 | AUC:0.7586 Anomaly AUC:0.5277
[2023-09-09 16:39:35,092][main.py][line:106][INFO] [Epoch:83/100]: lr:0.00050 | loss1:0.0288 loss2:0.0236 loss3:0.0019 | AUC:0.7714 Anomaly AUC:0.5235
[2023-09-09 16:39:54,218][main.py][line:106][INFO] [Epoch:84/100]: lr:0.00050 | loss1:0.0139 loss2:0.0139 loss3:0.0009 | AUC:0.7692 Anomaly AUC:0.5498
[2023-09-09 16:40:13,395][main.py][line:106][INFO] [Epoch:85/100]: lr:0.00050 | loss1:0.0396 loss2:0.0351 loss3:0.0024 | AUC:0.7377 Anomaly AUC:0.5508
[2023-09-09 16:40:32,552][main.py][line:106][INFO] [Epoch:86/100]: lr:0.00050 | loss1:0.0104 loss2:0.0147 loss3:0.0009 | AUC:0.7627 Anomaly AUC:0.5547
[2023-09-09 16:40:51,667][main.py][line:106][INFO] [Epoch:87/100]: lr:0.00050 | loss1:0.0033 loss2:0.0034 loss3:0.0005 | AUC:0.7678 Anomaly AUC:0.5438
[2023-09-09 16:41:10,917][main.py][line:106][INFO] [Epoch:88/100]: lr:0.00050 | loss1:0.0015 loss2:0.0021 loss3:0.0003 | AUC:0.7797 Anomaly AUC:0.5575
[2023-09-09 16:41:30,063][main.py][line:106][INFO] [Epoch:89/100]: lr:0.00050 | loss1:0.0010 loss2:0.0018 loss3:0.0002 | AUC:0.7809 Anomaly AUC:0.5562
[2023-09-09 16:41:49,133][main.py][line:106][INFO] [Epoch:90/100]: lr:0.00050 | loss1:0.0008 loss2:0.0017 loss3:0.0002 | AUC:0.7774 Anomaly AUC:0.5495
[2023-09-09 16:42:08,234][main.py][line:106][INFO] [Epoch:91/100]: lr:0.00050 | loss1:0.0007 loss2:0.0015 loss3:0.0002 | AUC:0.7779 Anomaly AUC:0.5467
[2023-09-09 16:42:27,437][main.py][line:106][INFO] [Epoch:92/100]: lr:0.00050 | loss1:0.0006 loss2:0.0015 loss3:0.0002 | AUC:0.7764 Anomaly AUC:0.5424
[2023-09-09 16:42:46,586][main.py][line:106][INFO] [Epoch:93/100]: lr:0.00050 | loss1:0.0006 loss2:0.0015 loss3:0.0002 | AUC:0.7731 Anomaly AUC:0.5360
[2023-09-09 16:43:05,755][main.py][line:106][INFO] [Epoch:94/100]: lr:0.00050 | loss1:0.0694 loss2:0.0419 loss3:0.0024 | AUC:0.7538 Anomaly AUC:0.5455
[2023-09-09 16:43:24,765][main.py][line:106][INFO] [Epoch:95/100]: lr:0.00050 | loss1:0.0875 loss2:0.1129 loss3:0.0070 | AUC:0.7583 Anomaly AUC:0.5318
[2023-09-09 16:43:44,004][main.py][line:106][INFO] [Epoch:96/100]: lr:0.00050 | loss1:0.0181 loss2:0.0202 loss3:0.0018 | AUC:0.7885 Anomaly AUC:0.5553
[2023-09-09 16:44:03,120][main.py][line:106][INFO] [Epoch:97/100]: lr:0.00050 | loss1:0.0341 loss2:0.0267 loss3:0.0025 | AUC:0.7816 Anomaly AUC:0.5434
[2023-09-09 16:44:22,233][main.py][line:106][INFO] [Epoch:98/100]: lr:0.00050 | loss1:0.0093 loss2:0.0137 loss3:0.0009 | AUC:0.7494 Anomaly AUC:0.5157
[2023-09-09 16:44:41,403][main.py][line:106][INFO] [Epoch:99/100]: lr:0.00050 | loss1:0.0283 loss2:0.0180 loss3:0.0017 | AUC:0.7731 Anomaly AUC:0.5793
[2023-09-09 16:45:00,480][main.py][line:106][INFO] [Epoch:100/100]: lr:0.00050 | loss1:0.0173 loss2:0.0159 loss3:0.0012 | AUC:0.7875 Anomaly AUC:0.5303
[2023-09-09 16:45:00,503][main.py][line:116][INFO] Training completes in 31m 45s | best AUCAUC:0.8178 Anomaly AUC:0.5864