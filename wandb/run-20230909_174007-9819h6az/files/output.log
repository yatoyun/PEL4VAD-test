
8000 8100 2900
[2023-09-09 17:40:09,061][main.py][line:165][INFO] total params:24.3531M
[2023-09-09 17:40:09,061][main.py][line:168][INFO] Training Mode
[2023-09-09 17:40:09,062][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
      (embedding2): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(2048, 1024, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (selfatt2): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=1024, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=1024, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=1024, out_features=1024, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=1024, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 17:40:09,062][main.py][line:79][INFO] Optimizer:Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 5e-05
)
[2023-09-09 17:40:19,903][main.py][line:82][INFO] Random initialize AUCAUC:0.5656 Anomaly AUC:0.51347
[2023-09-09 17:40:49,613][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.8322 loss2:1.1904 loss3:0.3541 | AUC:0.7933 Anomaly AUC:0.5729
[2023-09-09 17:41:19,122][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.3928 loss2:0.9224 loss3:0.2457 | AUC:0.8420 Anomaly AUC:0.6471
[2023-09-09 17:41:48,673][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.1928 loss2:0.8320 loss3:0.1603 | AUC:0.8405 Anomaly AUC:0.6644
[2023-09-09 17:42:18,614][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0875 loss2:0.7700 loss3:0.1264 | AUC:0.8494 Anomaly AUC:0.6767
[2023-09-09 17:42:48,415][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0302 loss2:0.6972 loss3:0.0939 | AUC:0.8402 Anomaly AUC:0.6518
[2023-09-09 17:43:18,404][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0289 loss2:0.6814 loss3:0.0925 | AUC:0.8441 Anomaly AUC:0.6484
[2023-09-09 17:43:48,357][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0226 loss2:0.6065 loss3:0.0726 | AUC:0.8198 Anomaly AUC:0.6217
[2023-09-09 17:44:18,336][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0152 loss2:0.5372 loss3:0.0531 | AUC:0.8359 Anomaly AUC:0.6226
[2023-09-09 17:44:48,322][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0227 loss2:0.5333 loss3:0.0614 | AUC:0.8196 Anomaly AUC:0.6046
[2023-09-09 17:45:18,199][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0180 loss2:0.4937 loss3:0.0603 | AUC:0.8310 Anomaly AUC:0.6021
[2023-09-09 17:45:48,233][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0656 loss2:0.4632 loss3:0.0684 | AUC:0.6345 Anomaly AUC:0.5410
[2023-09-09 17:46:18,225][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0957 loss2:0.8192 loss3:0.1941 | AUC:0.8008 Anomaly AUC:0.6402
[2023-09-09 17:46:48,259][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0205 loss2:0.6394 loss3:0.0918 | AUC:0.8135 Anomaly AUC:0.6295
[2023-09-09 17:47:18,298][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0166 loss2:0.5057 loss3:0.0712 | AUC:0.8342 Anomaly AUC:0.6214
[2023-09-09 17:47:48,354][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0105 loss2:0.4042 loss3:0.0448 | AUC:0.8355 Anomaly AUC:0.6142
[2023-09-09 17:48:18,340][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0087 loss2:0.3504 loss3:0.0330 | AUC:0.8330 Anomaly AUC:0.6017
[2023-09-09 17:48:48,401][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0057 loss2:0.3069 loss3:0.0258 | AUC:0.8305 Anomaly AUC:0.6081
[2023-09-09 17:49:18,412][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0072 loss2:0.2842 loss3:0.0267 | AUC:0.8277 Anomaly AUC:0.6006
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 97, in train
    auc, ab_auc = test_func(test_loader, model, gt, cfg.dataset)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/test.py", line 37, in test_func
    logits, _ = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 26, in forward
    mask = self.get_mask(self.win_size, x.shape[1], seq_len)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 64, in get_mask
    m[j, min(max(j - w_len // 2 + k, 0), temporal_scale - 1)] = 1.
KeyboardInterrupt