
8000 8100 2900
[2023-09-09 20:49:36,385][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 20:49:36,386][main.py][line:168][INFO] Training Mode
[2023-09-09 20:49:36,386][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 20:49:36,386][main.py][line:79][INFO] Optimizer:AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
[2023-09-09 20:49:44,456][main.py][line:82][INFO] Random initialize AUCAUC:0.5802 Anomaly AUC:0.50318
[2023-09-09 20:50:02,810][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3546 loss2:1.1733 loss3:0.3478 | AUC:0.8167 Anomaly AUC:0.6703
[2023-09-09 20:50:20,929][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0509 loss2:0.8266 loss3:0.2588 | AUC:0.8157 Anomaly AUC:0.6681
[2023-09-09 20:50:39,457][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0257 loss2:0.7165 loss3:0.1992 | AUC:0.8242 Anomaly AUC:0.6697
[2023-09-09 20:50:58,019][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0223 loss2:0.6414 loss3:0.1459 | AUC:0.8304 Anomaly AUC:0.6715
[2023-09-09 20:51:16,740][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0166 loss2:0.5834 loss3:0.1094 | AUC:0.8316 Anomaly AUC:0.6695
[2023-09-09 20:51:35,671][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0099 loss2:0.5145 loss3:0.0781 | AUC:0.8364 Anomaly AUC:0.6712
[2023-09-09 20:51:54,569][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0095 loss2:0.4731 loss3:0.0684 | AUC:0.8345 Anomaly AUC:0.6673
[2023-09-09 20:52:13,512][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0063 loss2:0.4165 loss3:0.0439 | AUC:0.8426 Anomaly AUC:0.6685
[2023-09-09 20:52:32,581][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0128 loss2:0.4404 loss3:0.0637 | AUC:0.8207 Anomaly AUC:0.6622
[2023-09-09 20:52:51,401][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.0050 loss2:0.3683 loss3:0.0370 | AUC:0.8381 Anomaly AUC:0.6744
[2023-09-09 20:53:10,288][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0041 loss2:0.3325 loss3:0.0277 | AUC:0.8393 Anomaly AUC:0.6704
[2023-09-09 20:53:29,169][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0032 loss2:0.3035 loss3:0.0206 | AUC:0.8416 Anomaly AUC:0.6677
[2023-09-09 20:53:48,076][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0333 loss2:0.4531 loss3:0.0525 | AUC:0.8357 Anomaly AUC:0.6760
[2023-09-09 20:54:07,136][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0039 loss2:0.3180 loss3:0.0260 | AUC:0.8457 Anomaly AUC:0.6794
[2023-09-09 20:54:26,060][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0027 loss2:0.2695 loss3:0.0177 | AUC:0.8414 Anomaly AUC:0.6779
[2023-09-09 20:54:44,974][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0029 loss2:0.2586 loss3:0.0170 | AUC:0.8415 Anomaly AUC:0.6689
[2023-09-09 20:55:03,879][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0015 loss2:0.2384 loss3:0.0136 | AUC:0.8459 Anomaly AUC:0.6768
[2023-09-09 20:55:22,808][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0410 loss2:0.3934 loss3:0.0572 | AUC:0.7677 Anomaly AUC:0.6199
[2023-09-09 20:55:41,817][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0109 loss2:0.5439 loss3:0.0576 | AUC:0.8340 Anomaly AUC:0.6705
[2023-09-09 20:56:00,751][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0039 loss2:0.2996 loss3:0.0249 | AUC:0.8439 Anomaly AUC:0.6750
[2023-09-09 20:56:19,577][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0026 loss2:0.2441 loss3:0.0189 | AUC:0.8452 Anomaly AUC:0.6749
[2023-09-09 20:56:38,524][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0017 loss2:0.2199 loss3:0.0145 | AUC:0.8427 Anomaly AUC:0.6704
[2023-09-09 20:56:57,420][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0017 loss2:0.2086 loss3:0.0132 | AUC:0.8467 Anomaly AUC:0.6794
[2023-09-09 20:57:16,531][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0013 loss2:0.1976 loss3:0.0117 | AUC:0.8502 Anomaly AUC:0.6821
[2023-09-09 20:57:35,568][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0049 loss2:0.2204 loss3:0.0159 | AUC:0.8448 Anomaly AUC:0.6725
[2023-09-09 20:57:54,605][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0018 loss2:0.1915 loss3:0.0139 | AUC:0.8425 Anomaly AUC:0.6761
[2023-09-09 20:58:13,700][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0011 loss2:0.1774 loss3:0.0117 | AUC:0.8480 Anomaly AUC:0.6801
[2023-09-09 20:58:32,727][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0008 loss2:0.1664 loss3:0.0102 | AUC:0.8465 Anomaly AUC:0.6774
[2023-09-09 20:58:51,599][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0060 loss2:0.1852 loss3:0.0172 | AUC:0.8343 Anomaly AUC:0.6898
[2023-09-09 20:59:10,499][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0079 loss2:0.2016 loss3:0.0248 | AUC:0.8524 Anomaly AUC:0.6852
[2023-09-09 20:59:29,496][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0008 loss2:0.1572 loss3:0.0113 | AUC:0.8521 Anomaly AUC:0.6815
[2023-09-09 20:59:48,465][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0006 loss2:0.1479 loss3:0.0097 | AUC:0.8537 Anomaly AUC:0.6839
[2023-09-09 21:00:07,457][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0005 loss2:0.1398 loss3:0.0092 | AUC:0.8525 Anomaly AUC:0.6818
[2023-09-09 21:00:26,415][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0008 loss2:0.1372 loss3:0.0090 | AUC:0.8528 Anomaly AUC:0.6800
[2023-09-09 21:00:45,352][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0007 loss2:0.1323 loss3:0.0091 | AUC:0.8526 Anomaly AUC:0.6813
[2023-09-09 21:01:04,506][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0006 loss2:0.1265 loss3:0.0084 | AUC:0.8577 Anomaly AUC:0.6798
[2023-09-09 21:01:23,572][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0007 loss2:0.1233 loss3:0.0082 | AUC:0.8540 Anomaly AUC:0.6803
[2023-09-09 21:01:42,719][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0004 loss2:0.1166 loss3:0.0075 | AUC:0.8519 Anomaly AUC:0.6784
[2023-09-09 21:02:01,717][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0003 loss2:0.1115 loss3:0.0069 | AUC:0.8506 Anomaly AUC:0.6764
[2023-09-09 21:02:20,891][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0003 loss2:0.1071 loss3:0.0064 | AUC:0.8541 Anomaly AUC:0.6805
[2023-09-09 21:02:39,801][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.2622 loss2:0.5259 loss3:0.1262 | AUC:0.8438 Anomaly AUC:0.6870
[2023-09-09 21:02:58,869][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0057 loss2:0.3939 loss3:0.0196 | AUC:0.8590 Anomaly AUC:0.6892
[2023-09-09 21:03:17,954][main.py][line:106][INFO] [Epoch:43/100]: lr:0.00010 | loss1:0.0055 loss2:0.2072 loss3:0.0116 | AUC:0.8590 Anomaly AUC:0.6915
[2023-09-09 21:03:37,034][main.py][line:106][INFO] [Epoch:44/100]: lr:0.00010 | loss1:0.0026 loss2:0.1727 loss3:0.0090 | AUC:0.8583 Anomaly AUC:0.6911
[2023-09-09 21:03:56,191][main.py][line:106][INFO] [Epoch:45/100]: lr:0.00010 | loss1:0.0012 loss2:0.1524 loss3:0.0061 | AUC:0.8567 Anomaly AUC:0.6901
[2023-09-09 21:04:15,254][main.py][line:106][INFO] [Epoch:46/100]: lr:0.00010 | loss1:0.0012 loss2:0.1444 loss3:0.0054 | AUC:0.8539 Anomaly AUC:0.6838
[2023-09-09 21:04:34,462][main.py][line:106][INFO] [Epoch:47/100]: lr:0.00010 | loss1:0.0009 loss2:0.1356 loss3:0.0045 | AUC:0.8543 Anomaly AUC:0.6883
[2023-09-09 21:04:53,543][main.py][line:106][INFO] [Epoch:48/100]: lr:0.00010 | loss1:0.0007 loss2:0.1310 loss3:0.0039 | AUC:0.8537 Anomaly AUC:0.6865
[2023-09-09 21:05:12,484][main.py][line:106][INFO] [Epoch:49/100]: lr:0.00010 | loss1:0.0006 loss2:0.1227 loss3:0.0033 | AUC:0.8537 Anomaly AUC:0.6876
[2023-09-09 21:05:31,635][main.py][line:106][INFO] [Epoch:50/100]: lr:0.00010 | loss1:0.0005 loss2:0.1163 loss3:0.0027 | AUC:0.8535 Anomaly AUC:0.6887
[2023-09-09 21:05:50,711][main.py][line:106][INFO] [Epoch:51/100]: lr:0.00010 | loss1:0.0005 loss2:0.1122 loss3:0.0021 | AUC:0.8532 Anomaly AUC:0.6883
[2023-09-09 21:06:09,733][main.py][line:106][INFO] [Epoch:52/100]: lr:0.00010 | loss1:0.0004 loss2:0.1072 loss3:0.0014 | AUC:0.8535 Anomaly AUC:0.6892
[2023-09-09 21:06:28,974][main.py][line:106][INFO] [Epoch:53/100]: lr:0.00010 | loss1:0.0004 loss2:0.1021 loss3:0.0008 | AUC:0.8540 Anomaly AUC:0.6917
[2023-09-09 21:06:47,971][main.py][line:106][INFO] [Epoch:54/100]: lr:0.00010 | loss1:0.0003 loss2:0.0980 loss3:0.0006 | AUC:0.8547 Anomaly AUC:0.6925
[2023-09-09 21:07:07,025][main.py][line:106][INFO] [Epoch:55/100]: lr:0.00010 | loss1:0.0003 loss2:0.0929 loss3:0.0006 | AUC:0.8540 Anomaly AUC:0.6924
[2023-09-09 21:07:26,155][main.py][line:106][INFO] [Epoch:56/100]: lr:0.00010 | loss1:0.0003 loss2:0.0891 loss3:0.0005 | AUC:0.8532 Anomaly AUC:0.6902
[2023-09-09 21:07:45,270][main.py][line:106][INFO] [Epoch:57/100]: lr:0.00010 | loss1:0.0002 loss2:0.0851 loss3:0.0005 | AUC:0.8528 Anomaly AUC:0.6891
[2023-09-09 21:08:04,314][main.py][line:106][INFO] [Epoch:58/100]: lr:0.00010 | loss1:0.0002 loss2:0.0819 loss3:0.0005 | AUC:0.8528 Anomaly AUC:0.6895
[2023-09-09 21:08:23,364][main.py][line:106][INFO] [Epoch:59/100]: lr:0.00010 | loss1:0.0002 loss2:0.0781 loss3:0.0004 | AUC:0.8525 Anomaly AUC:0.6896
[2023-09-09 21:08:42,524][main.py][line:106][INFO] [Epoch:60/100]: lr:0.00010 | loss1:0.0002 loss2:0.0754 loss3:0.0004 | AUC:0.8525 Anomaly AUC:0.6890
[2023-09-09 21:09:01,613][main.py][line:106][INFO] [Epoch:61/100]: lr:0.00010 | loss1:0.0150 loss2:0.0802 loss3:0.0022 | AUC:0.7825 Anomaly AUC:0.5468
[2023-09-09 21:09:20,729][main.py][line:106][INFO] [Epoch:62/100]: lr:0.00010 | loss1:0.0877 loss2:0.4993 loss3:0.0399 | AUC:0.8416 Anomaly AUC:0.6711
[2023-09-09 21:09:39,917][main.py][line:106][INFO] [Epoch:63/100]: lr:0.00010 | loss1:0.0093 loss2:0.1433 loss3:0.0116 | AUC:0.8344 Anomaly AUC:0.6699
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 59, in train_func
    loss.backward()
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt