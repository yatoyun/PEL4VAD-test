
8000 8100 2900
[2023-09-09 21:11:42,263][main.py][line:165][INFO] total params:7.5400M
[2023-09-09 21:11:42,264][main.py][line:168][INFO] Training Mode
[2023-09-09 21:11:42,264][main.py][line:78][INFO] Model:XModel(
  (self_attention): XEncoder(
    (self_attn): TCA(
      (q): Linear(in_features=1024, out_features=128, bias=True)
      (k): Linear(in_features=1024, out_features=128, bias=True)
      (v): Linear(in_features=1024, out_features=128, bias=True)
      (o): Linear(in_features=128, out_features=1024, bias=True)
      (act): Softmax(dim=-1)
    )
    (linear1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
    (linear2): Conv1d(512, 300, kernel_size=(1,), stride=(1,))
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (loc_adj): DistanceAdj()
    (DR_DMU): WSAD(
      (embedding): Temporal(
        (conv_1): Sequential(
          (0): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): ReLU()
        )
      )
      (triplet): TripletMarginLoss()
      (Amemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (Nmemory): Memory_Unit(
        (sig): Sigmoid()
      )
      (selfatt): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=512, out_features=2048, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=512, out_features=512, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=512, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
      )
      (encoder_mu): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (encoder_var): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
      )
      (relu): ReLU()
    )
  )
  (classifier): Conv1d(300, 1, kernel_size=(9,), stride=(1,))
  (dropout): Dropout(p=0.1, inplace=False)
)
[2023-09-09 21:11:42,264][main.py][line:79][INFO] Optimizer:AdamW (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
[2023-09-09 21:11:50,180][main.py][line:82][INFO] Random initialize AUCAUC:0.5802 Anomaly AUC:0.50318
[2023-09-09 21:12:08,511][main.py][line:106][INFO] [Epoch:1/100]: lr:0.00010 | loss1:0.3516 loss2:1.1696 loss3:0.3472 | AUC:0.8143 Anomaly AUC:0.6714
[2023-09-09 21:12:26,616][main.py][line:106][INFO] [Epoch:2/100]: lr:0.00010 | loss1:0.0512 loss2:0.8260 loss3:0.2585 | AUC:0.8147 Anomaly AUC:0.6706
[2023-09-09 21:12:45,043][main.py][line:106][INFO] [Epoch:3/100]: lr:0.00010 | loss1:0.0265 loss2:0.7147 loss3:0.1987 | AUC:0.8194 Anomaly AUC:0.6706
[2023-09-09 21:13:03,723][main.py][line:106][INFO] [Epoch:4/100]: lr:0.00010 | loss1:0.0223 loss2:0.6368 loss3:0.1464 | AUC:0.8252 Anomaly AUC:0.6565
[2023-09-09 21:13:22,339][main.py][line:106][INFO] [Epoch:5/100]: lr:0.00010 | loss1:0.0159 loss2:0.5722 loss3:0.1067 | AUC:0.8310 Anomaly AUC:0.6723
[2023-09-09 21:13:41,045][main.py][line:106][INFO] [Epoch:6/100]: lr:0.00010 | loss1:0.0111 loss2:0.5123 loss3:0.0776 | AUC:0.8367 Anomaly AUC:0.6656
[2023-09-09 21:13:59,821][main.py][line:106][INFO] [Epoch:7/100]: lr:0.00010 | loss1:0.0098 loss2:0.4699 loss3:0.0618 | AUC:0.8404 Anomaly AUC:0.6721
[2023-09-09 21:14:18,661][main.py][line:106][INFO] [Epoch:8/100]: lr:0.00010 | loss1:0.0093 loss2:0.4202 loss3:0.0447 | AUC:0.8192 Anomaly AUC:0.6713
[2023-09-09 21:14:37,428][main.py][line:106][INFO] [Epoch:9/100]: lr:0.00010 | loss1:0.0069 loss2:0.3959 loss3:0.0424 | AUC:0.8394 Anomaly AUC:0.6746
[2023-09-09 21:14:56,240][main.py][line:106][INFO] [Epoch:10/100]: lr:0.00010 | loss1:0.1522 loss2:0.5967 loss3:0.1021 | AUC:0.8025 Anomaly AUC:0.6731
[2023-09-09 21:15:15,084][main.py][line:106][INFO] [Epoch:11/100]: lr:0.00010 | loss1:0.0235 loss2:0.5120 loss3:0.0626 | AUC:0.8388 Anomaly AUC:0.6798
[2023-09-09 21:15:33,980][main.py][line:106][INFO] [Epoch:12/100]: lr:0.00010 | loss1:0.0143 loss2:0.4157 loss3:0.0450 | AUC:0.8459 Anomaly AUC:0.6862
[2023-09-09 21:15:52,818][main.py][line:106][INFO] [Epoch:13/100]: lr:0.00010 | loss1:0.0085 loss2:0.3646 loss3:0.0351 | AUC:0.8435 Anomaly AUC:0.6790
[2023-09-09 21:16:11,738][main.py][line:106][INFO] [Epoch:14/100]: lr:0.00010 | loss1:0.0042 loss2:0.3306 loss3:0.0235 | AUC:0.8485 Anomaly AUC:0.6827
[2023-09-09 21:16:30,622][main.py][line:106][INFO] [Epoch:15/100]: lr:0.00010 | loss1:0.0035 loss2:0.3088 loss3:0.0191 | AUC:0.8502 Anomaly AUC:0.6866
[2023-09-09 21:16:49,670][main.py][line:106][INFO] [Epoch:16/100]: lr:0.00010 | loss1:0.0028 loss2:0.2928 loss3:0.0168 | AUC:0.8545 Anomaly AUC:0.6886
[2023-09-09 21:17:08,479][main.py][line:106][INFO] [Epoch:17/100]: lr:0.00010 | loss1:0.0022 loss2:0.2763 loss3:0.0147 | AUC:0.8519 Anomaly AUC:0.6852
[2023-09-09 21:17:27,353][main.py][line:106][INFO] [Epoch:18/100]: lr:0.00010 | loss1:0.0019 loss2:0.2622 loss3:0.0133 | AUC:0.8521 Anomaly AUC:0.6848
[2023-09-09 21:17:46,187][main.py][line:106][INFO] [Epoch:19/100]: lr:0.00010 | loss1:0.0019 loss2:0.2508 loss3:0.0125 | AUC:0.8516 Anomaly AUC:0.6854
[2023-09-09 21:18:05,106][main.py][line:106][INFO] [Epoch:20/100]: lr:0.00010 | loss1:0.0052 loss2:0.2559 loss3:0.0143 | AUC:0.8417 Anomaly AUC:0.6693
[2023-09-09 21:18:23,919][main.py][line:106][INFO] [Epoch:21/100]: lr:0.00010 | loss1:0.0180 loss2:0.3149 loss3:0.0318 | AUC:0.8537 Anomaly AUC:0.6809
[2023-09-09 21:18:42,818][main.py][line:106][INFO] [Epoch:22/100]: lr:0.00010 | loss1:0.0023 loss2:0.2385 loss3:0.0151 | AUC:0.8481 Anomaly AUC:0.6723
[2023-09-09 21:19:01,706][main.py][line:106][INFO] [Epoch:23/100]: lr:0.00010 | loss1:0.0015 loss2:0.2193 loss3:0.0123 | AUC:0.8518 Anomaly AUC:0.6791
[2023-09-09 21:19:20,606][main.py][line:106][INFO] [Epoch:24/100]: lr:0.00010 | loss1:0.0013 loss2:0.2096 loss3:0.0112 | AUC:0.8543 Anomaly AUC:0.6840
[2023-09-09 21:19:39,582][main.py][line:106][INFO] [Epoch:25/100]: lr:0.00010 | loss1:0.0012 loss2:0.2006 loss3:0.0105 | AUC:0.8546 Anomaly AUC:0.6850
[2023-09-09 21:19:58,419][main.py][line:106][INFO] [Epoch:26/100]: lr:0.00010 | loss1:0.0011 loss2:0.1929 loss3:0.0099 | AUC:0.8550 Anomaly AUC:0.6864
[2023-09-09 21:20:17,397][main.py][line:106][INFO] [Epoch:27/100]: lr:0.00010 | loss1:0.0010 loss2:0.1859 loss3:0.0095 | AUC:0.8543 Anomaly AUC:0.6855
[2023-09-09 21:20:36,359][main.py][line:106][INFO] [Epoch:28/100]: lr:0.00010 | loss1:0.0010 loss2:0.1779 loss3:0.0092 | AUC:0.8548 Anomaly AUC:0.6876
[2023-09-09 21:20:55,250][main.py][line:106][INFO] [Epoch:29/100]: lr:0.00010 | loss1:0.0009 loss2:0.1725 loss3:0.0088 | AUC:0.8543 Anomaly AUC:0.6880
[2023-09-09 21:21:14,131][main.py][line:106][INFO] [Epoch:30/100]: lr:0.00010 | loss1:0.0009 loss2:0.1666 loss3:0.0085 | AUC:0.8541 Anomaly AUC:0.6884
[2023-09-09 21:21:33,113][main.py][line:106][INFO] [Epoch:31/100]: lr:0.00010 | loss1:0.0008 loss2:0.1607 loss3:0.0082 | AUC:0.8551 Anomaly AUC:0.6888
[2023-09-09 21:21:52,033][main.py][line:106][INFO] [Epoch:32/100]: lr:0.00010 | loss1:0.0008 loss2:0.1567 loss3:0.0079 | AUC:0.8541 Anomaly AUC:0.6879
[2023-09-09 21:22:11,088][main.py][line:106][INFO] [Epoch:33/100]: lr:0.00010 | loss1:0.0008 loss2:0.1501 loss3:0.0077 | AUC:0.8525 Anomaly AUC:0.6846
[2023-09-09 21:22:29,991][main.py][line:106][INFO] [Epoch:34/100]: lr:0.00010 | loss1:0.0007 loss2:0.1461 loss3:0.0074 | AUC:0.8541 Anomaly AUC:0.6878
[2023-09-09 21:22:48,957][main.py][line:106][INFO] [Epoch:35/100]: lr:0.00010 | loss1:0.0007 loss2:0.1410 loss3:0.0071 | AUC:0.8535 Anomaly AUC:0.6864
[2023-09-09 21:23:08,022][main.py][line:106][INFO] [Epoch:36/100]: lr:0.00010 | loss1:0.0007 loss2:0.1363 loss3:0.0069 | AUC:0.8535 Anomaly AUC:0.6874
[2023-09-09 21:23:27,062][main.py][line:106][INFO] [Epoch:37/100]: lr:0.00010 | loss1:0.0006 loss2:0.1322 loss3:0.0066 | AUC:0.8537 Anomaly AUC:0.6874
[2023-09-09 21:23:45,962][main.py][line:106][INFO] [Epoch:38/100]: lr:0.00010 | loss1:0.0006 loss2:0.1287 loss3:0.0063 | AUC:0.8543 Anomaly AUC:0.6886
[2023-09-09 21:24:04,904][main.py][line:106][INFO] [Epoch:39/100]: lr:0.00010 | loss1:0.0006 loss2:0.1241 loss3:0.0060 | AUC:0.8536 Anomaly AUC:0.6878
[2023-09-09 21:24:23,853][main.py][line:106][INFO] [Epoch:40/100]: lr:0.00010 | loss1:0.0005 loss2:0.1203 loss3:0.0057 | AUC:0.8543 Anomaly AUC:0.6887
[2023-09-09 21:24:43,023][main.py][line:106][INFO] [Epoch:41/100]: lr:0.00010 | loss1:0.0005 loss2:0.1168 loss3:0.0054 | AUC:0.8538 Anomaly AUC:0.6858
[2023-09-09 21:25:02,131][main.py][line:106][INFO] [Epoch:42/100]: lr:0.00010 | loss1:0.0565 loss2:0.2801 loss3:0.0394 | AUC:0.8425 Anomaly AUC:0.6894
Traceback (most recent call last):
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 203, in <module>
    main(cfg)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 170, in main
    train(model, train_nloader, train_aloader, test_loader, gt, logger)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/main.py", line 90, in train
    loss1, loss2, cost = train_func(train_nloader, train_aloader, model, optimizer, criterion, criterion2, criterion3, logger_wandb, args.lamda, args.alpha)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/train.py", line 30, in train_func
    logits, x_k, output_MSNSD = model(v_input, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/model.py", line 37, in forward
    x_e, x_v = self.self_attention(x, seq_len)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/modules.py", line 34, in forward
    x_k = self.DR_DMU(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/model.py", line 72, in forward
    x = self.selfatt(x)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 76, in forward
    x = attn(x) + x
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 14, in forward
    return self.fn(self.norm(x), **kwargs)
  File "/home/yukaneko/miniconda3/envs/detection/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yukaneko/dev/AbnormalDetection/PEL4VAD/DR_DMU/translayer.py", line 55, in forward
    tmp_ones = torch.ones(n).cuda()
KeyboardInterrupt